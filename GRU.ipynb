{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
   "id": "f0055bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 11:14:01.381965: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/lib/python3.11/site-packages/h5py/__init__.py:36: UserWarning: h5py is running against HDF5 1.14.2 when it was built against 1.14.1, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
=======
   "execution_count": 11,
   "id": "f01c67d3",
   "metadata": {},
   "outputs": [],
>>>>>>> 1ce5a549fd7b813ed9c4e92a5fc64e67e3973b50
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GRU\n",
<<<<<<< HEAD
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
=======
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error"
>>>>>>> 1ce5a549fd7b813ed9c4e92a5fc64e67e3973b50
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "id": "4ada5b51",
=======
   "execution_count": 12,
   "id": "bee2e983",
>>>>>>> 1ce5a549fd7b813ed9c4e92a5fc64e67e3973b50
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>0.9133</td>\n",
       "      <td>1.4419</td>\n",
       "      <td>1.7200</td>\n",
       "      <td>1.0377</td>\n",
       "      <td>6.8273</td>\n",
       "      <td>5.1597</td>\n",
       "      <td>7.7555</td>\n",
       "      <td>46.27</td>\n",
       "      <td>92.55</td>\n",
       "      <td>3.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>0.9143</td>\n",
       "      <td>1.4402</td>\n",
       "      <td>1.7296</td>\n",
       "      <td>1.0371</td>\n",
       "      <td>6.8258</td>\n",
       "      <td>5.1668</td>\n",
       "      <td>7.7564</td>\n",
       "      <td>46.13</td>\n",
       "      <td>91.48</td>\n",
       "      <td>3.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>0.9189</td>\n",
       "      <td>1.4404</td>\n",
       "      <td>1.7292</td>\n",
       "      <td>1.0333</td>\n",
       "      <td>6.8272</td>\n",
       "      <td>5.1638</td>\n",
       "      <td>7.7546</td>\n",
       "      <td>45.72</td>\n",
       "      <td>92.53</td>\n",
       "      <td>3.379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>0.9168</td>\n",
       "      <td>1.4314</td>\n",
       "      <td>1.7409</td>\n",
       "      <td>1.0351</td>\n",
       "      <td>6.8280</td>\n",
       "      <td>5.1981</td>\n",
       "      <td>7.7539</td>\n",
       "      <td>45.67</td>\n",
       "      <td>93.31</td>\n",
       "      <td>3.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>0.9218</td>\n",
       "      <td>1.4357</td>\n",
       "      <td>1.7342</td>\n",
       "      <td>1.0345</td>\n",
       "      <td>6.8274</td>\n",
       "      <td>5.1827</td>\n",
       "      <td>7.7553</td>\n",
       "      <td>45.50</td>\n",
       "      <td>92.70</td>\n",
       "      <td>3.375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  Australia  Europe  Brazil  Canada   China  \\\n",
       "0             0  2010-01-04     0.9133  1.4419  1.7200  1.0377  6.8273   \n",
       "1             1  2010-01-05     0.9143  1.4402  1.7296  1.0371  6.8258   \n",
       "2             2  2010-01-06     0.9189  1.4404  1.7292  1.0333  6.8272   \n",
       "3             3  2010-01-07     0.9168  1.4314  1.7409  1.0351  6.8280   \n",
       "4             4  2010-01-08     0.9218  1.4357  1.7342  1.0345  6.8274   \n",
       "\n",
       "   Denmark  Hong Kong  India  Japan  Malaysia  \n",
       "0   5.1597     7.7555  46.27  92.55     3.396  \n",
       "1   5.1668     7.7564  46.13  91.48     3.385  \n",
       "2   5.1638     7.7546  45.72  92.53     3.379  \n",
       "3   5.1981     7.7539  45.67  93.31     3.368  \n",
       "4   5.1827     7.7553  45.50  92.70     3.375  "
      ]
     },
<<<<<<< HEAD
     "execution_count": 2,
=======
     "execution_count": 12,
>>>>>>> 1ce5a549fd7b813ed9c4e92a5fc64e67e3973b50
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "id": "fa564929",
=======
   "execution_count": 13,
   "id": "c5cd89d2",
>>>>>>> 1ce5a549fd7b813ed9c4e92a5fc64e67e3973b50
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data.drop(\"Unnamed: 0.1\", axis = 1, inplace = True)\n",
    "    data.rename(columns={\"Unnamed: 0\": \"Date\"}, inplace=True)\n",
    "    data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
    "    data.set_index(\"Date\", inplace=True)\n",
    "    data.replace(0, np.nan, inplace=True)\n",
    "    display(data)\n",
    "    print(\"Filling missing Values: \")\n",
    "    display(data.interpolate(method='linear', limit_direction='forward'))\n",
    "    data.interpolate(method='linear', limit_direction='forward', inplace = True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "id": "6457d99b",
=======
   "execution_count": 14,
   "id": "5f1ffad1",
>>>>>>> 1ce5a549fd7b813ed9c4e92a5fc64e67e3973b50
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>0.9133</td>\n",
       "      <td>1.4419</td>\n",
       "      <td>1.7200</td>\n",
       "      <td>1.0377</td>\n",
       "      <td>6.8273</td>\n",
       "      <td>5.1597</td>\n",
       "      <td>7.7555</td>\n",
       "      <td>46.27</td>\n",
       "      <td>92.55</td>\n",
       "      <td>3.3960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.9143</td>\n",
       "      <td>1.4402</td>\n",
       "      <td>1.7296</td>\n",
       "      <td>1.0371</td>\n",
       "      <td>6.8258</td>\n",
       "      <td>5.1668</td>\n",
       "      <td>7.7564</td>\n",
       "      <td>46.13</td>\n",
       "      <td>91.48</td>\n",
       "      <td>3.3850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>0.9189</td>\n",
       "      <td>1.4404</td>\n",
       "      <td>1.7292</td>\n",
       "      <td>1.0333</td>\n",
       "      <td>6.8272</td>\n",
       "      <td>5.1638</td>\n",
       "      <td>7.7546</td>\n",
       "      <td>45.72</td>\n",
       "      <td>92.53</td>\n",
       "      <td>3.3790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>0.9168</td>\n",
       "      <td>1.4314</td>\n",
       "      <td>1.7409</td>\n",
       "      <td>1.0351</td>\n",
       "      <td>6.8280</td>\n",
       "      <td>5.1981</td>\n",
       "      <td>7.7539</td>\n",
       "      <td>45.67</td>\n",
       "      <td>93.31</td>\n",
       "      <td>3.3680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>0.9218</td>\n",
       "      <td>1.4357</td>\n",
       "      <td>1.7342</td>\n",
       "      <td>1.0345</td>\n",
       "      <td>6.8274</td>\n",
       "      <td>5.1827</td>\n",
       "      <td>7.7553</td>\n",
       "      <td>45.50</td>\n",
       "      <td>92.70</td>\n",
       "      <td>3.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>0.6978</td>\n",
       "      <td>1.1174</td>\n",
       "      <td>4.0507</td>\n",
       "      <td>1.3073</td>\n",
       "      <td>6.9954</td>\n",
       "      <td>6.6829</td>\n",
       "      <td>7.7874</td>\n",
       "      <td>71.45</td>\n",
       "      <td>109.47</td>\n",
       "      <td>4.1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>0.7004</td>\n",
       "      <td>1.1217</td>\n",
       "      <td>4.0152</td>\n",
       "      <td>1.3058</td>\n",
       "      <td>6.9864</td>\n",
       "      <td>6.6589</td>\n",
       "      <td>7.7857</td>\n",
       "      <td>71.30</td>\n",
       "      <td>108.85</td>\n",
       "      <td>4.1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>0.7030</td>\n",
       "      <td>1.1227</td>\n",
       "      <td>4.0190</td>\n",
       "      <td>1.2962</td>\n",
       "      <td>6.9618</td>\n",
       "      <td>6.6554</td>\n",
       "      <td>7.7894</td>\n",
       "      <td>71.36</td>\n",
       "      <td>108.67</td>\n",
       "      <td>4.0918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3648 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Australia  Europe  Brazil  Canada   China  Denmark  Hong Kong  \\\n",
       "Date                                                                        \n",
       "2010-01-04     0.9133  1.4419  1.7200  1.0377  6.8273   5.1597     7.7555   \n",
       "2010-01-05     0.9143  1.4402  1.7296  1.0371  6.8258   5.1668     7.7564   \n",
       "2010-01-06     0.9189  1.4404  1.7292  1.0333  6.8272   5.1638     7.7546   \n",
       "2010-01-07     0.9168  1.4314  1.7409  1.0351  6.8280   5.1981     7.7539   \n",
       "2010-01-08     0.9218  1.4357  1.7342  1.0345  6.8274   5.1827     7.7553   \n",
       "...               ...     ...     ...     ...     ...      ...        ...   \n",
       "2019-12-27     0.6978  1.1174  4.0507  1.3073  6.9954   6.6829     7.7874   \n",
       "2019-12-28        NaN     NaN     NaN     NaN     NaN      NaN        NaN   \n",
       "2019-12-29        NaN     NaN     NaN     NaN     NaN      NaN        NaN   \n",
       "2019-12-30     0.7004  1.1217  4.0152  1.3058  6.9864   6.6589     7.7857   \n",
       "2019-12-31     0.7030  1.1227  4.0190  1.2962  6.9618   6.6554     7.7894   \n",
       "\n",
       "            India   Japan  Malaysia  \n",
       "Date                                 \n",
       "2010-01-04  46.27   92.55    3.3960  \n",
       "2010-01-05  46.13   91.48    3.3850  \n",
       "2010-01-06  45.72   92.53    3.3790  \n",
       "2010-01-07  45.67   93.31    3.3680  \n",
       "2010-01-08  45.50   92.70    3.3750  \n",
       "...           ...     ...       ...  \n",
       "2019-12-27  71.45  109.47    4.1260  \n",
       "2019-12-28    NaN     NaN       NaN  \n",
       "2019-12-29    NaN     NaN       NaN  \n",
       "2019-12-30  71.30  108.85    4.1053  \n",
       "2019-12-31  71.36  108.67    4.0918  \n",
       "\n",
       "[3648 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling missing Values: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>0.913300</td>\n",
       "      <td>1.441900</td>\n",
       "      <td>1.720000</td>\n",
       "      <td>1.0377</td>\n",
       "      <td>6.8273</td>\n",
       "      <td>5.1597</td>\n",
       "      <td>7.755500</td>\n",
       "      <td>46.27</td>\n",
       "      <td>92.550000</td>\n",
       "      <td>3.3960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.914300</td>\n",
       "      <td>1.440200</td>\n",
       "      <td>1.729600</td>\n",
       "      <td>1.0371</td>\n",
       "      <td>6.8258</td>\n",
       "      <td>5.1668</td>\n",
       "      <td>7.756400</td>\n",
       "      <td>46.13</td>\n",
       "      <td>91.480000</td>\n",
       "      <td>3.3850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>0.918900</td>\n",
       "      <td>1.440400</td>\n",
       "      <td>1.729200</td>\n",
       "      <td>1.0333</td>\n",
       "      <td>6.8272</td>\n",
       "      <td>5.1638</td>\n",
       "      <td>7.754600</td>\n",
       "      <td>45.72</td>\n",
       "      <td>92.530000</td>\n",
       "      <td>3.3790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>0.916800</td>\n",
       "      <td>1.431400</td>\n",
       "      <td>1.740900</td>\n",
       "      <td>1.0351</td>\n",
       "      <td>6.8280</td>\n",
       "      <td>5.1981</td>\n",
       "      <td>7.753900</td>\n",
       "      <td>45.67</td>\n",
       "      <td>93.310000</td>\n",
       "      <td>3.3680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>0.921800</td>\n",
       "      <td>1.435700</td>\n",
       "      <td>1.734200</td>\n",
       "      <td>1.0345</td>\n",
       "      <td>6.8274</td>\n",
       "      <td>5.1827</td>\n",
       "      <td>7.755300</td>\n",
       "      <td>45.50</td>\n",
       "      <td>92.700000</td>\n",
       "      <td>3.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>0.697800</td>\n",
       "      <td>1.117400</td>\n",
       "      <td>4.050700</td>\n",
       "      <td>1.3073</td>\n",
       "      <td>6.9954</td>\n",
       "      <td>6.6829</td>\n",
       "      <td>7.787400</td>\n",
       "      <td>71.45</td>\n",
       "      <td>109.470000</td>\n",
       "      <td>4.1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-28</th>\n",
       "      <td>0.698667</td>\n",
       "      <td>1.118833</td>\n",
       "      <td>4.038867</td>\n",
       "      <td>1.3068</td>\n",
       "      <td>6.9924</td>\n",
       "      <td>6.6749</td>\n",
       "      <td>7.786833</td>\n",
       "      <td>71.40</td>\n",
       "      <td>109.263333</td>\n",
       "      <td>4.1191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-29</th>\n",
       "      <td>0.699533</td>\n",
       "      <td>1.120267</td>\n",
       "      <td>4.027033</td>\n",
       "      <td>1.3063</td>\n",
       "      <td>6.9894</td>\n",
       "      <td>6.6669</td>\n",
       "      <td>7.786267</td>\n",
       "      <td>71.35</td>\n",
       "      <td>109.056667</td>\n",
       "      <td>4.1122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>0.700400</td>\n",
       "      <td>1.121700</td>\n",
       "      <td>4.015200</td>\n",
       "      <td>1.3058</td>\n",
       "      <td>6.9864</td>\n",
       "      <td>6.6589</td>\n",
       "      <td>7.785700</td>\n",
       "      <td>71.30</td>\n",
       "      <td>108.850000</td>\n",
       "      <td>4.1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>0.703000</td>\n",
       "      <td>1.122700</td>\n",
       "      <td>4.019000</td>\n",
       "      <td>1.2962</td>\n",
       "      <td>6.9618</td>\n",
       "      <td>6.6554</td>\n",
       "      <td>7.789400</td>\n",
       "      <td>71.36</td>\n",
       "      <td>108.670000</td>\n",
       "      <td>4.0918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3648 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Australia    Europe    Brazil  Canada   China  Denmark  Hong Kong  \\\n",
       "Date                                                                            \n",
       "2010-01-04   0.913300  1.441900  1.720000  1.0377  6.8273   5.1597   7.755500   \n",
       "2010-01-05   0.914300  1.440200  1.729600  1.0371  6.8258   5.1668   7.756400   \n",
       "2010-01-06   0.918900  1.440400  1.729200  1.0333  6.8272   5.1638   7.754600   \n",
       "2010-01-07   0.916800  1.431400  1.740900  1.0351  6.8280   5.1981   7.753900   \n",
       "2010-01-08   0.921800  1.435700  1.734200  1.0345  6.8274   5.1827   7.755300   \n",
       "...               ...       ...       ...     ...     ...      ...        ...   \n",
       "2019-12-27   0.697800  1.117400  4.050700  1.3073  6.9954   6.6829   7.787400   \n",
       "2019-12-28   0.698667  1.118833  4.038867  1.3068  6.9924   6.6749   7.786833   \n",
       "2019-12-29   0.699533  1.120267  4.027033  1.3063  6.9894   6.6669   7.786267   \n",
       "2019-12-30   0.700400  1.121700  4.015200  1.3058  6.9864   6.6589   7.785700   \n",
       "2019-12-31   0.703000  1.122700  4.019000  1.2962  6.9618   6.6554   7.789400   \n",
       "\n",
       "            India       Japan  Malaysia  \n",
       "Date                                     \n",
       "2010-01-04  46.27   92.550000    3.3960  \n",
       "2010-01-05  46.13   91.480000    3.3850  \n",
       "2010-01-06  45.72   92.530000    3.3790  \n",
       "2010-01-07  45.67   93.310000    3.3680  \n",
       "2010-01-08  45.50   92.700000    3.3750  \n",
       "...           ...         ...       ...  \n",
       "2019-12-27  71.45  109.470000    4.1260  \n",
       "2019-12-28  71.40  109.263333    4.1191  \n",
       "2019-12-29  71.35  109.056667    4.1122  \n",
       "2019-12-30  71.30  108.850000    4.1053  \n",
       "2019-12-31  71.36  108.670000    4.0918  \n",
       "\n",
       "[3648 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "id": "646f53c7",
=======
   "execution_count": 15,
   "id": "ab94630e",
>>>>>>> 1ce5a549fd7b813ed9c4e92a5fc64e67e3973b50
   "metadata": {},
   "outputs": [],
   "source": [
    "LOOK_BACK = 30\n",
    "PREDICT_DAY = 1\n",
    "SPLIT_RATIO = 0.8"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
   "id": "03455b20",
=======
   "execution_count": 16,
   "id": "673eb188",
>>>>>>> 1ce5a549fd7b813ed9c4e92a5fc64e67e3973b50
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Data(data, lookback = LOOK_BACK, pred_len = PREDICT_DAY, split_ratio = SPLIT_RATIO, model = \"FNN\"):\n",
    "    if lookback<2:\n",
    "        print(\"ERROR: Lookback too small\")\n",
    "        return -1\n",
    "\n",
    "# declarations\n",
    "\n",
    "    x = {}\n",
    "    y = {}\n",
    "    xtr = {}\n",
    "    xt = {}\n",
    "    ytr = {}\n",
    "    yt = {}\n",
    "    scalers = {}\n",
    "\n",
    "# Creating stepped data\n",
    "\n",
    "    for i in data.columns:\n",
    "        xtemp = pd.DataFrame(data[i])\n",
    "        for j in range(1,lookback+1):\n",
    "            xtemp[i+str(j)] = data[i].shift(-1*j)\n",
    "        x[i] = xtemp.dropna()\n",
    "\n",
    "# Splitting data into x and y\n",
    "        \n",
    "    for i in x.keys():\n",
    "        y[i] = pd.DataFrame(x[i].iloc[:,-pred_len])\n",
    "        x[i] = x[i].iloc[:,:-pred_len]\n",
    "        \n",
    "# Normalizing x and y values\n",
    "        \n",
    "    for i in x.keys():\n",
    "        scalers[i+\"_x\"] = MinMaxScaler(feature_range=(0,1))\n",
    "        x[i] = scalers[i+\"_x\"].fit_transform(x[i])\n",
    "        scalers[i+\"_y\"] = MinMaxScaler(feature_range=(0,1))\n",
    "        y[i] = scalers[i+\"_y\"].fit_transform(y[i])\n",
    "\n",
    "# setting train and test sizes\n",
    "        \n",
    "    tr_len = int(split_ratio*y[\"India\"].shape[0])\n",
    "    t_len = y[\"India\"].shape[0]-tr_len\n",
    "    \n",
    "# creating training and testing data\n",
    "    \n",
    "    for i in x.keys():\n",
    "        xtr[i] = x[i][:tr_len]\n",
    "        ytr[i] = y[i][:tr_len]\n",
    "        xt[i] = x[i][-t_len:]\n",
    "        yt[i] = y[i][-t_len:]\n",
    "        \n",
    "# returning pertinent data\n",
    "    \n",
    "    return x,y,xtr,xt,ytr,yt,scalers"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "id": "a44da24a",
=======
   "execution_count": 17,
   "id": "6fc04bda",
>>>>>>> 1ce5a549fd7b813ed9c4e92a5fc64e67e3973b50
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,xtr,xt,ytr,yt,scalers = Create_Data(data, model=\"RNN\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "id": "61d652d3",
=======
   "execution_count": 18,
   "id": "333787a1",
>>>>>>> 1ce5a549fd7b813ed9c4e92a5fc64e67e3973b50
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_model(x,y, lookback = LOOK_BACK, Pred_size = PREDICT_DAY):\n",
    "    models = {}\n",
    "    for i in x.keys():\n",
    "        models[i] = Sequential()\n",
    "        models[i].add(GRU(32,input_shape=(LOOK_BACK,1,),return_sequences=True,activation=\"relu\"))\n",
    "        models[i].add(GRU(64,return_sequences=True,activation=\"relu\"))\n",
    "        models[i].add(Dropout(0.2))\n",
    "        models[i].add(GRU(128,return_sequences=True,activation=\"relu\"))\n",
    "        models[i].add(Dropout(0.2))\n",
    "        models[i].add(GRU(64,return_sequences=True,activation=\"relu\"))\n",
    "        models[i].add(Dropout(0.2))\n",
    "        models[i].add(GRU(16,activation=\"relu\"))\n",
    "        models[i].add(Dense(Pred_size))\n",
    "        models[i].compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "        print(i)\n",
<<<<<<< HEAD
=======
    "        display(models[i].summary())\n",
>>>>>>> 1ce5a549fd7b813ed9c4e92a5fc64e67e3973b50
    "    return models"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
   "id": "b9df8199",
=======
   "execution_count": 19,
   "id": "f2df7ede",
>>>>>>> 1ce5a549fd7b813ed9c4e92a5fc64e67e3973b50
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 11:14:03.110719: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:14:03.134332: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:14:03.134752: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:14:03.137409: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:14:03.137837: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:14:03.138143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:14:03.239089: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:14:03.239322: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:14:03.239504: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:14:03.239647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4807 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2023-10-05 11:14:03.240396: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
=======
      "Australia\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 30, 32)            3360      \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 30, 64)            18816     \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 30, 128)           74496     \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 30, 64)            37248     \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_4 (GRU)                 (None, 16)                3936      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137,873\n",
      "Trainable params: 137,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
>>>>>>> 1ce5a549fd7b813ed9c4e92a5fc64e67e3973b50
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "WARNING:tensorflow:Layer gru_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Australia\n",
      "WARNING:tensorflow:Layer gru_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Europe\n",
      "WARNING:tensorflow:Layer gru_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Brazil\n",
      "WARNING:tensorflow:Layer gru_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Canada\n",
      "WARNING:tensorflow:Layer gru_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "China\n",
      "WARNING:tensorflow:Layer gru_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Denmark\n",
      "WARNING:tensorflow:Layer gru_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Hong Kong\n",
      "WARNING:tensorflow:Layer gru_35 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_37 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_38 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_39 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "India\n",
      "WARNING:tensorflow:Layer gru_40 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_41 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_42 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_43 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_44 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Japan\n",
      "WARNING:tensorflow:Layer gru_45 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_46 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_47 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Malaysia\n"
     ]
=======
      "Europe\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_5 (GRU)                 (None, 30, 32)            3360      \n",
      "                                                                 \n",
      " gru_6 (GRU)                 (None, 30, 64)            18816     \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_7 (GRU)                 (None, 30, 128)           74496     \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_8 (GRU)                 (None, 30, 64)            37248     \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_9 (GRU)                 (None, 16)                3936      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137,873\n",
      "Trainable params: 137,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brazil\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_10 (GRU)                (None, 30, 32)            3360      \n",
      "                                                                 \n",
      " gru_11 (GRU)                (None, 30, 64)            18816     \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_12 (GRU)                (None, 30, 128)           74496     \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_13 (GRU)                (None, 30, 64)            37248     \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_14 (GRU)                (None, 16)                3936      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137,873\n",
      "Trainable params: 137,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canada\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_15 (GRU)                (None, 30, 32)            3360      \n",
      "                                                                 \n",
      " gru_16 (GRU)                (None, 30, 64)            18816     \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_17 (GRU)                (None, 30, 128)           74496     \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_18 (GRU)                (None, 30, 64)            37248     \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_19 (GRU)                (None, 16)                3936      \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137,873\n",
      "Trainable params: 137,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "China\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_20 (GRU)                (None, 30, 32)            3360      \n",
      "                                                                 \n",
      " gru_21 (GRU)                (None, 30, 64)            18816     \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_22 (GRU)                (None, 30, 128)           74496     \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_23 (GRU)                (None, 30, 64)            37248     \n",
      "                                                                 \n",
      " dropout_44 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_24 (GRU)                (None, 16)                3936      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137,873\n",
      "Trainable params: 137,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denmark\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_25 (GRU)                (None, 30, 32)            3360      \n",
      "                                                                 \n",
      " gru_26 (GRU)                (None, 30, 64)            18816     \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_27 (GRU)                (None, 30, 128)           74496     \n",
      "                                                                 \n",
      " dropout_46 (Dropout)        (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_28 (GRU)                (None, 30, 64)            37248     \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_29 (GRU)                (None, 16)                3936      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137,873\n",
      "Trainable params: 137,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hong Kong\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_30 (GRU)                (None, 30, 32)            3360      \n",
      "                                                                 \n",
      " gru_31 (GRU)                (None, 30, 64)            18816     \n",
      "                                                                 \n",
      " dropout_48 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_32 (GRU)                (None, 30, 128)           74496     \n",
      "                                                                 \n",
      " dropout_49 (Dropout)        (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_33 (GRU)                (None, 30, 64)            37248     \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_34 (GRU)                (None, 16)                3936      \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137,873\n",
      "Trainable params: 137,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_35 (GRU)                (None, 30, 32)            3360      \n",
      "                                                                 \n",
      " gru_36 (GRU)                (None, 30, 64)            18816     \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_37 (GRU)                (None, 30, 128)           74496     \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_38 (GRU)                (None, 30, 64)            37248     \n",
      "                                                                 \n",
      " dropout_53 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_39 (GRU)                (None, 16)                3936      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137,873\n",
      "Trainable params: 137,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Japan\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_40 (GRU)                (None, 30, 32)            3360      \n",
      "                                                                 \n",
      " gru_41 (GRU)                (None, 30, 64)            18816     \n",
      "                                                                 \n",
      " dropout_54 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_42 (GRU)                (None, 30, 128)           74496     \n",
      "                                                                 \n",
      " dropout_55 (Dropout)        (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_43 (GRU)                (None, 30, 64)            37248     \n",
      "                                                                 \n",
      " dropout_56 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_44 (GRU)                (None, 16)                3936      \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137,873\n",
      "Trainable params: 137,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malaysia\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_45 (GRU)                (None, 30, 32)            3360      \n",
      "                                                                 \n",
      " gru_46 (GRU)                (None, 30, 64)            18816     \n",
      "                                                                 \n",
      " dropout_57 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_47 (GRU)                (None, 30, 128)           74496     \n",
      "                                                                 \n",
      " dropout_58 (Dropout)        (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_48 (GRU)                (None, 30, 64)            37248     \n",
      "                                                                 \n",
      " dropout_59 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_49 (GRU)                (None, 16)                3936      \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137,873\n",
      "Trainable params: 137,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
>>>>>>> 1ce5a549fd7b813ed9c4e92a5fc64e67e3973b50
    }
   ],
   "source": [
    "m = Create_model(x,y)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
   "id": "ac49b394",
=======
   "execution_count": 20,
   "id": "599dc5fc",
>>>>>>> 1ce5a549fd7b813ed9c4e92a5fc64e67e3973b50
   "metadata": {},
   "outputs": [],
   "source": [
    "def Execute_model(model,xtr,ytr,xt,yt, scaler):\n",
<<<<<<< HEAD
    "        MAPE = {}\n",
    "        MAE = {}\n",
    "        MSE = {}\n",
    "        for i in model.keys():\n",
    "                print(i)\n",
    "                # Creating EarlyStopping and ReduceLROnPlateau callbacks\n",
    "                es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "                reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "                \n",
    "                # Training the model with EarlyStopping and ReduceLROnPlateau callbacks\n",
    "                model[i].fit(xtr[i], ytr[i], epochs=100, batch_size=64, verbose=1, validation_split=0.2, callbacks=[es, reduce_lr])\n",
    "                \n",
    "                # collecting predicted and actual values\n",
    "                temp = model[i].predict(xt[i])\n",
    "                pred = scaler[i+\"_y\"].inverse_transform(temp)\n",
    "                act = scaler[i+\"_y\"].inverse_transform(yt[i])\n",
    "                \n",
    "                # calculating Mean Square Error, Mean Absolute Error, and Mean Absolute Error\n",
    "                MSE[i] = mean_squared_error(act,pred)\n",
    "                MAE[i] = mean_absolute_error(act,pred)\n",
    "                MAPE[i] = mean_absolute_percentage_error(act, pred)\n",
    "                \n",
    "        # Tabulating Data\n",
    "        results = pd.DataFrame([MSE,MAE,MAPE])\n",
    "        results[\"Metric\"] = [\"MSE\",\"MAE\",\"MAPE\"]\n",
    "        results.set_index(\"Metric\",inplace=True)\n",
    "        \n",
    "        return results"
=======
    "    MAPE = {}\n",
    "    MAE = {}\n",
    "    MSE = {}\n",
    "    for i in model.keys():\n",
    "        print(i)\n",
    "# Training the model\n",
    "        \n",
    "        model[i].fit(xtr[i], ytr[i], epochs=10, batch_size=1, verbose=1)\n",
    "        \n",
    "# collecting predicted and actual values\n",
    "        \n",
    "        temp = model[i].predict(xt[i])\n",
    "        pred = scaler[i+\"_y\"].inverse_transform(temp)\n",
    "        act = scaler[i+\"_y\"].inverse_transform(yt[i])\n",
    "        \n",
    "# calculating Mean Square Error, Mean Absolute Error, and Mean Absolute Error\n",
    "\n",
    "        MSE[i] = mean_squared_error(act,pred)\n",
    "        MAE[i] = mean_absolute_error(act,pred)\n",
    "        MAPE[i] = mean_absolute_percentage_error(act, pred)\n",
    "        \n",
    "# Tabulating Data\n",
    "\n",
    "    results = pd.DataFrame([MSE,MAE,MAPE])\n",
    "    results[\"Metric\"] = [\"MSE\",\"MAE\",\"MAPE\"]\n",
    "    results.set_index(\"Metric\",inplace=True)\n",
    "    \n",
    "    return results\n"
>>>>>>> 1ce5a549fd7b813ed9c4e92a5fc64e67e3973b50
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
   "id": "ab6c31d7",
=======
   "execution_count": null,
   "id": "ddd35db5",
>>>>>>> 1ce5a549fd7b813ed9c4e92a5fc64e67e3973b50
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Australia\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 11:14:10.948746: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55582022e0a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-10-05 11:14:10.948778: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2023-10-05 11:14:10.954070: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-10-05 11:14:10.967593: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8902\n",
      "2023-10-05 11:14:11.070412: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 12s 158ms/step - loss: 0.1149 - val_loss: 0.0668 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 6s 151ms/step - loss: 0.0115 - val_loss: 0.0033 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0034 - val_loss: 4.4200e-04 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 6s 149ms/step - loss: 0.0027 - val_loss: 4.2158e-04 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0023 - val_loss: 3.1557e-04 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 0.0023 - val_loss: 3.8697e-04 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 6s 152ms/step - loss: 0.0019 - val_loss: 3.4505e-04 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 6s 152ms/step - loss: 0.0017 - val_loss: 3.2180e-04 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 6s 152ms/step - loss: 0.0017 - val_loss: 7.7808e-04 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 0.0015 - val_loss: 3.2936e-04 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 0.0015 - val_loss: 3.4141e-04 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0015 - val_loss: 3.2143e-04 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0014 - val_loss: 3.2038e-04 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0015 - val_loss: 4.2588e-04 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 0.0014 - val_loss: 2.9183e-04 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0014 - val_loss: 3.3566e-04 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0014 - val_loss: 3.4969e-04 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0014 - val_loss: 2.8679e-04 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0014 - val_loss: 3.1325e-04 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 5s 140ms/step - loss: 0.0014 - val_loss: 3.0532e-04 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0014 - val_loss: 3.5411e-04 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0014 - val_loss: 2.8278e-04 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0013 - val_loss: 2.8825e-04 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0013 - val_loss: 3.0705e-04 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 0.0014 - val_loss: 3.0419e-04 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0013 - val_loss: 2.9650e-04 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0014 - val_loss: 3.1633e-04 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 6s 149ms/step - loss: 0.0013 - val_loss: 3.2854e-04 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 5s 148ms/step - loss: 0.0014 - val_loss: 3.5491e-04 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 0.0013 - val_loss: 2.7775e-04 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 5s 148ms/step - loss: 0.0013 - val_loss: 2.5828e-04 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 5s 140ms/step - loss: 0.0013 - val_loss: 2.9022e-04 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 5s 148ms/step - loss: 0.0013 - val_loss: 2.8491e-04 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0013 - val_loss: 2.6832e-04 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0013 - val_loss: 2.5537e-04 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0013 - val_loss: 2.6742e-04 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0013 - val_loss: 3.1307e-04 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 0.0012 - val_loss: 3.6494e-04 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0013 - val_loss: 2.8521e-04 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 5s 148ms/step - loss: 0.0013 - val_loss: 2.4293e-04 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0012 - val_loss: 3.3672e-04 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0012 - val_loss: 2.7049e-04 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0013 - val_loss: 2.3985e-04 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 6s 148ms/step - loss: 0.0013 - val_loss: 2.6762e-04 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 0.0012 - val_loss: 2.5204e-04 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0012 - val_loss: 2.6846e-04 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 0.0012 - val_loss: 2.4470e-04 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0012 - val_loss: 2.9567e-04 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 6s 148ms/step - loss: 0.0012 - val_loss: 2.3718e-04 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0012 - val_loss: 2.8426e-04 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0012 - val_loss: 2.3328e-04 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 0.0012 - val_loss: 2.9457e-04 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 6s 149ms/step - loss: 0.0012 - val_loss: 2.3769e-04 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0012 - val_loss: 2.8261e-04 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0012 - val_loss: 3.9251e-04 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 6s 150ms/step - loss: 0.0011 - val_loss: 2.1917e-04 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0012 - val_loss: 2.2269e-04 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0012 - val_loss: 2.2222e-04 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 5s 140ms/step - loss: 0.0011 - val_loss: 2.3311e-04 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 0.0011 - val_loss: 2.0505e-04 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0012 - val_loss: 2.0490e-04 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0011 - val_loss: 2.2258e-04 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0011 - val_loss: 2.1325e-04 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0011 - val_loss: 2.5059e-04 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0011 - val_loss: 2.5582e-04 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0011 - val_loss: 2.2391e-04 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 5s 139ms/step - loss: 0.0011 - val_loss: 2.4802e-04 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0012 - val_loss: 2.8206e-04 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 0.0011 - val_loss: 2.1534e-04 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0011 - val_loss: 2.2905e-04 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0011 - val_loss: 1.9177e-04 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0011 - val_loss: 2.0249e-04 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0011 - val_loss: 1.9236e-04 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0011 - val_loss: 2.0348e-04 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 0.0011 - val_loss: 1.9934e-04 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 0.0011 - val_loss: 3.0805e-04 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0011 - val_loss: 1.8364e-04 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0010 - val_loss: 2.5389e-04 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0011 - val_loss: 2.7444e-04 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 0.0011 - val_loss: 2.0635e-04 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0011 - val_loss: 1.9446e-04 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 9.9163e-04 - val_loss: 2.0126e-04 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0010 - val_loss: 2.7675e-04 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0011 - val_loss: 2.3231e-04 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0010 - val_loss: 2.0456e-04 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0010 - val_loss: 2.3016e-04 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 5s 140ms/step - loss: 0.0010 - val_loss: 1.7186e-04 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0010 - val_loss: 1.6979e-04 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 0.0010 - val_loss: 2.3338e-04 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0010 - val_loss: 1.8881e-04 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0010 - val_loss: 1.9114e-04 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0010 - val_loss: 2.0891e-04 - lr: 1.0000e-04\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 9.4487e-04 - val_loss: 1.6560e-04 - lr: 1.0000e-04\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - 5s 140ms/step - loss: 9.8849e-04 - val_loss: 2.1377e-04 - lr: 1.0000e-04\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 9.5232e-04 - val_loss: 1.6971e-04 - lr: 1.0000e-04\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 9.9154e-04 - val_loss: 2.5412e-04 - lr: 1.0000e-04\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 9.7738e-04 - val_loss: 1.7078e-04 - lr: 1.0000e-04\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 9.5689e-04 - val_loss: 2.9642e-04 - lr: 1.0000e-04\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 9.4190e-04 - val_loss: 1.6273e-04 - lr: 1.0000e-04\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 9.2113e-04 - val_loss: 1.5705e-04 - lr: 1.0000e-04\n",
      "23/23 [==============================] - 1s 28ms/step\n",
      "Europe\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 11s 152ms/step - loss: 0.0930 - val_loss: 0.0624 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 0.0090 - val_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0033 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 149ms/step - loss: 0.0026 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0023 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0023 - val_loss: 8.1375e-04 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0021 - val_loss: 9.1695e-04 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0019 - val_loss: 8.6921e-04 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0019 - val_loss: 8.9425e-04 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 6s 148ms/step - loss: 0.0017 - val_loss: 7.9926e-04 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0017 - val_loss: 6.3800e-04 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0017 - val_loss: 7.5869e-04 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 0.0015 - val_loss: 7.6350e-04 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0015 - val_loss: 6.6569e-04 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0015 - val_loss: 6.0142e-04 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0013 - val_loss: 7.2631e-04 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0013 - val_loss: 6.4404e-04 - lr: 2.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0013 - val_loss: 7.1932e-04 - lr: 2.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 0.0014 - val_loss: 5.9110e-04 - lr: 2.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0013 - val_loss: 6.3214e-04 - lr: 2.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0013 - val_loss: 6.5167e-04 - lr: 2.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 0.0013 - val_loss: 6.3887e-04 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0014 - val_loss: 6.4796e-04 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0013 - val_loss: 7.3026e-04 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0013 - val_loss: 6.9745e-04 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0012 - val_loss: 7.2172e-04 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0013 - val_loss: 6.1465e-04 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0013 - val_loss: 6.1954e-04 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0013 - val_loss: 6.2595e-04 - lr: 1.0000e-04\n",
      "Epoch 29: early stopping\n",
      "23/23 [==============================] - 1s 27ms/step\n",
      "Brazil\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 12s 171ms/step - loss: 0.0427 - val_loss: 0.0079 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0024 - val_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 6s 158ms/step - loss: 0.0014 - val_loss: 3.9174e-04 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0010 - val_loss: 3.6201e-04 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 148ms/step - loss: 9.4737e-04 - val_loss: 3.6527e-04 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 8.1236e-04 - val_loss: 6.4986e-04 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 7.3444e-04 - val_loss: 6.4848e-04 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 139ms/step - loss: 7.1946e-04 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 7.3195e-04 - val_loss: 3.7859e-04 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 6.2399e-04 - val_loss: 4.4048e-04 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 6.2869e-04 - val_loss: 3.8043e-04 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 5.9669e-04 - val_loss: 3.8265e-04 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 6.3970e-04 - val_loss: 3.7182e-04 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 5.5940e-04 - val_loss: 3.9675e-04 - lr: 1.0000e-04\n",
      "Epoch 14: early stopping\n",
      "23/23 [==============================] - 1s 32ms/step\n",
      "Canada\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 12s 161ms/step - loss: 0.0465 - val_loss: 0.0056 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0029 - val_loss: 6.4081e-04 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0017 - val_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0012 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0011 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0011 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 140ms/step - loss: 0.0011 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 9.5863e-04 - val_loss: 6.9128e-04 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 8.7086e-04 - val_loss: 7.4379e-04 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 140ms/step - loss: 8.3026e-04 - val_loss: 0.0010 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 8.3769e-04 - val_loss: 8.3152e-04 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 8.0824e-04 - val_loss: 0.0015 - lr: 2.0000e-04\n",
      "Epoch 12: early stopping\n",
      "23/23 [==============================] - 1s 27ms/step\n",
      "China\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 12s 160ms/step - loss: 0.0385 - val_loss: 0.0079 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0017 - val_loss: 8.6125e-04 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 140ms/step - loss: 9.0734e-04 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 7.2152e-04 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 6.4496e-04 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 5.9291e-04 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 5.4497e-04 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 6s 148ms/step - loss: 4.7776e-04 - val_loss: 0.0012 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 4.8674e-04 - val_loss: 0.0014 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 4.8104e-04 - val_loss: 0.0013 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 4.5921e-04 - val_loss: 0.0016 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 4.6390e-04 - val_loss: 0.0014 - lr: 2.0000e-04\n",
      "Epoch 12: early stopping\n",
      "23/23 [==============================] - 1s 30ms/step\n",
      "Denmark\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 12s 163ms/step - loss: 0.0535 - val_loss: 0.0081 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 6s 150ms/step - loss: 0.0033 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0023 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0020 - val_loss: 0.0010 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0019 - val_loss: 7.0652e-04 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0018 - val_loss: 9.9428e-04 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0016 - val_loss: 8.2494e-04 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 148ms/step - loss: 0.0016 - val_loss: 7.2779e-04 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 6s 152ms/step - loss: 0.0015 - val_loss: 7.1557e-04 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0015 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0015 - val_loss: 8.1398e-04 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 148ms/step - loss: 0.0013 - val_loss: 6.8710e-04 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0012 - val_loss: 7.1327e-04 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 148ms/step - loss: 0.0013 - val_loss: 7.1122e-04 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0013 - val_loss: 7.3023e-04 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 0.0014 - val_loss: 6.9047e-04 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0014 - val_loss: 7.2197e-04 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0013 - val_loss: 7.3976e-04 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0013 - val_loss: 8.1763e-04 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0013 - val_loss: 7.2377e-04 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0013 - val_loss: 6.8857e-04 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0013 - val_loss: 6.9428e-04 - lr: 1.0000e-04\n",
      "Epoch 22: early stopping\n",
      "23/23 [==============================] - 1s 24ms/step\n",
      "Hong Kong\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 12s 166ms/step - loss: 0.0123 - val_loss: 0.0031 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 0.0030 - val_loss: 0.0043 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0024 - val_loss: 0.0025 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0022 - val_loss: 0.0025 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0019 - val_loss: 0.0028 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 0.0018 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 0.0017 - val_loss: 0.0035 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0015 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 0.0014 - val_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0013 - val_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0011 - val_loss: 9.8551e-04 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0011 - val_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0011 - val_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 0.0010 - val_loss: 9.3775e-04 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 139ms/step - loss: 9.8965e-04 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0010 - val_loss: 0.0037 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 8.9215e-04 - val_loss: 0.0018 - lr: 2.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 6s 151ms/step - loss: 8.6952e-04 - val_loss: 0.0020 - lr: 2.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 6s 154ms/step - loss: 8.3731e-04 - val_loss: 0.0018 - lr: 2.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 6s 152ms/step - loss: 8.4006e-04 - val_loss: 0.0018 - lr: 2.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 8.8056e-04 - val_loss: 0.0012 - lr: 2.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 6s 150ms/step - loss: 9.2786e-04 - val_loss: 0.0017 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 6s 148ms/step - loss: 7.8869e-04 - val_loss: 0.0017 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 8.8457e-04 - val_loss: 0.0016 - lr: 1.0000e-04\n",
      "Epoch 24: early stopping\n",
      "23/23 [==============================] - 1s 35ms/step\n",
      "India\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 12s 169ms/step - loss: 0.0548 - val_loss: 0.0032 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 148ms/step - loss: 0.0023 - val_loss: 1.9665e-04 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0014 - val_loss: 0.0010 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 6s 149ms/step - loss: 0.0013 - val_loss: 1.5735e-04 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 6s 152ms/step - loss: 0.0011 - val_loss: 5.8708e-04 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0011 - val_loss: 2.2473e-04 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 9.6318e-04 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 148ms/step - loss: 9.3098e-04 - val_loss: 5.1275e-04 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 8.9026e-04 - val_loss: 9.4176e-04 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 8.7151e-04 - val_loss: 4.8349e-04 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 8.4126e-04 - val_loss: 5.2658e-04 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 8.0278e-04 - val_loss: 5.4412e-04 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 6s 153ms/step - loss: 7.9744e-04 - val_loss: 4.7639e-04 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 7.8355e-04 - val_loss: 7.7350e-04 - lr: 1.0000e-04\n",
      "Epoch 14: early stopping\n",
      "23/23 [==============================] - 1s 25ms/step\n",
      "Japan\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 11s 155ms/step - loss: 0.0587 - val_loss: 0.0075 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 6s 149ms/step - loss: 0.0036 - val_loss: 0.0010 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0024 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 0.0019 - val_loss: 9.3814e-04 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0017 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0014 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0013 - val_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 139ms/step - loss: 0.0011 - val_loss: 0.0013 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0012 - val_loss: 0.0012 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0011 - val_loss: 0.0016 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0011 - val_loss: 0.0013 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 140ms/step - loss: 0.0011 - val_loss: 0.0012 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0010 - val_loss: 0.0016 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0011 - val_loss: 0.0015 - lr: 1.0000e-04\n",
      "Epoch 14: early stopping\n",
      "23/23 [==============================] - 1s 24ms/step\n",
      "Malaysia\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 12s 156ms/step - loss: 0.0317 - val_loss: 0.0263 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0020 - val_loss: 4.5755e-04 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0013 - val_loss: 3.9089e-04 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 6s 149ms/step - loss: 0.0011 - val_loss: 6.1381e-04 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 6s 150ms/step - loss: 0.0012 - val_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 9.4459e-04 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 9.0863e-04 - val_loss: 0.0053 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 149ms/step - loss: 0.0010 - val_loss: 7.0940e-04 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 8.7527e-04 - val_loss: 0.0013 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 6s 148ms/step - loss: 8.0359e-04 - val_loss: 0.0011 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 8.7158e-04 - val_loss: 0.0016 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 8.5844e-04 - val_loss: 6.4833e-04 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 8.5865e-04 - val_loss: 0.0018 - lr: 1.0000e-04\n",
      "Epoch 13: early stopping\n",
      "23/23 [==============================] - 1s 24ms/step\n"
=======
      "Australia\n",
      "Epoch 1/10\n",
      "2894/2894 [==============================] - 53s 17ms/step - loss: 0.0051\n",
      "Epoch 2/10\n",
      "2894/2894 [==============================] - 55s 19ms/step - loss: 0.0019\n",
      "Epoch 3/10\n",
      "2894/2894 [==============================] - 55s 19ms/step - loss: 0.0014\n",
      "Epoch 4/10\n",
      "2894/2894 [==============================] - 55s 19ms/step - loss: 0.0013\n",
      "Epoch 5/10\n",
      "2894/2894 [==============================] - 55s 19ms/step - loss: 0.0012\n",
      "Epoch 6/10\n",
      "2894/2894 [==============================] - 55s 19ms/step - loss: 0.0011\n",
      "Epoch 7/10\n",
      "2894/2894 [==============================] - 55s 19ms/step - loss: 9.1700e-04\n",
      "Epoch 8/10\n",
      "2894/2894 [==============================] - 57s 20ms/step - loss: 8.6619e-04\n",
      "Epoch 9/10\n",
      "2894/2894 [==============================] - 55s 19ms/step - loss: 8.7329e-04\n",
      "Epoch 10/10\n",
      "2894/2894 [==============================] - 55s 19ms/step - loss: 7.8355e-04\n",
      "23/23 [==============================] - 1s 9ms/step\n",
      "Europe\n",
      "Epoch 1/10\n",
      "2894/2894 [==============================] - 58s 19ms/step - loss: 0.0043\n",
      "Epoch 2/10\n",
      "2894/2894 [==============================] - 56s 19ms/step - loss: 0.0018\n",
      "Epoch 3/10\n",
      "2894/2894 [==============================] - 56s 19ms/step - loss: 0.0014\n",
      "Epoch 4/10\n",
      "2894/2894 [==============================] - 56s 19ms/step - loss: 0.0012\n",
      "Epoch 5/10\n",
      "2894/2894 [==============================] - 56s 19ms/step - loss: 0.0010\n",
      "Epoch 6/10\n",
      "2894/2894 [==============================] - 58s 20ms/step - loss: 9.2133e-04\n",
      "Epoch 7/10\n",
      "2894/2894 [==============================] - 58s 20ms/step - loss: 8.5641e-04\n",
      "Epoch 8/10\n",
      "2894/2894 [==============================] - 60s 21ms/step - loss: 8.5200e-04\n",
      "Epoch 9/10\n",
      "2894/2894 [==============================] - 57s 20ms/step - loss: 7.5083e-04\n",
      "Epoch 10/10\n",
      "2894/2894 [==============================] - 61s 21ms/step - loss: 7.3358e-04\n",
      "23/23 [==============================] - 1s 10ms/step\n",
      "Brazil\n",
      "Epoch 1/10\n",
      "2894/2894 [==============================] - 65s 21ms/step - loss: 0.0031\n",
      "Epoch 2/10\n",
      "2894/2894 [==============================] - 60s 21ms/step - loss: 0.0012\n",
      "Epoch 3/10\n",
      "2351/2894 [=======================>......] - ETA: 11s - loss: 9.6029e-04"
>>>>>>> 1ce5a549fd7b813ed9c4e92a5fc64e67e3973b50
     ]
    }
   ],
   "source": [
    "result = Execute_model(m,xtr,ytr,xt,yt,scalers)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "id": "7343e142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>0.000124</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.009242</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>0.003952</td>\n",
       "      <td>0.001586</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>2.294304</td>\n",
       "      <td>1.649837</td>\n",
       "      <td>0.002425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.008015</td>\n",
       "      <td>0.006034</td>\n",
       "      <td>0.076209</td>\n",
       "      <td>0.017116</td>\n",
       "      <td>0.049505</td>\n",
       "      <td>0.032226</td>\n",
       "      <td>0.013362</td>\n",
       "      <td>1.376090</td>\n",
       "      <td>1.123847</td>\n",
       "      <td>0.046184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE</th>\n",
       "      <td>0.011438</td>\n",
       "      <td>0.005189</td>\n",
       "      <td>0.019576</td>\n",
       "      <td>0.012992</td>\n",
       "      <td>0.007234</td>\n",
       "      <td>0.004993</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>0.019652</td>\n",
       "      <td>0.010213</td>\n",
       "      <td>0.011257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Australia    Europe    Brazil    Canada     China   Denmark  \\\n",
       "Metric                                                                \n",
       "MSE      0.000124  0.000064  0.009242  0.000373  0.003952  0.001586   \n",
       "MAE      0.008015  0.006034  0.076209  0.017116  0.049505  0.032226   \n",
       "MAPE     0.011438  0.005189  0.019576  0.012992  0.007234  0.004993   \n",
       "\n",
       "        Hong Kong     India     Japan  Malaysia  \n",
       "Metric                                           \n",
       "MSE      0.000198  2.294304  1.649837  0.002425  \n",
       "MAE      0.013362  1.376090  1.123847  0.046184  \n",
       "MAPE     0.001704  0.019652  0.010213  0.011257  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "id": "76c1e7a1",
   "metadata": {},
   "outputs": [],
>>>>>>> 1ce5a549fd7b813ed9c4e92a5fc64e67e3973b50
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.11.5"
=======
   "version": "3.10.6"
>>>>>>> 1ce5a549fd7b813ed9c4e92a5fc64e67e3973b50
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
