{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "063947c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 13:23:45.692225: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/lib/python3.11/site-packages/h5py/__init__.py:36: UserWarning: h5py is running against HDF5 1.14.2 when it was built against 1.14.1, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Conv1D,\n",
    "    LSTM,\n",
    "    MaxPooling1D,\n",
    "    SimpleRNN,\n",
    "    Flatten,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_squared_error,\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8840a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>0.9133</td>\n",
       "      <td>1.4419</td>\n",
       "      <td>1.7200</td>\n",
       "      <td>1.0377</td>\n",
       "      <td>6.8273</td>\n",
       "      <td>5.1597</td>\n",
       "      <td>7.7555</td>\n",
       "      <td>46.27</td>\n",
       "      <td>92.55</td>\n",
       "      <td>3.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>0.9143</td>\n",
       "      <td>1.4402</td>\n",
       "      <td>1.7296</td>\n",
       "      <td>1.0371</td>\n",
       "      <td>6.8258</td>\n",
       "      <td>5.1668</td>\n",
       "      <td>7.7564</td>\n",
       "      <td>46.13</td>\n",
       "      <td>91.48</td>\n",
       "      <td>3.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>0.9189</td>\n",
       "      <td>1.4404</td>\n",
       "      <td>1.7292</td>\n",
       "      <td>1.0333</td>\n",
       "      <td>6.8272</td>\n",
       "      <td>5.1638</td>\n",
       "      <td>7.7546</td>\n",
       "      <td>45.72</td>\n",
       "      <td>92.53</td>\n",
       "      <td>3.379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>0.9168</td>\n",
       "      <td>1.4314</td>\n",
       "      <td>1.7409</td>\n",
       "      <td>1.0351</td>\n",
       "      <td>6.8280</td>\n",
       "      <td>5.1981</td>\n",
       "      <td>7.7539</td>\n",
       "      <td>45.67</td>\n",
       "      <td>93.31</td>\n",
       "      <td>3.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>0.9218</td>\n",
       "      <td>1.4357</td>\n",
       "      <td>1.7342</td>\n",
       "      <td>1.0345</td>\n",
       "      <td>6.8274</td>\n",
       "      <td>5.1827</td>\n",
       "      <td>7.7553</td>\n",
       "      <td>45.50</td>\n",
       "      <td>92.70</td>\n",
       "      <td>3.375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  Australia  Europe  Brazil  Canada   China  \\\n",
       "0             0  2010-01-04     0.9133  1.4419  1.7200  1.0377  6.8273   \n",
       "1             1  2010-01-05     0.9143  1.4402  1.7296  1.0371  6.8258   \n",
       "2             2  2010-01-06     0.9189  1.4404  1.7292  1.0333  6.8272   \n",
       "3             3  2010-01-07     0.9168  1.4314  1.7409  1.0351  6.8280   \n",
       "4             4  2010-01-08     0.9218  1.4357  1.7342  1.0345  6.8274   \n",
       "\n",
       "   Denmark  Hong Kong  India  Japan  Malaysia  \n",
       "0   5.1597     7.7555  46.27  92.55     3.396  \n",
       "1   5.1668     7.7564  46.13  91.48     3.385  \n",
       "2   5.1638     7.7546  45.72  92.53     3.379  \n",
       "3   5.1981     7.7539  45.67  93.31     3.368  \n",
       "4   5.1827     7.7553  45.50  92.70     3.375  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d26a87e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data.drop(\"Unnamed: 0.1\", axis=1, inplace=True)\n",
    "    data.rename(columns={\"Unnamed: 0\": \"Date\"}, inplace=True)\n",
    "    data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
    "    data.set_index(\"Date\", inplace=True)\n",
    "    data.replace(0, np.nan, inplace=True)\n",
    "    display(data)\n",
    "    print(\"Filling missing Values: \")\n",
    "    display(data.interpolate(method=\"linear\", limit_direction=\"forward\"))\n",
    "    data.interpolate(method=\"linear\", limit_direction=\"forward\", inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbe2f008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>0.9133</td>\n",
       "      <td>1.4419</td>\n",
       "      <td>1.7200</td>\n",
       "      <td>1.0377</td>\n",
       "      <td>6.8273</td>\n",
       "      <td>5.1597</td>\n",
       "      <td>7.7555</td>\n",
       "      <td>46.27</td>\n",
       "      <td>92.55</td>\n",
       "      <td>3.3960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.9143</td>\n",
       "      <td>1.4402</td>\n",
       "      <td>1.7296</td>\n",
       "      <td>1.0371</td>\n",
       "      <td>6.8258</td>\n",
       "      <td>5.1668</td>\n",
       "      <td>7.7564</td>\n",
       "      <td>46.13</td>\n",
       "      <td>91.48</td>\n",
       "      <td>3.3850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>0.9189</td>\n",
       "      <td>1.4404</td>\n",
       "      <td>1.7292</td>\n",
       "      <td>1.0333</td>\n",
       "      <td>6.8272</td>\n",
       "      <td>5.1638</td>\n",
       "      <td>7.7546</td>\n",
       "      <td>45.72</td>\n",
       "      <td>92.53</td>\n",
       "      <td>3.3790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>0.9168</td>\n",
       "      <td>1.4314</td>\n",
       "      <td>1.7409</td>\n",
       "      <td>1.0351</td>\n",
       "      <td>6.8280</td>\n",
       "      <td>5.1981</td>\n",
       "      <td>7.7539</td>\n",
       "      <td>45.67</td>\n",
       "      <td>93.31</td>\n",
       "      <td>3.3680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>0.9218</td>\n",
       "      <td>1.4357</td>\n",
       "      <td>1.7342</td>\n",
       "      <td>1.0345</td>\n",
       "      <td>6.8274</td>\n",
       "      <td>5.1827</td>\n",
       "      <td>7.7553</td>\n",
       "      <td>45.50</td>\n",
       "      <td>92.70</td>\n",
       "      <td>3.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>0.6978</td>\n",
       "      <td>1.1174</td>\n",
       "      <td>4.0507</td>\n",
       "      <td>1.3073</td>\n",
       "      <td>6.9954</td>\n",
       "      <td>6.6829</td>\n",
       "      <td>7.7874</td>\n",
       "      <td>71.45</td>\n",
       "      <td>109.47</td>\n",
       "      <td>4.1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>0.7004</td>\n",
       "      <td>1.1217</td>\n",
       "      <td>4.0152</td>\n",
       "      <td>1.3058</td>\n",
       "      <td>6.9864</td>\n",
       "      <td>6.6589</td>\n",
       "      <td>7.7857</td>\n",
       "      <td>71.30</td>\n",
       "      <td>108.85</td>\n",
       "      <td>4.1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>0.7030</td>\n",
       "      <td>1.1227</td>\n",
       "      <td>4.0190</td>\n",
       "      <td>1.2962</td>\n",
       "      <td>6.9618</td>\n",
       "      <td>6.6554</td>\n",
       "      <td>7.7894</td>\n",
       "      <td>71.36</td>\n",
       "      <td>108.67</td>\n",
       "      <td>4.0918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3648 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Australia  Europe  Brazil  Canada   China  Denmark  Hong Kong  \\\n",
       "Date                                                                        \n",
       "2010-01-04     0.9133  1.4419  1.7200  1.0377  6.8273   5.1597     7.7555   \n",
       "2010-01-05     0.9143  1.4402  1.7296  1.0371  6.8258   5.1668     7.7564   \n",
       "2010-01-06     0.9189  1.4404  1.7292  1.0333  6.8272   5.1638     7.7546   \n",
       "2010-01-07     0.9168  1.4314  1.7409  1.0351  6.8280   5.1981     7.7539   \n",
       "2010-01-08     0.9218  1.4357  1.7342  1.0345  6.8274   5.1827     7.7553   \n",
       "...               ...     ...     ...     ...     ...      ...        ...   \n",
       "2019-12-27     0.6978  1.1174  4.0507  1.3073  6.9954   6.6829     7.7874   \n",
       "2019-12-28        NaN     NaN     NaN     NaN     NaN      NaN        NaN   \n",
       "2019-12-29        NaN     NaN     NaN     NaN     NaN      NaN        NaN   \n",
       "2019-12-30     0.7004  1.1217  4.0152  1.3058  6.9864   6.6589     7.7857   \n",
       "2019-12-31     0.7030  1.1227  4.0190  1.2962  6.9618   6.6554     7.7894   \n",
       "\n",
       "            India   Japan  Malaysia  \n",
       "Date                                 \n",
       "2010-01-04  46.27   92.55    3.3960  \n",
       "2010-01-05  46.13   91.48    3.3850  \n",
       "2010-01-06  45.72   92.53    3.3790  \n",
       "2010-01-07  45.67   93.31    3.3680  \n",
       "2010-01-08  45.50   92.70    3.3750  \n",
       "...           ...     ...       ...  \n",
       "2019-12-27  71.45  109.47    4.1260  \n",
       "2019-12-28    NaN     NaN       NaN  \n",
       "2019-12-29    NaN     NaN       NaN  \n",
       "2019-12-30  71.30  108.85    4.1053  \n",
       "2019-12-31  71.36  108.67    4.0918  \n",
       "\n",
       "[3648 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling missing Values: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>0.913300</td>\n",
       "      <td>1.441900</td>\n",
       "      <td>1.720000</td>\n",
       "      <td>1.0377</td>\n",
       "      <td>6.8273</td>\n",
       "      <td>5.1597</td>\n",
       "      <td>7.755500</td>\n",
       "      <td>46.27</td>\n",
       "      <td>92.550000</td>\n",
       "      <td>3.3960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.914300</td>\n",
       "      <td>1.440200</td>\n",
       "      <td>1.729600</td>\n",
       "      <td>1.0371</td>\n",
       "      <td>6.8258</td>\n",
       "      <td>5.1668</td>\n",
       "      <td>7.756400</td>\n",
       "      <td>46.13</td>\n",
       "      <td>91.480000</td>\n",
       "      <td>3.3850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>0.918900</td>\n",
       "      <td>1.440400</td>\n",
       "      <td>1.729200</td>\n",
       "      <td>1.0333</td>\n",
       "      <td>6.8272</td>\n",
       "      <td>5.1638</td>\n",
       "      <td>7.754600</td>\n",
       "      <td>45.72</td>\n",
       "      <td>92.530000</td>\n",
       "      <td>3.3790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>0.916800</td>\n",
       "      <td>1.431400</td>\n",
       "      <td>1.740900</td>\n",
       "      <td>1.0351</td>\n",
       "      <td>6.8280</td>\n",
       "      <td>5.1981</td>\n",
       "      <td>7.753900</td>\n",
       "      <td>45.67</td>\n",
       "      <td>93.310000</td>\n",
       "      <td>3.3680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>0.921800</td>\n",
       "      <td>1.435700</td>\n",
       "      <td>1.734200</td>\n",
       "      <td>1.0345</td>\n",
       "      <td>6.8274</td>\n",
       "      <td>5.1827</td>\n",
       "      <td>7.755300</td>\n",
       "      <td>45.50</td>\n",
       "      <td>92.700000</td>\n",
       "      <td>3.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>0.697800</td>\n",
       "      <td>1.117400</td>\n",
       "      <td>4.050700</td>\n",
       "      <td>1.3073</td>\n",
       "      <td>6.9954</td>\n",
       "      <td>6.6829</td>\n",
       "      <td>7.787400</td>\n",
       "      <td>71.45</td>\n",
       "      <td>109.470000</td>\n",
       "      <td>4.1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-28</th>\n",
       "      <td>0.698667</td>\n",
       "      <td>1.118833</td>\n",
       "      <td>4.038867</td>\n",
       "      <td>1.3068</td>\n",
       "      <td>6.9924</td>\n",
       "      <td>6.6749</td>\n",
       "      <td>7.786833</td>\n",
       "      <td>71.40</td>\n",
       "      <td>109.263333</td>\n",
       "      <td>4.1191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-29</th>\n",
       "      <td>0.699533</td>\n",
       "      <td>1.120267</td>\n",
       "      <td>4.027033</td>\n",
       "      <td>1.3063</td>\n",
       "      <td>6.9894</td>\n",
       "      <td>6.6669</td>\n",
       "      <td>7.786267</td>\n",
       "      <td>71.35</td>\n",
       "      <td>109.056667</td>\n",
       "      <td>4.1122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>0.700400</td>\n",
       "      <td>1.121700</td>\n",
       "      <td>4.015200</td>\n",
       "      <td>1.3058</td>\n",
       "      <td>6.9864</td>\n",
       "      <td>6.6589</td>\n",
       "      <td>7.785700</td>\n",
       "      <td>71.30</td>\n",
       "      <td>108.850000</td>\n",
       "      <td>4.1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>0.703000</td>\n",
       "      <td>1.122700</td>\n",
       "      <td>4.019000</td>\n",
       "      <td>1.2962</td>\n",
       "      <td>6.9618</td>\n",
       "      <td>6.6554</td>\n",
       "      <td>7.789400</td>\n",
       "      <td>71.36</td>\n",
       "      <td>108.670000</td>\n",
       "      <td>4.0918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3648 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Australia    Europe    Brazil  Canada   China  Denmark  Hong Kong  \\\n",
       "Date                                                                            \n",
       "2010-01-04   0.913300  1.441900  1.720000  1.0377  6.8273   5.1597   7.755500   \n",
       "2010-01-05   0.914300  1.440200  1.729600  1.0371  6.8258   5.1668   7.756400   \n",
       "2010-01-06   0.918900  1.440400  1.729200  1.0333  6.8272   5.1638   7.754600   \n",
       "2010-01-07   0.916800  1.431400  1.740900  1.0351  6.8280   5.1981   7.753900   \n",
       "2010-01-08   0.921800  1.435700  1.734200  1.0345  6.8274   5.1827   7.755300   \n",
       "...               ...       ...       ...     ...     ...      ...        ...   \n",
       "2019-12-27   0.697800  1.117400  4.050700  1.3073  6.9954   6.6829   7.787400   \n",
       "2019-12-28   0.698667  1.118833  4.038867  1.3068  6.9924   6.6749   7.786833   \n",
       "2019-12-29   0.699533  1.120267  4.027033  1.3063  6.9894   6.6669   7.786267   \n",
       "2019-12-30   0.700400  1.121700  4.015200  1.3058  6.9864   6.6589   7.785700   \n",
       "2019-12-31   0.703000  1.122700  4.019000  1.2962  6.9618   6.6554   7.789400   \n",
       "\n",
       "            India       Japan  Malaysia  \n",
       "Date                                     \n",
       "2010-01-04  46.27   92.550000    3.3960  \n",
       "2010-01-05  46.13   91.480000    3.3850  \n",
       "2010-01-06  45.72   92.530000    3.3790  \n",
       "2010-01-07  45.67   93.310000    3.3680  \n",
       "2010-01-08  45.50   92.700000    3.3750  \n",
       "...           ...         ...       ...  \n",
       "2019-12-27  71.45  109.470000    4.1260  \n",
       "2019-12-28  71.40  109.263333    4.1191  \n",
       "2019-12-29  71.35  109.056667    4.1122  \n",
       "2019-12-30  71.30  108.850000    4.1053  \n",
       "2019-12-31  71.36  108.670000    4.0918  \n",
       "\n",
       "[3648 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5feeefc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOOK_BACK = 30\n",
    "PREDICT_DAY = 1\n",
    "SPLIT_RATIO = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8c01aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Data(\n",
    "    data, lookback=LOOK_BACK, pred_len=PREDICT_DAY, split_ratio=SPLIT_RATIO, model=\"FNN\"\n",
    "):\n",
    "    if lookback < 2:\n",
    "        print(\"ERROR: Lookback too small\")\n",
    "        return -1\n",
    "\n",
    "    # declarations\n",
    "\n",
    "    x = {}\n",
    "    y = {}\n",
    "    xtr = {}\n",
    "    xt = {}\n",
    "    ytr = {}\n",
    "    yt = {}\n",
    "    scalers = {}\n",
    "\n",
    "    # Creating stepped data\n",
    "\n",
    "    for i in data.columns:\n",
    "        xtemp = pd.DataFrame(data[i])\n",
    "        for j in range(1, lookback + 1):\n",
    "            xtemp[i + str(j)] = data[i].shift(-1 * j)\n",
    "        x[i] = xtemp.dropna()\n",
    "\n",
    "    # Splitting data into x and y\n",
    "\n",
    "    for i in x.keys():\n",
    "        y[i] = pd.DataFrame(x[i].iloc[:, -pred_len])\n",
    "        x[i] = x[i].iloc[:, :-pred_len]\n",
    "\n",
    "    # Normalizing x and y values\n",
    "\n",
    "    for i in x.keys():\n",
    "        scalers[i + \"_x\"] = MinMaxScaler(feature_range=(0, 1))\n",
    "        x[i] = scalers[i + \"_x\"].fit_transform(x[i])\n",
    "        scalers[i + \"_y\"] = MinMaxScaler(feature_range=(0, 1))\n",
    "        y[i] = scalers[i + \"_y\"].fit_transform(y[i])\n",
    "\n",
    "    # setting train and test sizes\n",
    "\n",
    "    tr_len = int(split_ratio * y[\"India\"].shape[0])\n",
    "    t_len = y[\"India\"].shape[0] - tr_len\n",
    "\n",
    "    # creating training and testing data\n",
    "\n",
    "    for i in x.keys():\n",
    "        xtr[i] = x[i][:tr_len]\n",
    "        ytr[i] = y[i][:tr_len]\n",
    "        xt[i] = x[i][-t_len:]\n",
    "        yt[i] = y[i][-t_len:]\n",
    "\n",
    "    # returning pertinent data\n",
    "\n",
    "    return x, y, xtr, xt, ytr, yt, scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1446bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, xtr, xt, ytr, yt, scalers = Create_Data(data, model=\"RNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7862eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_model(x, lookback=LOOK_BACK, pred_size=PREDICT_DAY):\n",
    "    models = {}\n",
    "    for key in x.keys():\n",
    "        input_dim = x[key].shape[\n",
    "            1\n",
    "        ]  # Assuming x[key] is a 2D numpy array, where the second dimension is the feature size\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        # Convolutional Layers\n",
    "        model.add(\n",
    "            Conv1D(\n",
    "                filters=64, kernel_size=7, activation=\"relu\", input_shape=(input_dim, 1)\n",
    "            )\n",
    "        )\n",
    "        model.add(Conv1D(filters=64, kernel_size=7, activation=\"relu\"))\n",
    "        model.add(MaxPooling1D(pool_size=1))\n",
    "        model.add(Conv1D(filters=32, kernel_size=5, activation=\"relu\"))\n",
    "        model.add(Conv1D(filters=32, kernel_size=5, activation=\"relu\"))\n",
    "        model.add(MaxPooling1D(pool_size=1))\n",
    "        model.add(Conv1D(filters=16, kernel_size=3, activation=\"relu\"))\n",
    "        model.add(Conv1D(filters=16, kernel_size=3, activation=\"relu\"))\n",
    "        model.add(MaxPooling1D(pool_size=1))\n",
    "\n",
    "        # Dense Layers\n",
    "        model.add(SimpleRNN(256, return_sequences=True, activation=\"relu\"))\n",
    "        model.add(SimpleRNN(128, return_sequences=True, activation=\"relu\"))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(SimpleRNN(64, return_sequences=True, activation=\"relu\"))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(SimpleRNN(32, activation=\"relu\"))\n",
    "\n",
    "        # Output Layer\n",
    "        model.add(Dense(pred_size, activation=\"linear\"))\n",
    "\n",
    "        model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
    "\n",
    "        models[key] = model\n",
    "\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae99e310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 13:23:47.477853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 13:23:47.505145: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 13:23:47.505573: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 13:23:47.509289: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 13:23:47.509733: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 13:23:47.510055: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 13:23:47.625798: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 13:23:47.626083: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 13:23:47.626290: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 13:23:47.626471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4807 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2023-10-12 13:23:47.627749: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "m = Create_model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01f24237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Execute_model(model, xtr, ytr, xt, yt, scaler):\n",
    "    MAPE = {}\n",
    "    MAE = {}\n",
    "    MSE = {}\n",
    "    for i in model.keys():\n",
    "        print(i)\n",
    "        # Creating EarlyStopping and ReduceLROnPlateau callbacks\n",
    "        es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=1, patience=10)\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", factor=0.2, patience=5, min_lr=0.0001\n",
    "        )\n",
    "\n",
    "        # Training the model with EarlyStopping and ReduceLROnPlateau callbacks\n",
    "        model[i].fit(\n",
    "            xtr[i],\n",
    "            ytr[i],\n",
    "            epochs=100,\n",
    "            batch_size=64,\n",
    "            verbose=1,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[es, reduce_lr],\n",
    "        )\n",
    "\n",
    "        # collecting predicted and actual values\n",
    "        temp = model[i].predict(xt[i])\n",
    "        pred = scaler[i + \"_y\"].inverse_transform(temp)\n",
    "        act = scaler[i + \"_y\"].inverse_transform(yt[i])\n",
    "\n",
    "        # calculating Mean Square Error, Mean Absolute Error, and Mean Absolute Error\n",
    "        MSE[i] = mean_squared_error(act, pred)\n",
    "        MAE[i] = mean_absolute_error(act, pred)\n",
    "        MAPE[i] = mean_absolute_percentage_error(act, pred)\n",
    "\n",
    "    # Tabulating Data\n",
    "    results = pd.DataFrame([MSE, MAE, MAPE])\n",
    "    results[\"Metric\"] = [\"MSE\", \"MAE\", \"MAPE\"]\n",
    "    results.set_index(\"Metric\", inplace=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3100357f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 13:23:53.434635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8902\n",
      "2023-10-12 13:23:54.032012: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f5af1316f40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-10-12 13:23:54.032050: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2023-10-12 13:23:54.038112: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-10-12 13:23:54.176678: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 9s 61ms/step - loss: 0.0774 - val_loss: 8.2781e-04 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0082 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0066 - val_loss: 0.0042 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0065 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0049 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0046 - val_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0048 - val_loss: 0.0012 - lr: 2.0000e-04\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 0.0037 - val_loss: 8.2323e-04 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0037 - val_loss: 7.7073e-04 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0036 - val_loss: 0.0017 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0035 - val_loss: 8.0083e-04 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0033 - val_loss: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0032 - val_loss: 9.9739e-04 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0032 - val_loss: 9.6973e-04 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 0.0032 - val_loss: 9.1063e-04 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 0.0031 - val_loss: 7.2529e-04 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 0.0032 - val_loss: 9.0891e-04 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0031 - val_loss: 6.9490e-04 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0029 - val_loss: 6.9971e-04 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0029 - val_loss: 8.5902e-04 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0029 - val_loss: 9.4927e-04 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 0.0028 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0029 - val_loss: 8.0464e-04 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0027 - val_loss: 9.2296e-04 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0025 - val_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0027 - val_loss: 6.2705e-04 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 0.0026 - val_loss: 7.6325e-04 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0024 - val_loss: 9.8340e-04 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0027 - val_loss: 8.5802e-04 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0025 - val_loss: 8.2518e-04 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0025 - val_loss: 7.5345e-04 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 0.0025 - val_loss: 5.7975e-04 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0024 - val_loss: 8.5400e-04 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0024 - val_loss: 7.0401e-04 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 0.0024 - val_loss: 6.0757e-04 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 1s 33ms/step - loss: 0.0023 - val_loss: 8.4291e-04 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0024 - val_loss: 5.7615e-04 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0022 - val_loss: 6.8498e-04 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0022 - val_loss: 9.6107e-04 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0023 - val_loss: 6.5644e-04 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0022 - val_loss: 5.4699e-04 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0022 - val_loss: 8.0976e-04 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0022 - val_loss: 5.5552e-04 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0021 - val_loss: 6.5487e-04 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0020 - val_loss: 5.1234e-04 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 0.0020 - val_loss: 5.7095e-04 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 0.0021 - val_loss: 7.9647e-04 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0020 - val_loss: 6.3009e-04 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 0.0021 - val_loss: 6.0813e-04 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0019 - val_loss: 6.8354e-04 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0020 - val_loss: 7.1234e-04 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 0.0020 - val_loss: 4.6571e-04 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0018 - val_loss: 6.7432e-04 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0018 - val_loss: 4.7473e-04 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0019 - val_loss: 5.8006e-04 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0018 - val_loss: 6.4440e-04 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0017 - val_loss: 5.3453e-04 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0018 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0017 - val_loss: 4.9543e-04 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0018 - val_loss: 7.5546e-04 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0018 - val_loss: 4.6854e-04 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0016 - val_loss: 5.5497e-04 - lr: 1.0000e-04\n",
      "Epoch 62: early stopping\n",
      "23/23 [==============================] - 1s 22ms/step\n",
      "Europe\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 6s 43ms/step - loss: 0.0534 - val_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 0.0068 - val_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0057 - val_loss: 0.0027 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0051 - val_loss: 0.0031 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0045 - val_loss: 0.0027 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0036 - val_loss: 0.0050 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0043 - val_loss: 0.0023 - lr: 2.0000e-04\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0032 - val_loss: 0.0030 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0033 - val_loss: 0.0023 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0030 - val_loss: 0.0025 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 0.0032 - val_loss: 0.0024 - lr: 2.0000e-04\n",
      "Epoch 11: early stopping\n",
      "23/23 [==============================] - 1s 6ms/step\n",
      "Brazil\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 6s 33ms/step - loss: 0.0233 - val_loss: 0.0030 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0036 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0022 - val_loss: 0.0074 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0026 - val_loss: 0.0042 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0024 - val_loss: 8.5285e-04 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0019 - val_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0018 - val_loss: 7.0809e-04 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0019 - val_loss: 0.0048 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0023 - val_loss: 8.7246e-04 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0012 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0012 - val_loss: 0.0052 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 1s 33ms/step - loss: 0.0015 - val_loss: 4.9129e-04 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0015 - val_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 0.0013 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0012 - val_loss: 7.0906e-04 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0012 - val_loss: 7.5172e-04 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0012 - val_loss: 8.9319e-04 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 9.4498e-04 - val_loss: 7.8313e-04 - lr: 2.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 9.5759e-04 - val_loss: 4.6518e-04 - lr: 2.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 8.9921e-04 - val_loss: 5.1590e-04 - lr: 2.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 9.1149e-04 - val_loss: 4.9032e-04 - lr: 2.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 8.2069e-04 - val_loss: 4.8082e-04 - lr: 2.0000e-04\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 7.5654e-04 - val_loss: 4.9656e-04 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 7.4807e-04 - val_loss: 6.4296e-04 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 7.8496e-04 - val_loss: 4.7470e-04 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 8.0706e-04 - val_loss: 4.4859e-04 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 7.7195e-04 - val_loss: 4.4970e-04 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 8.3974e-04 - val_loss: 4.5250e-04 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 1s 33ms/step - loss: 7.4318e-04 - val_loss: 4.6820e-04 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 7.7098e-04 - val_loss: 4.4663e-04 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 8.1338e-04 - val_loss: 5.2999e-04 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 1s 35ms/step - loss: 7.9242e-04 - val_loss: 4.8376e-04 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 7.7681e-04 - val_loss: 4.3707e-04 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 7.7869e-04 - val_loss: 4.3735e-04 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 8.7252e-04 - val_loss: 5.8272e-04 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 7.8293e-04 - val_loss: 4.6963e-04 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 8.2989e-04 - val_loss: 4.9800e-04 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 7.8337e-04 - val_loss: 4.3843e-04 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 7.7072e-04 - val_loss: 5.7028e-04 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 7.1487e-04 - val_loss: 4.5947e-04 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 6.7919e-04 - val_loss: 4.3061e-04 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 7.7280e-04 - val_loss: 5.4136e-04 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 7.7498e-04 - val_loss: 0.0014 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 7.8456e-04 - val_loss: 6.9819e-04 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 6.8922e-04 - val_loss: 4.2544e-04 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 7.5063e-04 - val_loss: 5.3881e-04 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 8.1012e-04 - val_loss: 5.3634e-04 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 1s 36ms/step - loss: 7.4745e-04 - val_loss: 5.9476e-04 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 7.8768e-04 - val_loss: 4.6850e-04 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 7.4681e-04 - val_loss: 4.4210e-04 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 7.4003e-04 - val_loss: 4.0037e-04 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 6.8625e-04 - val_loss: 4.1724e-04 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 7.4609e-04 - val_loss: 4.3839e-04 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 7.1002e-04 - val_loss: 5.1494e-04 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 7.0775e-04 - val_loss: 6.0275e-04 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 6.9605e-04 - val_loss: 4.1825e-04 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 6.8020e-04 - val_loss: 4.3647e-04 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 7.1004e-04 - val_loss: 4.8633e-04 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 7.0646e-04 - val_loss: 6.3752e-04 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 7.2256e-04 - val_loss: 4.0829e-04 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 7.0085e-04 - val_loss: 3.9900e-04 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 6.3740e-04 - val_loss: 5.0274e-04 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 6.3302e-04 - val_loss: 3.8763e-04 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 6.9511e-04 - val_loss: 6.4649e-04 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 6.9067e-04 - val_loss: 0.0014 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 6.8737e-04 - val_loss: 3.9770e-04 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 1s 33ms/step - loss: 6.6583e-04 - val_loss: 8.8611e-04 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 6.5555e-04 - val_loss: 4.2427e-04 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 6.6663e-04 - val_loss: 3.9240e-04 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 5.9846e-04 - val_loss: 3.8056e-04 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 6.6056e-04 - val_loss: 6.3476e-04 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 7.6375e-04 - val_loss: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 6.8881e-04 - val_loss: 8.7334e-04 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 6.3557e-04 - val_loss: 3.6270e-04 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 1s 33ms/step - loss: 5.8499e-04 - val_loss: 4.2394e-04 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 5.6956e-04 - val_loss: 3.7724e-04 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 6.6077e-04 - val_loss: 3.9163e-04 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 6.4981e-04 - val_loss: 5.3023e-04 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 7.0480e-04 - val_loss: 3.8219e-04 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 5.8532e-04 - val_loss: 5.9109e-04 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 6.6109e-04 - val_loss: 7.1646e-04 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 5.8486e-04 - val_loss: 3.8362e-04 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 6.2789e-04 - val_loss: 4.5377e-04 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 5.9883e-04 - val_loss: 3.6783e-04 - lr: 1.0000e-04\n",
      "Epoch 84: early stopping\n",
      "23/23 [==============================] - 1s 7ms/step\n",
      "Canada\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 6s 37ms/step - loss: 0.0207 - val_loss: 0.0030 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 1s 33ms/step - loss: 0.0035 - val_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 0.0030 - val_loss: 0.0047 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0022 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0017 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0015 - val_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0016 - val_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0015 - val_loss: 8.4263e-04 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0019 - val_loss: 9.9347e-04 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 0.0014 - val_loss: 8.7985e-04 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0011 - val_loss: 7.9238e-04 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 0.0013 - val_loss: 0.0026 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0012 - val_loss: 0.0031 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 9.5543e-04 - val_loss: 8.5402e-04 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 8.6484e-04 - val_loss: 7.0992e-04 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 8.1097e-04 - val_loss: 8.2341e-04 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 8.0743e-04 - val_loss: 0.0011 - lr: 2.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 8.6730e-04 - val_loss: 0.0015 - lr: 2.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 8.3500e-04 - val_loss: 6.6829e-04 - lr: 2.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 7.3615e-04 - val_loss: 6.5477e-04 - lr: 2.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 6.9493e-04 - val_loss: 6.4254e-04 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 1s 20ms/step - loss: 7.5989e-04 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 8.2498e-04 - val_loss: 6.8972e-04 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 6.7677e-04 - val_loss: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 7.0691e-04 - val_loss: 6.2648e-04 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 7.1535e-04 - val_loss: 6.8685e-04 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 7.1922e-04 - val_loss: 7.0767e-04 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 6.5992e-04 - val_loss: 5.2072e-04 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 6.7551e-04 - val_loss: 6.4719e-04 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 6.0012e-04 - val_loss: 5.2152e-04 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 6.2075e-04 - val_loss: 4.8222e-04 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 1s 33ms/step - loss: 5.9811e-04 - val_loss: 5.6949e-04 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 5.6328e-04 - val_loss: 5.9763e-04 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 6.2921e-04 - val_loss: 5.1908e-04 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 1s 33ms/step - loss: 6.6633e-04 - val_loss: 4.2381e-04 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 6.1383e-04 - val_loss: 7.3869e-04 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 5.7646e-04 - val_loss: 4.1925e-04 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 5.6863e-04 - val_loss: 7.8710e-04 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 5.8039e-04 - val_loss: 4.5481e-04 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 1s 33ms/step - loss: 5.5816e-04 - val_loss: 5.5184e-04 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 5.8668e-04 - val_loss: 9.0593e-04 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 6.0024e-04 - val_loss: 4.9319e-04 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 6.0579e-04 - val_loss: 5.0926e-04 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 5.5260e-04 - val_loss: 4.5466e-04 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 5.3582e-04 - val_loss: 3.6424e-04 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 4.6458e-04 - val_loss: 5.0997e-04 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 6.0461e-04 - val_loss: 9.4859e-04 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 5.4573e-04 - val_loss: 4.1084e-04 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 5.0398e-04 - val_loss: 3.8372e-04 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 5.2412e-04 - val_loss: 4.2509e-04 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 5.5821e-04 - val_loss: 4.1700e-04 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 5.5810e-04 - val_loss: 5.5835e-04 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 5.1458e-04 - val_loss: 4.7057e-04 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 5.3569e-04 - val_loss: 4.7732e-04 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 4.5877e-04 - val_loss: 4.8772e-04 - lr: 1.0000e-04\n",
      "Epoch 55: early stopping\n",
      "23/23 [==============================] - 1s 6ms/step\n",
      "China\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 7s 45ms/step - loss: 0.0194 - val_loss: 0.0054 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0024 - val_loss: 0.0026 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 0.0016 - val_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0015 - val_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0013 - val_loss: 0.0027 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0014 - val_loss: 0.0074 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0011 - val_loss: 0.0047 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 9.8757e-04 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0013 - val_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 8.7982e-04 - val_loss: 0.0030 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 7.6610e-04 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 8.0547e-04 - val_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 6.5747e-04 - val_loss: 0.0027 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 6.4542e-04 - val_loss: 0.0033 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 5.9377e-04 - val_loss: 0.0031 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 5.4118e-04 - val_loss: 0.0020 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 5.5029e-04 - val_loss: 0.0023 - lr: 2.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 5.4687e-04 - val_loss: 0.0027 - lr: 2.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 5.1686e-04 - val_loss: 0.0020 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 4.9932e-04 - val_loss: 0.0023 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 5.4051e-04 - val_loss: 9.3508e-04 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 5.9026e-04 - val_loss: 0.0028 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 5.2012e-04 - val_loss: 0.0017 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 4.7711e-04 - val_loss: 0.0021 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 4.8411e-04 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 4.9883e-04 - val_loss: 0.0014 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 4.7200e-04 - val_loss: 0.0018 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 1s 33ms/step - loss: 4.7358e-04 - val_loss: 0.0020 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 4.8785e-04 - val_loss: 0.0018 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 4.8761e-04 - val_loss: 0.0021 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 4.6930e-04 - val_loss: 0.0020 - lr: 1.0000e-04\n",
      "Epoch 31: early stopping\n",
      "23/23 [==============================] - 1s 7ms/step\n",
      "Denmark\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 6s 34ms/step - loss: 0.0317 - val_loss: 0.0109 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0048 - val_loss: 0.0030 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 0.0046 - val_loss: 0.0113 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 0.0033 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0036 - val_loss: 0.0067 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0028 - val_loss: 0.0045 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0024 - val_loss: 0.0387 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0035 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0024 - val_loss: 0.0034 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0018 - val_loss: 0.0022 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0016 - val_loss: 0.0039 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 1s 35ms/step - loss: 0.0016 - val_loss: 0.0029 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0015 - val_loss: 0.0029 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0015 - val_loss: 0.0017 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 0.0014 - val_loss: 0.0022 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0015 - val_loss: 0.0015 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0015 - val_loss: 0.0014 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0015 - val_loss: 0.0022 - lr: 1.0000e-04\n",
      "Epoch 18: early stopping\n",
      "23/23 [==============================] - 0s 5ms/step\n",
      "Hong Kong\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 7s 50ms/step - loss: 0.0092 - val_loss: 0.0072 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0037 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 0.0032 - val_loss: 0.0080 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 0.0024 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 0.0019 - val_loss: 0.0034 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0020 - val_loss: 0.0038 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 0.0024 - val_loss: 0.0042 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0014 - val_loss: 0.0050 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0011 - val_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 9.2125e-04 - val_loss: 0.0033 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 8.7497e-04 - val_loss: 0.0038 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 8.2148e-04 - val_loss: 0.0026 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 8.3050e-04 - val_loss: 0.0031 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 8.4695e-04 - val_loss: 0.0033 - lr: 2.0000e-04\n",
      "Epoch 14: early stopping\n",
      "23/23 [==============================] - 1s 8ms/step\n",
      "India\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 6s 39ms/step - loss: 0.0277 - val_loss: 3.2766e-04 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0044 - val_loss: 0.0026 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 0.0035 - val_loss: 5.6465e-04 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0029 - val_loss: 0.0054 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0024 - val_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0020 - val_loss: 0.0038 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0017 - val_loss: 5.5088e-04 - lr: 2.0000e-04\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0015 - val_loss: 0.0016 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0017 - val_loss: 0.0015 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0016 - val_loss: 9.2807e-04 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0015 - val_loss: 0.0013 - lr: 2.0000e-04\n",
      "Epoch 11: early stopping\n",
      "23/23 [==============================] - 1s 7ms/step\n",
      "Japan\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 6s 36ms/step - loss: 0.0529 - val_loss: 0.0088 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 0.0072 - val_loss: 0.0052 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0047 - val_loss: 0.0068 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0038 - val_loss: 0.0094 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 0.0037 - val_loss: 0.0054 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 0.0034 - val_loss: 0.0051 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0030 - val_loss: 0.0100 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0027 - val_loss: 0.0049 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0025 - val_loss: 0.0052 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0024 - val_loss: 0.0029 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0024 - val_loss: 0.0032 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0023 - val_loss: 0.0046 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 1s 32ms/step - loss: 0.0021 - val_loss: 0.0048 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0022 - val_loss: 0.0067 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0023 - val_loss: 0.0034 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0022 - val_loss: 0.0045 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0020 - val_loss: 0.0049 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0020 - val_loss: 0.0063 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0021 - val_loss: 0.0044 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0019 - val_loss: 0.0048 - lr: 1.0000e-04\n",
      "Epoch 20: early stopping\n",
      "23/23 [==============================] - 1s 8ms/step\n",
      "Malaysia\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 6s 42ms/step - loss: 0.0172 - val_loss: 0.0435 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0040 - val_loss: 0.0044 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0030 - val_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0024 - val_loss: 0.0041 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0026 - val_loss: 0.0085 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 0.0019 - val_loss: 0.0118 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0026 - val_loss: 0.0182 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0020 - val_loss: 0.0037 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0016 - val_loss: 0.0051 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0016 - val_loss: 0.0098 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0014 - val_loss: 0.0066 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0015 - val_loss: 0.0092 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0014 - val_loss: 0.0073 - lr: 2.0000e-04\n",
      "Epoch 13: early stopping\n",
      "23/23 [==============================] - 1s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "result = Execute_model(m, xtr, ytr, xt, yt, scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89029291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.010391</td>\n",
       "      <td>0.000106</td>\n",
       "      <td>0.004194</td>\n",
       "      <td>0.006066</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>3.234171</td>\n",
       "      <td>8.204543</td>\n",
       "      <td>0.009996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.010680</td>\n",
       "      <td>0.012490</td>\n",
       "      <td>0.082521</td>\n",
       "      <td>0.008762</td>\n",
       "      <td>0.053419</td>\n",
       "      <td>0.067502</td>\n",
       "      <td>0.016639</td>\n",
       "      <td>1.659348</td>\n",
       "      <td>2.685284</td>\n",
       "      <td>0.095595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE</th>\n",
       "      <td>0.015108</td>\n",
       "      <td>0.010724</td>\n",
       "      <td>0.021170</td>\n",
       "      <td>0.006655</td>\n",
       "      <td>0.007821</td>\n",
       "      <td>0.010305</td>\n",
       "      <td>0.002122</td>\n",
       "      <td>0.023735</td>\n",
       "      <td>0.024394</td>\n",
       "      <td>0.023276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Australia    Europe    Brazil    Canada     China   Denmark  \\\n",
       "Metric                                                                \n",
       "MSE      0.000184  0.000253  0.010391  0.000106  0.004194  0.006066   \n",
       "MAE      0.010680  0.012490  0.082521  0.008762  0.053419  0.067502   \n",
       "MAPE     0.015108  0.010724  0.021170  0.006655  0.007821  0.010305   \n",
       "\n",
       "        Hong Kong     India     Japan  Malaysia  \n",
       "Metric                                           \n",
       "MSE      0.000305  3.234171  8.204543  0.009996  \n",
       "MAE      0.016639  1.659348  2.685284  0.095595  \n",
       "MAPE     0.002122  0.023735  0.024394  0.023276  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c617382",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Define the filename\n",
    "filename = \"../results.json\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.isfile(filename):\n",
    "    # If the file exists, load the existing data\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "else:\n",
    "    # If the file doesn't exist, create an empty dictionary\n",
    "    data = {}\n",
    "\n",
    "# Add the result to the dictionary\n",
    "data[\"CNN+RNN\"] = result.to_dict()\n",
    "\n",
    "# Write the dictionary to the file\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
