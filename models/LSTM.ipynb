{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f01c67d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error,\n",
    "    mean_squared_error,\n",
    ")\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bee2e983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>0.9133</td>\n",
       "      <td>1.4419</td>\n",
       "      <td>1.7200</td>\n",
       "      <td>1.0377</td>\n",
       "      <td>6.8273</td>\n",
       "      <td>5.1597</td>\n",
       "      <td>7.7555</td>\n",
       "      <td>46.27</td>\n",
       "      <td>92.55</td>\n",
       "      <td>3.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>0.9143</td>\n",
       "      <td>1.4402</td>\n",
       "      <td>1.7296</td>\n",
       "      <td>1.0371</td>\n",
       "      <td>6.8258</td>\n",
       "      <td>5.1668</td>\n",
       "      <td>7.7564</td>\n",
       "      <td>46.13</td>\n",
       "      <td>91.48</td>\n",
       "      <td>3.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>0.9189</td>\n",
       "      <td>1.4404</td>\n",
       "      <td>1.7292</td>\n",
       "      <td>1.0333</td>\n",
       "      <td>6.8272</td>\n",
       "      <td>5.1638</td>\n",
       "      <td>7.7546</td>\n",
       "      <td>45.72</td>\n",
       "      <td>92.53</td>\n",
       "      <td>3.379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>0.9168</td>\n",
       "      <td>1.4314</td>\n",
       "      <td>1.7409</td>\n",
       "      <td>1.0351</td>\n",
       "      <td>6.8280</td>\n",
       "      <td>5.1981</td>\n",
       "      <td>7.7539</td>\n",
       "      <td>45.67</td>\n",
       "      <td>93.31</td>\n",
       "      <td>3.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>0.9218</td>\n",
       "      <td>1.4357</td>\n",
       "      <td>1.7342</td>\n",
       "      <td>1.0345</td>\n",
       "      <td>6.8274</td>\n",
       "      <td>5.1827</td>\n",
       "      <td>7.7553</td>\n",
       "      <td>45.50</td>\n",
       "      <td>92.70</td>\n",
       "      <td>3.375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  Australia  Europe  Brazil  Canada   China  \\\n",
       "0             0  2010-01-04     0.9133  1.4419  1.7200  1.0377  6.8273   \n",
       "1             1  2010-01-05     0.9143  1.4402  1.7296  1.0371  6.8258   \n",
       "2             2  2010-01-06     0.9189  1.4404  1.7292  1.0333  6.8272   \n",
       "3             3  2010-01-07     0.9168  1.4314  1.7409  1.0351  6.8280   \n",
       "4             4  2010-01-08     0.9218  1.4357  1.7342  1.0345  6.8274   \n",
       "\n",
       "   Denmark  Hong Kong  India  Japan  Malaysia  \n",
       "0   5.1597     7.7555  46.27  92.55     3.396  \n",
       "1   5.1668     7.7564  46.13  91.48     3.385  \n",
       "2   5.1638     7.7546  45.72  92.53     3.379  \n",
       "3   5.1981     7.7539  45.67  93.31     3.368  \n",
       "4   5.1827     7.7553  45.50  92.70     3.375  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5cd89d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data.drop(\"Unnamed: 0.1\", axis=1, inplace=True)\n",
    "    data.rename(columns={\"Unnamed: 0\": \"Date\"}, inplace=True)\n",
    "    data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
    "    data.set_index(\"Date\", inplace=True)\n",
    "    data.replace(0, np.nan, inplace=True)\n",
    "    display(data)\n",
    "    print(\"Filling missing Values: \")\n",
    "    display(data.interpolate(method=\"linear\", limit_direction=\"forward\"))\n",
    "    data.interpolate(method=\"linear\", limit_direction=\"forward\", inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f1ffad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>0.9133</td>\n",
       "      <td>1.4419</td>\n",
       "      <td>1.7200</td>\n",
       "      <td>1.0377</td>\n",
       "      <td>6.8273</td>\n",
       "      <td>5.1597</td>\n",
       "      <td>7.7555</td>\n",
       "      <td>46.27</td>\n",
       "      <td>92.55</td>\n",
       "      <td>3.3960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.9143</td>\n",
       "      <td>1.4402</td>\n",
       "      <td>1.7296</td>\n",
       "      <td>1.0371</td>\n",
       "      <td>6.8258</td>\n",
       "      <td>5.1668</td>\n",
       "      <td>7.7564</td>\n",
       "      <td>46.13</td>\n",
       "      <td>91.48</td>\n",
       "      <td>3.3850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>0.9189</td>\n",
       "      <td>1.4404</td>\n",
       "      <td>1.7292</td>\n",
       "      <td>1.0333</td>\n",
       "      <td>6.8272</td>\n",
       "      <td>5.1638</td>\n",
       "      <td>7.7546</td>\n",
       "      <td>45.72</td>\n",
       "      <td>92.53</td>\n",
       "      <td>3.3790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>0.9168</td>\n",
       "      <td>1.4314</td>\n",
       "      <td>1.7409</td>\n",
       "      <td>1.0351</td>\n",
       "      <td>6.8280</td>\n",
       "      <td>5.1981</td>\n",
       "      <td>7.7539</td>\n",
       "      <td>45.67</td>\n",
       "      <td>93.31</td>\n",
       "      <td>3.3680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>0.9218</td>\n",
       "      <td>1.4357</td>\n",
       "      <td>1.7342</td>\n",
       "      <td>1.0345</td>\n",
       "      <td>6.8274</td>\n",
       "      <td>5.1827</td>\n",
       "      <td>7.7553</td>\n",
       "      <td>45.50</td>\n",
       "      <td>92.70</td>\n",
       "      <td>3.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>0.6978</td>\n",
       "      <td>1.1174</td>\n",
       "      <td>4.0507</td>\n",
       "      <td>1.3073</td>\n",
       "      <td>6.9954</td>\n",
       "      <td>6.6829</td>\n",
       "      <td>7.7874</td>\n",
       "      <td>71.45</td>\n",
       "      <td>109.47</td>\n",
       "      <td>4.1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>0.7004</td>\n",
       "      <td>1.1217</td>\n",
       "      <td>4.0152</td>\n",
       "      <td>1.3058</td>\n",
       "      <td>6.9864</td>\n",
       "      <td>6.6589</td>\n",
       "      <td>7.7857</td>\n",
       "      <td>71.30</td>\n",
       "      <td>108.85</td>\n",
       "      <td>4.1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>0.7030</td>\n",
       "      <td>1.1227</td>\n",
       "      <td>4.0190</td>\n",
       "      <td>1.2962</td>\n",
       "      <td>6.9618</td>\n",
       "      <td>6.6554</td>\n",
       "      <td>7.7894</td>\n",
       "      <td>71.36</td>\n",
       "      <td>108.67</td>\n",
       "      <td>4.0918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3648 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Australia  Europe  Brazil  Canada   China  Denmark  Hong Kong  \\\n",
       "Date                                                                        \n",
       "2010-01-04     0.9133  1.4419  1.7200  1.0377  6.8273   5.1597     7.7555   \n",
       "2010-01-05     0.9143  1.4402  1.7296  1.0371  6.8258   5.1668     7.7564   \n",
       "2010-01-06     0.9189  1.4404  1.7292  1.0333  6.8272   5.1638     7.7546   \n",
       "2010-01-07     0.9168  1.4314  1.7409  1.0351  6.8280   5.1981     7.7539   \n",
       "2010-01-08     0.9218  1.4357  1.7342  1.0345  6.8274   5.1827     7.7553   \n",
       "...               ...     ...     ...     ...     ...      ...        ...   \n",
       "2019-12-27     0.6978  1.1174  4.0507  1.3073  6.9954   6.6829     7.7874   \n",
       "2019-12-28        NaN     NaN     NaN     NaN     NaN      NaN        NaN   \n",
       "2019-12-29        NaN     NaN     NaN     NaN     NaN      NaN        NaN   \n",
       "2019-12-30     0.7004  1.1217  4.0152  1.3058  6.9864   6.6589     7.7857   \n",
       "2019-12-31     0.7030  1.1227  4.0190  1.2962  6.9618   6.6554     7.7894   \n",
       "\n",
       "            India   Japan  Malaysia  \n",
       "Date                                 \n",
       "2010-01-04  46.27   92.55    3.3960  \n",
       "2010-01-05  46.13   91.48    3.3850  \n",
       "2010-01-06  45.72   92.53    3.3790  \n",
       "2010-01-07  45.67   93.31    3.3680  \n",
       "2010-01-08  45.50   92.70    3.3750  \n",
       "...           ...     ...       ...  \n",
       "2019-12-27  71.45  109.47    4.1260  \n",
       "2019-12-28    NaN     NaN       NaN  \n",
       "2019-12-29    NaN     NaN       NaN  \n",
       "2019-12-30  71.30  108.85    4.1053  \n",
       "2019-12-31  71.36  108.67    4.0918  \n",
       "\n",
       "[3648 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling missing Values: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>0.913300</td>\n",
       "      <td>1.441900</td>\n",
       "      <td>1.720000</td>\n",
       "      <td>1.0377</td>\n",
       "      <td>6.8273</td>\n",
       "      <td>5.1597</td>\n",
       "      <td>7.755500</td>\n",
       "      <td>46.27</td>\n",
       "      <td>92.550000</td>\n",
       "      <td>3.3960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.914300</td>\n",
       "      <td>1.440200</td>\n",
       "      <td>1.729600</td>\n",
       "      <td>1.0371</td>\n",
       "      <td>6.8258</td>\n",
       "      <td>5.1668</td>\n",
       "      <td>7.756400</td>\n",
       "      <td>46.13</td>\n",
       "      <td>91.480000</td>\n",
       "      <td>3.3850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>0.918900</td>\n",
       "      <td>1.440400</td>\n",
       "      <td>1.729200</td>\n",
       "      <td>1.0333</td>\n",
       "      <td>6.8272</td>\n",
       "      <td>5.1638</td>\n",
       "      <td>7.754600</td>\n",
       "      <td>45.72</td>\n",
       "      <td>92.530000</td>\n",
       "      <td>3.3790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>0.916800</td>\n",
       "      <td>1.431400</td>\n",
       "      <td>1.740900</td>\n",
       "      <td>1.0351</td>\n",
       "      <td>6.8280</td>\n",
       "      <td>5.1981</td>\n",
       "      <td>7.753900</td>\n",
       "      <td>45.67</td>\n",
       "      <td>93.310000</td>\n",
       "      <td>3.3680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>0.921800</td>\n",
       "      <td>1.435700</td>\n",
       "      <td>1.734200</td>\n",
       "      <td>1.0345</td>\n",
       "      <td>6.8274</td>\n",
       "      <td>5.1827</td>\n",
       "      <td>7.755300</td>\n",
       "      <td>45.50</td>\n",
       "      <td>92.700000</td>\n",
       "      <td>3.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>0.697800</td>\n",
       "      <td>1.117400</td>\n",
       "      <td>4.050700</td>\n",
       "      <td>1.3073</td>\n",
       "      <td>6.9954</td>\n",
       "      <td>6.6829</td>\n",
       "      <td>7.787400</td>\n",
       "      <td>71.45</td>\n",
       "      <td>109.470000</td>\n",
       "      <td>4.1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-28</th>\n",
       "      <td>0.698667</td>\n",
       "      <td>1.118833</td>\n",
       "      <td>4.038867</td>\n",
       "      <td>1.3068</td>\n",
       "      <td>6.9924</td>\n",
       "      <td>6.6749</td>\n",
       "      <td>7.786833</td>\n",
       "      <td>71.40</td>\n",
       "      <td>109.263333</td>\n",
       "      <td>4.1191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-29</th>\n",
       "      <td>0.699533</td>\n",
       "      <td>1.120267</td>\n",
       "      <td>4.027033</td>\n",
       "      <td>1.3063</td>\n",
       "      <td>6.9894</td>\n",
       "      <td>6.6669</td>\n",
       "      <td>7.786267</td>\n",
       "      <td>71.35</td>\n",
       "      <td>109.056667</td>\n",
       "      <td>4.1122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>0.700400</td>\n",
       "      <td>1.121700</td>\n",
       "      <td>4.015200</td>\n",
       "      <td>1.3058</td>\n",
       "      <td>6.9864</td>\n",
       "      <td>6.6589</td>\n",
       "      <td>7.785700</td>\n",
       "      <td>71.30</td>\n",
       "      <td>108.850000</td>\n",
       "      <td>4.1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>0.703000</td>\n",
       "      <td>1.122700</td>\n",
       "      <td>4.019000</td>\n",
       "      <td>1.2962</td>\n",
       "      <td>6.9618</td>\n",
       "      <td>6.6554</td>\n",
       "      <td>7.789400</td>\n",
       "      <td>71.36</td>\n",
       "      <td>108.670000</td>\n",
       "      <td>4.0918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3648 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Australia    Europe    Brazil  Canada   China  Denmark  Hong Kong  \\\n",
       "Date                                                                            \n",
       "2010-01-04   0.913300  1.441900  1.720000  1.0377  6.8273   5.1597   7.755500   \n",
       "2010-01-05   0.914300  1.440200  1.729600  1.0371  6.8258   5.1668   7.756400   \n",
       "2010-01-06   0.918900  1.440400  1.729200  1.0333  6.8272   5.1638   7.754600   \n",
       "2010-01-07   0.916800  1.431400  1.740900  1.0351  6.8280   5.1981   7.753900   \n",
       "2010-01-08   0.921800  1.435700  1.734200  1.0345  6.8274   5.1827   7.755300   \n",
       "...               ...       ...       ...     ...     ...      ...        ...   \n",
       "2019-12-27   0.697800  1.117400  4.050700  1.3073  6.9954   6.6829   7.787400   \n",
       "2019-12-28   0.698667  1.118833  4.038867  1.3068  6.9924   6.6749   7.786833   \n",
       "2019-12-29   0.699533  1.120267  4.027033  1.3063  6.9894   6.6669   7.786267   \n",
       "2019-12-30   0.700400  1.121700  4.015200  1.3058  6.9864   6.6589   7.785700   \n",
       "2019-12-31   0.703000  1.122700  4.019000  1.2962  6.9618   6.6554   7.789400   \n",
       "\n",
       "            India       Japan  Malaysia  \n",
       "Date                                     \n",
       "2010-01-04  46.27   92.550000    3.3960  \n",
       "2010-01-05  46.13   91.480000    3.3850  \n",
       "2010-01-06  45.72   92.530000    3.3790  \n",
       "2010-01-07  45.67   93.310000    3.3680  \n",
       "2010-01-08  45.50   92.700000    3.3750  \n",
       "...           ...         ...       ...  \n",
       "2019-12-27  71.45  109.470000    4.1260  \n",
       "2019-12-28  71.40  109.263333    4.1191  \n",
       "2019-12-29  71.35  109.056667    4.1122  \n",
       "2019-12-30  71.30  108.850000    4.1053  \n",
       "2019-12-31  71.36  108.670000    4.0918  \n",
       "\n",
       "[3648 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab94630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOOK_BACK = 30\n",
    "PREDICT_DAY = 1\n",
    "SPLIT_RATIO = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "673eb188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Data(\n",
    "    data, lookback=LOOK_BACK, pred_len=PREDICT_DAY, split_ratio=SPLIT_RATIO, model=\"FNN\"\n",
    "):\n",
    "    if lookback < 2:\n",
    "        print(\"ERROR: Lookback too small\")\n",
    "        return -1\n",
    "\n",
    "    # declarations\n",
    "\n",
    "    x = {}\n",
    "    y = {}\n",
    "    xtr = {}\n",
    "    xt = {}\n",
    "    ytr = {}\n",
    "    yt = {}\n",
    "    scalers = {}\n",
    "\n",
    "    # Creating stepped data\n",
    "\n",
    "    for i in data.columns:\n",
    "        xtemp = pd.DataFrame(data[i])\n",
    "        for j in range(1, lookback + 1):\n",
    "            xtemp[i + str(j)] = data[i].shift(-1 * j)\n",
    "        x[i] = xtemp.dropna()\n",
    "\n",
    "    # Splitting data into x and y\n",
    "\n",
    "    for i in x.keys():\n",
    "        y[i] = pd.DataFrame(x[i].iloc[:, -pred_len])\n",
    "        x[i] = x[i].iloc[:, :-pred_len]\n",
    "\n",
    "    # Normalizing x and y values\n",
    "\n",
    "    for i in x.keys():\n",
    "        scalers[i + \"_x\"] = MinMaxScaler(feature_range=(0, 1))\n",
    "        x[i] = scalers[i + \"_x\"].fit_transform(x[i])\n",
    "        scalers[i + \"_y\"] = MinMaxScaler(feature_range=(0, 1))\n",
    "        y[i] = scalers[i + \"_y\"].fit_transform(y[i])\n",
    "\n",
    "    # setting train and test sizes\n",
    "\n",
    "    tr_len = int(split_ratio * y[\"India\"].shape[0])\n",
    "    t_len = y[\"India\"].shape[0] - tr_len\n",
    "\n",
    "    # creating training and testing data\n",
    "\n",
    "    for i in x.keys():\n",
    "        xtr[i] = x[i][:tr_len]\n",
    "        ytr[i] = y[i][:tr_len]\n",
    "        xt[i] = x[i][-t_len:]\n",
    "        yt[i] = y[i][-t_len:]\n",
    "\n",
    "    # returning pertinent data\n",
    "\n",
    "    return x, y, xtr, xt, ytr, yt, scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fc04bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, xtr, xt, ytr, yt, scalers = Create_Data(data, model=\"RNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "333787a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_model(x, y, lookback=LOOK_BACK, Pred_size=PREDICT_DAY):\n",
    "    models = {}\n",
    "    for i in x.keys():\n",
    "        models[i] = Sequential()\n",
    "        models[i].add(\n",
    "            LSTM(\n",
    "                32,\n",
    "                input_shape=(\n",
    "                    LOOK_BACK,\n",
    "                    1,\n",
    "                ),\n",
    "                return_sequences=True,\n",
    "                activation=\"relu\",\n",
    "            )\n",
    "        )\n",
    "        models[i].add(LSTM(64, return_sequences=True, activation=\"relu\"))\n",
    "        models[i].add(Dropout(0.2))\n",
    "        models[i].add(LSTM(128, return_sequences=True, activation=\"relu\"))\n",
    "        models[i].add(Dropout(0.2))\n",
    "        models[i].add(LSTM(64, return_sequences=True, activation=\"relu\"))\n",
    "        models[i].add(Dropout(0.2))\n",
    "        models[i].add(LSTM(16, activation=\"relu\"))\n",
    "        models[i].add(Dense(Pred_size))\n",
    "        models[i].compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "        print(i)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2df7ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Australia\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Europe\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Brazil\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Canada\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "China\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Denmark\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_35 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Hong Kong\n",
      "WARNING:tensorflow:Layer lstm_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_37 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_38 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_39 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_40 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "India\n",
      "WARNING:tensorflow:Layer lstm_41 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_42 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_43 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_44 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_45 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Japan\n",
      "WARNING:tensorflow:Layer lstm_46 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_47 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_50 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Malaysia\n"
     ]
    }
   ],
   "source": [
    "m = Create_model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "599dc5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Execute_model(model, xtr, ytr, xt, yt, scaler):\n",
    "    MAPE = {}\n",
    "    MAE = {}\n",
    "    MSE = {}\n",
    "    for i in model.keys():\n",
    "        print(i)\n",
    "        # Creating EarlyStopping and ReduceLROnPlateau callbacks\n",
    "        es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=1, patience=10)\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", factor=0.2, patience=5, min_lr=0.0001\n",
    "        )\n",
    "\n",
    "        # Training the model with EarlyStopping and ReduceLROnPlateau callbacks\n",
    "        model[i].fit(\n",
    "            xtr[i],\n",
    "            ytr[i],\n",
    "            epochs=100,\n",
    "            batch_size=64,\n",
    "            verbose=1,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[es, reduce_lr],\n",
    "        )\n",
    "\n",
    "        # collecting predicted and actual values\n",
    "        temp = model[i].predict(xt[i])\n",
    "        pred = scaler[i + \"_y\"].inverse_transform(temp)\n",
    "        act = scaler[i + \"_y\"].inverse_transform(yt[i])\n",
    "\n",
    "        # calculating Mean Square Error, Mean Absolute Error, and Mean Absolute Error\n",
    "        MSE[i] = mean_squared_error(act, pred)\n",
    "        MAE[i] = mean_absolute_error(act, pred)\n",
    "        MAPE[i] = mean_absolute_percentage_error(act, pred)\n",
    "\n",
    "    # Tabulating Data\n",
    "    results = pd.DataFrame([MSE, MAE, MAPE])\n",
    "    results[\"Metric\"] = [\"MSE\", \"MAE\", \"MAPE\"]\n",
    "    results.set_index(\"Metric\", inplace=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddd35db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 12:44:39.999388: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5625924b8a60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-10-12 12:44:39.999422: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2023-10-12 12:44:40.005107: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-10-12 12:44:40.019558: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8902\n",
      "2023-10-12 12:44:40.122984: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 11s 143ms/step - loss: 0.0818 - val_loss: 0.0126 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 129ms/step - loss: 0.0058 - val_loss: 7.9843e-04 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 0.0040 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0036 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 4s 119ms/step - loss: 0.0035 - val_loss: 9.5746e-04 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 4s 115ms/step - loss: 0.0034 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 4s 119ms/step - loss: 0.0032 - val_loss: 0.0030 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 4s 119ms/step - loss: 0.0030 - val_loss: 0.0012 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0030 - val_loss: 0.0016 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0029 - val_loss: 0.0010 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0030 - val_loss: 0.0012 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 4s 119ms/step - loss: 0.0029 - val_loss: 0.0017 - lr: 2.0000e-04\n",
      "Epoch 12: early stopping\n",
      "23/23 [==============================] - 1s 23ms/step\n",
      "Europe\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 10s 131ms/step - loss: 0.0969 - val_loss: 0.0250 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0066 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0042 - val_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0039 - val_loss: 0.0027 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 4s 119ms/step - loss: 0.0037 - val_loss: 0.0033 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 4s 117ms/step - loss: 0.0038 - val_loss: 0.0025 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 4s 119ms/step - loss: 0.0035 - val_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 4s 115ms/step - loss: 0.0032 - val_loss: 0.0027 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0029 - val_loss: 0.0027 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0029 - val_loss: 0.0025 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0028 - val_loss: 0.0024 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 129ms/step - loss: 0.0028 - val_loss: 0.0024 - lr: 2.0000e-04\n",
      "Epoch 12: early stopping\n",
      "23/23 [==============================] - 1s 25ms/step\n",
      "Brazil\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 11s 151ms/step - loss: 0.0476 - val_loss: 0.0026 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 4s 117ms/step - loss: 0.0030 - val_loss: 8.1229e-04 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 4s 119ms/step - loss: 0.0016 - val_loss: 9.4684e-04 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0015 - val_loss: 9.9402e-04 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0013 - val_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0012 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 0.0013 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0010 - val_loss: 8.6053e-04 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 9.8989e-04 - val_loss: 0.0014 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 9.9678e-04 - val_loss: 9.9367e-04 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0010 - val_loss: 6.4300e-04 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0011 - val_loss: 0.0011 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 9.4038e-04 - val_loss: 0.0010 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 9.8584e-04 - val_loss: 8.4194e-04 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 9.9827e-04 - val_loss: 6.1359e-04 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 9.7599e-04 - val_loss: 0.0011 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 9.4991e-04 - val_loss: 9.0177e-04 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0010 - val_loss: 7.1618e-04 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 8.9961e-04 - val_loss: 9.1848e-04 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 8.7601e-04 - val_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 9.4224e-04 - val_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 9.5154e-04 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 8.9037e-04 - val_loss: 7.5604e-04 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 9.2155e-04 - val_loss: 7.7787e-04 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 8.1280e-04 - val_loss: 8.3317e-04 - lr: 1.0000e-04\n",
      "Epoch 25: early stopping\n",
      "23/23 [==============================] - 1s 24ms/step\n",
      "Canada\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 11s 139ms/step - loss: 0.0281 - val_loss: 0.0046 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0025 - val_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 0.0016 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0015 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 4s 119ms/step - loss: 0.0013 - val_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 129ms/step - loss: 0.0015 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0013 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0013 - val_loss: 0.0030 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 0.0015 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0013 - val_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0013 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0013 - val_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 129ms/step - loss: 0.0012 - val_loss: 0.0012 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 133ms/step - loss: 0.0011 - val_loss: 0.0012 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0011 - val_loss: 0.0012 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 4s 120ms/step - loss: 0.0011 - val_loss: 0.0012 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0011 - val_loss: 0.0013 - lr: 2.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.0011 - val_loss: 0.0011 - lr: 2.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0011 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0011 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 0.0011 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 0.0011 - val_loss: 0.0014 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0010 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0011 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0011 - val_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.0011 - val_loss: 0.0014 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 0.0010 - val_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.0011 - val_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0011 - val_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 0.0011 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 5s 133ms/step - loss: 0.0011 - val_loss: 0.0017 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 5s 137ms/step - loss: 0.0011 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 0.0010 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 5s 129ms/step - loss: 0.0010 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0010 - val_loss: 0.0018 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 9.7477e-04 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 0.0010 - val_loss: 0.0015 - lr: 1.0000e-04\n",
      "Epoch 37: early stopping\n",
      "23/23 [==============================] - 1s 24ms/step\n",
      "China\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 12s 142ms/step - loss: 0.0303 - val_loss: 0.0146 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0019 - val_loss: 0.0035 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 0.0011 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0010 - val_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 9.3050e-04 - val_loss: 0.0034 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 135ms/step - loss: 8.8758e-04 - val_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 136ms/step - loss: 8.9826e-04 - val_loss: 0.0030 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 130ms/step - loss: 8.3669e-04 - val_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 133ms/step - loss: 9.7789e-04 - val_loss: 0.0032 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 7.9166e-04 - val_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 8.6045e-04 - val_loss: 0.0025 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 129ms/step - loss: 7.4599e-04 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 7.2147e-04 - val_loss: 0.0035 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 131ms/step - loss: 6.8427e-04 - val_loss: 0.0029 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 130ms/step - loss: 6.8947e-04 - val_loss: 0.0030 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 5s 130ms/step - loss: 6.0962e-04 - val_loss: 0.0032 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 6.3605e-04 - val_loss: 0.0030 - lr: 2.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 5s 131ms/step - loss: 6.6463e-04 - val_loss: 0.0028 - lr: 2.0000e-04\n",
      "Epoch 18: early stopping\n",
      "23/23 [==============================] - 1s 27ms/step\n",
      "Denmark\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 11s 142ms/step - loss: 0.0530 - val_loss: 0.0106 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 138ms/step - loss: 0.0041 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0035 - val_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0031 - val_loss: 0.0026 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0028 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 131ms/step - loss: 0.0026 - val_loss: 0.0025 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0025 - val_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 129ms/step - loss: 0.0023 - val_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 136ms/step - loss: 0.0021 - val_loss: 0.0050 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 131ms/step - loss: 0.0022 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0019 - val_loss: 0.0010 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0017 - val_loss: 0.0012 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 131ms/step - loss: 0.0017 - val_loss: 0.0011 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 0.0018 - val_loss: 0.0012 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 133ms/step - loss: 0.0016 - val_loss: 0.0011 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 5s 133ms/step - loss: 0.0017 - val_loss: 0.0011 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 5s 133ms/step - loss: 0.0016 - val_loss: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.0016 - val_loss: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 0.0016 - val_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 5s 129ms/step - loss: 0.0016 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 0.0016 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 5s 131ms/step - loss: 0.0016 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 5s 130ms/step - loss: 0.0015 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 0.0016 - val_loss: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 0.0015 - val_loss: 9.9645e-04 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 5s 148ms/step - loss: 0.0016 - val_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 6s 157ms/step - loss: 0.0015 - val_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 6s 159ms/step - loss: 0.0015 - val_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 6s 157ms/step - loss: 0.0016 - val_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 6s 157ms/step - loss: 0.0014 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 6s 163ms/step - loss: 0.0016 - val_loss: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 6s 160ms/step - loss: 0.0015 - val_loss: 9.6625e-04 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 6s 157ms/step - loss: 0.0015 - val_loss: 9.9376e-04 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 6s 162ms/step - loss: 0.0015 - val_loss: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 6s 160ms/step - loss: 0.0015 - val_loss: 9.9172e-04 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 6s 159ms/step - loss: 0.0014 - val_loss: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 6s 173ms/step - loss: 0.0014 - val_loss: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 6s 163ms/step - loss: 0.0015 - val_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 6s 166ms/step - loss: 0.0014 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0015 - val_loss: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 6s 159ms/step - loss: 0.0014 - val_loss: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 6s 161ms/step - loss: 0.0014 - val_loss: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 42: early stopping\n",
      "23/23 [==============================] - 1s 31ms/step\n",
      "Hong Kong\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 13s 177ms/step - loss: 0.0169 - val_loss: 0.0511 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0071 - val_loss: 0.0052 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 6s 158ms/step - loss: 0.0052 - val_loss: 0.0057 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 6s 171ms/step - loss: 0.0047 - val_loss: 0.0050 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 6s 161ms/step - loss: 0.0046 - val_loss: 0.0035 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 6s 167ms/step - loss: 0.0040 - val_loss: 0.0043 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 6s 161ms/step - loss: 0.0035 - val_loss: 0.0080 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 6s 165ms/step - loss: 0.0035 - val_loss: 0.0088 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 6s 168ms/step - loss: 0.0034 - val_loss: 0.0175 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 6s 165ms/step - loss: 0.0032 - val_loss: 0.0055 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 6s 170ms/step - loss: 0.0027 - val_loss: 0.0066 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 6s 170ms/step - loss: 0.0027 - val_loss: 0.0078 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 6s 170ms/step - loss: 0.0027 - val_loss: 0.0071 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 6s 172ms/step - loss: 0.0026 - val_loss: 0.0062 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 6s 167ms/step - loss: 0.0026 - val_loss: 0.0069 - lr: 2.0000e-04\n",
      "Epoch 15: early stopping\n",
      "23/23 [==============================] - 1s 33ms/step\n",
      "India\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 13s 185ms/step - loss: 0.0605 - val_loss: 0.0105 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 6s 163ms/step - loss: 0.0035 - val_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 6s 163ms/step - loss: 0.0021 - val_loss: 4.4266e-04 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 6s 161ms/step - loss: 0.0015 - val_loss: 6.4674e-04 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 6s 165ms/step - loss: 0.0015 - val_loss: 3.9415e-04 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 6s 165ms/step - loss: 0.0013 - val_loss: 7.5815e-04 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0013 - val_loss: 6.4734e-04 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 6s 159ms/step - loss: 0.0013 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 6s 159ms/step - loss: 0.0012 - val_loss: 6.3853e-04 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0012 - val_loss: 4.4418e-04 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0012 - val_loss: 3.2402e-04 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0012 - val_loss: 4.5574e-04 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 6s 166ms/step - loss: 0.0011 - val_loss: 6.2903e-04 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0012 - val_loss: 5.5597e-04 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 6s 162ms/step - loss: 0.0012 - val_loss: 4.7285e-04 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 6s 165ms/step - loss: 0.0011 - val_loss: 4.2686e-04 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 6s 165ms/step - loss: 0.0011 - val_loss: 7.5225e-04 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 6s 163ms/step - loss: 0.0011 - val_loss: 6.1501e-04 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 6s 162ms/step - loss: 0.0011 - val_loss: 4.1318e-04 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0011 - val_loss: 6.2459e-04 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 6s 166ms/step - loss: 0.0011 - val_loss: 7.4832e-04 - lr: 1.0000e-04\n",
      "Epoch 21: early stopping\n",
      "23/23 [==============================] - 1s 31ms/step\n",
      "Japan\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 13s 185ms/step - loss: 0.0958 - val_loss: 0.0320 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 6s 163ms/step - loss: 0.0064 - val_loss: 0.0033 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 6s 162ms/step - loss: 0.0028 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 6s 169ms/step - loss: 0.0027 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 6s 162ms/step - loss: 0.0024 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 6s 162ms/step - loss: 0.0025 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 6s 166ms/step - loss: 0.0020 - val_loss: 0.0043 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 6s 162ms/step - loss: 0.0023 - val_loss: 0.0025 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 6s 162ms/step - loss: 0.0019 - val_loss: 0.0024 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 6s 163ms/step - loss: 0.0020 - val_loss: 0.0025 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 6s 160ms/step - loss: 0.0019 - val_loss: 0.0025 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 6s 170ms/step - loss: 0.0020 - val_loss: 0.0022 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 6s 163ms/step - loss: 0.0019 - val_loss: 0.0023 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 6s 161ms/step - loss: 0.0019 - val_loss: 0.0021 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 6s 165ms/step - loss: 0.0019 - val_loss: 0.0025 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 6s 160ms/step - loss: 0.0019 - val_loss: 0.0027 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0018 - val_loss: 0.0023 - lr: 2.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0018 - val_loss: 0.0025 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 6s 166ms/step - loss: 0.0017 - val_loss: 0.0023 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 6s 164ms/step - loss: 0.0018 - val_loss: 0.0026 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 6s 161ms/step - loss: 0.0018 - val_loss: 0.0022 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 6s 165ms/step - loss: 0.0017 - val_loss: 0.0020 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 5s 129ms/step - loss: 0.0017 - val_loss: 0.0023 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 5s 130ms/step - loss: 0.0017 - val_loss: 0.0030 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 5s 136ms/step - loss: 0.0017 - val_loss: 0.0022 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 5s 137ms/step - loss: 0.0016 - val_loss: 0.0025 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 5s 136ms/step - loss: 0.0017 - val_loss: 0.0024 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0017 - val_loss: 0.0021 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 5s 138ms/step - loss: 0.0017 - val_loss: 0.0020 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0018 - val_loss: 0.0022 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0018 - val_loss: 0.0020 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 5s 131ms/step - loss: 0.0017 - val_loss: 0.0027 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.0017 - val_loss: 0.0021 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 0.0016 - val_loss: 0.0023 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 0.0016 - val_loss: 0.0023 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 5s 129ms/step - loss: 0.0016 - val_loss: 0.0024 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 6s 157ms/step - loss: 0.0016 - val_loss: 0.0019 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 5s 131ms/step - loss: 0.0016 - val_loss: 0.0023 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 0.0016 - val_loss: 0.0021 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 0.0015 - val_loss: 0.0022 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 5s 130ms/step - loss: 0.0016 - val_loss: 0.0021 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 5s 129ms/step - loss: 0.0016 - val_loss: 0.0019 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 5s 131ms/step - loss: 0.0015 - val_loss: 0.0019 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.0015 - val_loss: 0.0021 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 0.0014 - val_loss: 0.0021 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 4s 120ms/step - loss: 0.0014 - val_loss: 0.0021 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.0015 - val_loss: 0.0023 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0015 - val_loss: 0.0023 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0015 - val_loss: 0.0019 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.0015 - val_loss: 0.0020 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.0014 - val_loss: 0.0020 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0014 - val_loss: 0.0021 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0014 - val_loss: 0.0022 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 0.0014 - val_loss: 0.0021 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0014 - val_loss: 0.0021 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0013 - val_loss: 0.0023 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0014 - val_loss: 0.0021 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0013 - val_loss: 0.0018 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0013 - val_loss: 0.0020 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0014 - val_loss: 0.0021 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 5s 129ms/step - loss: 0.0013 - val_loss: 0.0018 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0013 - val_loss: 0.0020 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0014 - val_loss: 0.0019 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 5s 129ms/step - loss: 0.0014 - val_loss: 0.0019 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0014 - val_loss: 0.0018 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0014 - val_loss: 0.0017 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 0.0013 - val_loss: 0.0018 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0012 - val_loss: 0.0018 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 0.0013 - val_loss: 0.0017 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 5s 131ms/step - loss: 0.0014 - val_loss: 0.0020 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 5s 130ms/step - loss: 0.0013 - val_loss: 0.0020 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0013 - val_loss: 0.0017 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 5s 129ms/step - loss: 0.0014 - val_loss: 0.0024 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0014 - val_loss: 0.0016 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0012 - val_loss: 0.0018 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 4s 122ms/step - loss: 0.0012 - val_loss: 0.0021 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0012 - val_loss: 0.0017 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0013 - val_loss: 0.0016 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 5s 129ms/step - loss: 0.0012 - val_loss: 0.0019 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0012 - val_loss: 0.0020 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0012 - val_loss: 0.0019 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0012 - val_loss: 0.0017 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0012 - val_loss: 0.0017 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.0012 - val_loss: 0.0019 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0012 - val_loss: 0.0016 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0012 - val_loss: 0.0016 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0012 - val_loss: 0.0016 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 4s 119ms/step - loss: 0.0012 - val_loss: 0.0018 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0012 - val_loss: 0.0020 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0012 - val_loss: 0.0018 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0012 - val_loss: 0.0017 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0012 - val_loss: 0.0017 - lr: 1.0000e-04\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0012 - val_loss: 0.0022 - lr: 1.0000e-04\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0012 - val_loss: 0.0016 - lr: 1.0000e-04\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0011 - val_loss: 0.0015 - lr: 1.0000e-04\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0012 - val_loss: 0.0019 - lr: 1.0000e-04\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0011 - val_loss: 0.0016 - lr: 1.0000e-04\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0011 - val_loss: 0.0018 - lr: 1.0000e-04\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0011 - val_loss: 0.0019 - lr: 1.0000e-04\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 0.0011 - val_loss: 0.0019 - lr: 1.0000e-04\n",
      "23/23 [==============================] - 1s 29ms/step\n",
      "Malaysia\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 11s 141ms/step - loss: 0.0373 - val_loss: 0.0214 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0047 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.0019 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0018 - val_loss: 0.0030 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0016 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0016 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.0015 - val_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0014 - val_loss: 0.0040 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0014 - val_loss: 0.0036 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0014 - val_loss: 0.0043 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0014 - val_loss: 0.0038 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0013 - val_loss: 0.0036 - lr: 2.0000e-04\n",
      "Epoch 12: early stopping\n",
      "23/23 [==============================] - 1s 25ms/step\n"
     ]
    }
   ],
   "source": [
    "result = Execute_model(m, xtr, ytr, xt, yt, scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76c1e7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>0.000286</td>\n",
       "      <td>0.000240</td>\n",
       "      <td>0.014901</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.006747</td>\n",
       "      <td>0.003098</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>5.632858</td>\n",
       "      <td>1.948811</td>\n",
       "      <td>0.001067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.013376</td>\n",
       "      <td>0.012442</td>\n",
       "      <td>0.097841</td>\n",
       "      <td>0.015569</td>\n",
       "      <td>0.065657</td>\n",
       "      <td>0.045565</td>\n",
       "      <td>0.035281</td>\n",
       "      <td>2.118775</td>\n",
       "      <td>1.214787</td>\n",
       "      <td>0.023370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE</th>\n",
       "      <td>0.018847</td>\n",
       "      <td>0.010739</td>\n",
       "      <td>0.025142</td>\n",
       "      <td>0.011805</td>\n",
       "      <td>0.009622</td>\n",
       "      <td>0.007054</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>0.030047</td>\n",
       "      <td>0.011043</td>\n",
       "      <td>0.005779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Australia    Europe    Brazil    Canada     China   Denmark  \\\n",
       "Metric                                                                \n",
       "MSE      0.000286  0.000240  0.014901  0.000330  0.006747  0.003098   \n",
       "MAE      0.013376  0.012442  0.097841  0.015569  0.065657  0.045565   \n",
       "MAPE     0.018847  0.010739  0.025142  0.011805  0.009622  0.007054   \n",
       "\n",
       "        Hong Kong     India     Japan  Malaysia  \n",
       "Metric                                           \n",
       "MSE      0.001413  5.632858  1.948811  0.001067  \n",
       "MAE      0.035281  2.118775  1.214787  0.023370  \n",
       "MAPE     0.004499  0.030047  0.011043  0.005779  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61367a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Define the filename\n",
    "filename = \"../results.json\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.isfile(filename):\n",
    "    # If the file exists, load the existing data\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "else:\n",
    "    # If the file doesn't exist, create an empty dictionary\n",
    "    data = {}\n",
    "\n",
    "# Add the result to the dictionary\n",
    "data[\"LSTM\"] = result.to_dict()\n",
    "\n",
    "# Write the dictionary to the file\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump(data, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
