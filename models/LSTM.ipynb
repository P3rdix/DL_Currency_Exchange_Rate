{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01c67d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 11:36:20.338475: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/lib/python3.11/site-packages/h5py/__init__.py:36: UserWarning: h5py is running against HDF5 1.14.2 when it was built against 1.14.1, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bee2e983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>0.9133</td>\n",
       "      <td>1.4419</td>\n",
       "      <td>1.7200</td>\n",
       "      <td>1.0377</td>\n",
       "      <td>6.8273</td>\n",
       "      <td>5.1597</td>\n",
       "      <td>7.7555</td>\n",
       "      <td>46.27</td>\n",
       "      <td>92.55</td>\n",
       "      <td>3.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>0.9143</td>\n",
       "      <td>1.4402</td>\n",
       "      <td>1.7296</td>\n",
       "      <td>1.0371</td>\n",
       "      <td>6.8258</td>\n",
       "      <td>5.1668</td>\n",
       "      <td>7.7564</td>\n",
       "      <td>46.13</td>\n",
       "      <td>91.48</td>\n",
       "      <td>3.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>0.9189</td>\n",
       "      <td>1.4404</td>\n",
       "      <td>1.7292</td>\n",
       "      <td>1.0333</td>\n",
       "      <td>6.8272</td>\n",
       "      <td>5.1638</td>\n",
       "      <td>7.7546</td>\n",
       "      <td>45.72</td>\n",
       "      <td>92.53</td>\n",
       "      <td>3.379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>0.9168</td>\n",
       "      <td>1.4314</td>\n",
       "      <td>1.7409</td>\n",
       "      <td>1.0351</td>\n",
       "      <td>6.8280</td>\n",
       "      <td>5.1981</td>\n",
       "      <td>7.7539</td>\n",
       "      <td>45.67</td>\n",
       "      <td>93.31</td>\n",
       "      <td>3.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>0.9218</td>\n",
       "      <td>1.4357</td>\n",
       "      <td>1.7342</td>\n",
       "      <td>1.0345</td>\n",
       "      <td>6.8274</td>\n",
       "      <td>5.1827</td>\n",
       "      <td>7.7553</td>\n",
       "      <td>45.50</td>\n",
       "      <td>92.70</td>\n",
       "      <td>3.375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  Australia  Europe  Brazil  Canada   China  \\\n",
       "0             0  2010-01-04     0.9133  1.4419  1.7200  1.0377  6.8273   \n",
       "1             1  2010-01-05     0.9143  1.4402  1.7296  1.0371  6.8258   \n",
       "2             2  2010-01-06     0.9189  1.4404  1.7292  1.0333  6.8272   \n",
       "3             3  2010-01-07     0.9168  1.4314  1.7409  1.0351  6.8280   \n",
       "4             4  2010-01-08     0.9218  1.4357  1.7342  1.0345  6.8274   \n",
       "\n",
       "   Denmark  Hong Kong  India  Japan  Malaysia  \n",
       "0   5.1597     7.7555  46.27  92.55     3.396  \n",
       "1   5.1668     7.7564  46.13  91.48     3.385  \n",
       "2   5.1638     7.7546  45.72  92.53     3.379  \n",
       "3   5.1981     7.7539  45.67  93.31     3.368  \n",
       "4   5.1827     7.7553  45.50  92.70     3.375  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5cd89d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data.drop(\"Unnamed: 0.1\", axis = 1, inplace = True)\n",
    "    data.rename(columns={\"Unnamed: 0\": \"Date\"}, inplace=True)\n",
    "    data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
    "    data.set_index(\"Date\", inplace=True)\n",
    "    data.replace(0, np.nan, inplace=True)\n",
    "    display(data)\n",
    "    print(\"Filling missing Values: \")\n",
    "    display(data.interpolate(method='linear', limit_direction='forward'))\n",
    "    data.interpolate(method='linear', limit_direction='forward', inplace = True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f1ffad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>0.9133</td>\n",
       "      <td>1.4419</td>\n",
       "      <td>1.7200</td>\n",
       "      <td>1.0377</td>\n",
       "      <td>6.8273</td>\n",
       "      <td>5.1597</td>\n",
       "      <td>7.7555</td>\n",
       "      <td>46.27</td>\n",
       "      <td>92.55</td>\n",
       "      <td>3.3960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.9143</td>\n",
       "      <td>1.4402</td>\n",
       "      <td>1.7296</td>\n",
       "      <td>1.0371</td>\n",
       "      <td>6.8258</td>\n",
       "      <td>5.1668</td>\n",
       "      <td>7.7564</td>\n",
       "      <td>46.13</td>\n",
       "      <td>91.48</td>\n",
       "      <td>3.3850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>0.9189</td>\n",
       "      <td>1.4404</td>\n",
       "      <td>1.7292</td>\n",
       "      <td>1.0333</td>\n",
       "      <td>6.8272</td>\n",
       "      <td>5.1638</td>\n",
       "      <td>7.7546</td>\n",
       "      <td>45.72</td>\n",
       "      <td>92.53</td>\n",
       "      <td>3.3790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>0.9168</td>\n",
       "      <td>1.4314</td>\n",
       "      <td>1.7409</td>\n",
       "      <td>1.0351</td>\n",
       "      <td>6.8280</td>\n",
       "      <td>5.1981</td>\n",
       "      <td>7.7539</td>\n",
       "      <td>45.67</td>\n",
       "      <td>93.31</td>\n",
       "      <td>3.3680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>0.9218</td>\n",
       "      <td>1.4357</td>\n",
       "      <td>1.7342</td>\n",
       "      <td>1.0345</td>\n",
       "      <td>6.8274</td>\n",
       "      <td>5.1827</td>\n",
       "      <td>7.7553</td>\n",
       "      <td>45.50</td>\n",
       "      <td>92.70</td>\n",
       "      <td>3.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>0.6978</td>\n",
       "      <td>1.1174</td>\n",
       "      <td>4.0507</td>\n",
       "      <td>1.3073</td>\n",
       "      <td>6.9954</td>\n",
       "      <td>6.6829</td>\n",
       "      <td>7.7874</td>\n",
       "      <td>71.45</td>\n",
       "      <td>109.47</td>\n",
       "      <td>4.1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>0.7004</td>\n",
       "      <td>1.1217</td>\n",
       "      <td>4.0152</td>\n",
       "      <td>1.3058</td>\n",
       "      <td>6.9864</td>\n",
       "      <td>6.6589</td>\n",
       "      <td>7.7857</td>\n",
       "      <td>71.30</td>\n",
       "      <td>108.85</td>\n",
       "      <td>4.1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>0.7030</td>\n",
       "      <td>1.1227</td>\n",
       "      <td>4.0190</td>\n",
       "      <td>1.2962</td>\n",
       "      <td>6.9618</td>\n",
       "      <td>6.6554</td>\n",
       "      <td>7.7894</td>\n",
       "      <td>71.36</td>\n",
       "      <td>108.67</td>\n",
       "      <td>4.0918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3648 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Australia  Europe  Brazil  Canada   China  Denmark  Hong Kong  \\\n",
       "Date                                                                        \n",
       "2010-01-04     0.9133  1.4419  1.7200  1.0377  6.8273   5.1597     7.7555   \n",
       "2010-01-05     0.9143  1.4402  1.7296  1.0371  6.8258   5.1668     7.7564   \n",
       "2010-01-06     0.9189  1.4404  1.7292  1.0333  6.8272   5.1638     7.7546   \n",
       "2010-01-07     0.9168  1.4314  1.7409  1.0351  6.8280   5.1981     7.7539   \n",
       "2010-01-08     0.9218  1.4357  1.7342  1.0345  6.8274   5.1827     7.7553   \n",
       "...               ...     ...     ...     ...     ...      ...        ...   \n",
       "2019-12-27     0.6978  1.1174  4.0507  1.3073  6.9954   6.6829     7.7874   \n",
       "2019-12-28        NaN     NaN     NaN     NaN     NaN      NaN        NaN   \n",
       "2019-12-29        NaN     NaN     NaN     NaN     NaN      NaN        NaN   \n",
       "2019-12-30     0.7004  1.1217  4.0152  1.3058  6.9864   6.6589     7.7857   \n",
       "2019-12-31     0.7030  1.1227  4.0190  1.2962  6.9618   6.6554     7.7894   \n",
       "\n",
       "            India   Japan  Malaysia  \n",
       "Date                                 \n",
       "2010-01-04  46.27   92.55    3.3960  \n",
       "2010-01-05  46.13   91.48    3.3850  \n",
       "2010-01-06  45.72   92.53    3.3790  \n",
       "2010-01-07  45.67   93.31    3.3680  \n",
       "2010-01-08  45.50   92.70    3.3750  \n",
       "...           ...     ...       ...  \n",
       "2019-12-27  71.45  109.47    4.1260  \n",
       "2019-12-28    NaN     NaN       NaN  \n",
       "2019-12-29    NaN     NaN       NaN  \n",
       "2019-12-30  71.30  108.85    4.1053  \n",
       "2019-12-31  71.36  108.67    4.0918  \n",
       "\n",
       "[3648 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling missing Values: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>0.913300</td>\n",
       "      <td>1.441900</td>\n",
       "      <td>1.720000</td>\n",
       "      <td>1.0377</td>\n",
       "      <td>6.8273</td>\n",
       "      <td>5.1597</td>\n",
       "      <td>7.755500</td>\n",
       "      <td>46.27</td>\n",
       "      <td>92.550000</td>\n",
       "      <td>3.3960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.914300</td>\n",
       "      <td>1.440200</td>\n",
       "      <td>1.729600</td>\n",
       "      <td>1.0371</td>\n",
       "      <td>6.8258</td>\n",
       "      <td>5.1668</td>\n",
       "      <td>7.756400</td>\n",
       "      <td>46.13</td>\n",
       "      <td>91.480000</td>\n",
       "      <td>3.3850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>0.918900</td>\n",
       "      <td>1.440400</td>\n",
       "      <td>1.729200</td>\n",
       "      <td>1.0333</td>\n",
       "      <td>6.8272</td>\n",
       "      <td>5.1638</td>\n",
       "      <td>7.754600</td>\n",
       "      <td>45.72</td>\n",
       "      <td>92.530000</td>\n",
       "      <td>3.3790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>0.916800</td>\n",
       "      <td>1.431400</td>\n",
       "      <td>1.740900</td>\n",
       "      <td>1.0351</td>\n",
       "      <td>6.8280</td>\n",
       "      <td>5.1981</td>\n",
       "      <td>7.753900</td>\n",
       "      <td>45.67</td>\n",
       "      <td>93.310000</td>\n",
       "      <td>3.3680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>0.921800</td>\n",
       "      <td>1.435700</td>\n",
       "      <td>1.734200</td>\n",
       "      <td>1.0345</td>\n",
       "      <td>6.8274</td>\n",
       "      <td>5.1827</td>\n",
       "      <td>7.755300</td>\n",
       "      <td>45.50</td>\n",
       "      <td>92.700000</td>\n",
       "      <td>3.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>0.697800</td>\n",
       "      <td>1.117400</td>\n",
       "      <td>4.050700</td>\n",
       "      <td>1.3073</td>\n",
       "      <td>6.9954</td>\n",
       "      <td>6.6829</td>\n",
       "      <td>7.787400</td>\n",
       "      <td>71.45</td>\n",
       "      <td>109.470000</td>\n",
       "      <td>4.1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-28</th>\n",
       "      <td>0.698667</td>\n",
       "      <td>1.118833</td>\n",
       "      <td>4.038867</td>\n",
       "      <td>1.3068</td>\n",
       "      <td>6.9924</td>\n",
       "      <td>6.6749</td>\n",
       "      <td>7.786833</td>\n",
       "      <td>71.40</td>\n",
       "      <td>109.263333</td>\n",
       "      <td>4.1191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-29</th>\n",
       "      <td>0.699533</td>\n",
       "      <td>1.120267</td>\n",
       "      <td>4.027033</td>\n",
       "      <td>1.3063</td>\n",
       "      <td>6.9894</td>\n",
       "      <td>6.6669</td>\n",
       "      <td>7.786267</td>\n",
       "      <td>71.35</td>\n",
       "      <td>109.056667</td>\n",
       "      <td>4.1122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>0.700400</td>\n",
       "      <td>1.121700</td>\n",
       "      <td>4.015200</td>\n",
       "      <td>1.3058</td>\n",
       "      <td>6.9864</td>\n",
       "      <td>6.6589</td>\n",
       "      <td>7.785700</td>\n",
       "      <td>71.30</td>\n",
       "      <td>108.850000</td>\n",
       "      <td>4.1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>0.703000</td>\n",
       "      <td>1.122700</td>\n",
       "      <td>4.019000</td>\n",
       "      <td>1.2962</td>\n",
       "      <td>6.9618</td>\n",
       "      <td>6.6554</td>\n",
       "      <td>7.789400</td>\n",
       "      <td>71.36</td>\n",
       "      <td>108.670000</td>\n",
       "      <td>4.0918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3648 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Australia    Europe    Brazil  Canada   China  Denmark  Hong Kong  \\\n",
       "Date                                                                            \n",
       "2010-01-04   0.913300  1.441900  1.720000  1.0377  6.8273   5.1597   7.755500   \n",
       "2010-01-05   0.914300  1.440200  1.729600  1.0371  6.8258   5.1668   7.756400   \n",
       "2010-01-06   0.918900  1.440400  1.729200  1.0333  6.8272   5.1638   7.754600   \n",
       "2010-01-07   0.916800  1.431400  1.740900  1.0351  6.8280   5.1981   7.753900   \n",
       "2010-01-08   0.921800  1.435700  1.734200  1.0345  6.8274   5.1827   7.755300   \n",
       "...               ...       ...       ...     ...     ...      ...        ...   \n",
       "2019-12-27   0.697800  1.117400  4.050700  1.3073  6.9954   6.6829   7.787400   \n",
       "2019-12-28   0.698667  1.118833  4.038867  1.3068  6.9924   6.6749   7.786833   \n",
       "2019-12-29   0.699533  1.120267  4.027033  1.3063  6.9894   6.6669   7.786267   \n",
       "2019-12-30   0.700400  1.121700  4.015200  1.3058  6.9864   6.6589   7.785700   \n",
       "2019-12-31   0.703000  1.122700  4.019000  1.2962  6.9618   6.6554   7.789400   \n",
       "\n",
       "            India       Japan  Malaysia  \n",
       "Date                                     \n",
       "2010-01-04  46.27   92.550000    3.3960  \n",
       "2010-01-05  46.13   91.480000    3.3850  \n",
       "2010-01-06  45.72   92.530000    3.3790  \n",
       "2010-01-07  45.67   93.310000    3.3680  \n",
       "2010-01-08  45.50   92.700000    3.3750  \n",
       "...           ...         ...       ...  \n",
       "2019-12-27  71.45  109.470000    4.1260  \n",
       "2019-12-28  71.40  109.263333    4.1191  \n",
       "2019-12-29  71.35  109.056667    4.1122  \n",
       "2019-12-30  71.30  108.850000    4.1053  \n",
       "2019-12-31  71.36  108.670000    4.0918  \n",
       "\n",
       "[3648 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab94630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOOK_BACK = 30\n",
    "PREDICT_DAY = 1\n",
    "SPLIT_RATIO = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "673eb188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Data(\n",
    "    data, lookback=LOOK_BACK, pred_len=PREDICT_DAY, split_ratio=SPLIT_RATIO, model=\"FNN\"\n",
    "):\n",
    "    if lookback < 2:\n",
    "        print(\"ERROR: Lookback too small\")\n",
    "        return -1\n",
    "\n",
    "    # declarations\n",
    "\n",
    "    x = {}\n",
    "    y = {}\n",
    "    xtr = {}\n",
    "    xt = {}\n",
    "    ytr = {}\n",
    "    yt = {}\n",
    "    scalers = {}\n",
    "\n",
    "    # Creating stepped data\n",
    "\n",
    "    for i in data.columns:\n",
    "        xtemp = pd.DataFrame(data[i])\n",
    "        for j in range(1, lookback + 1):\n",
    "            xtemp[i + str(j)] = data[i].shift(-1 * j)\n",
    "        x[i] = xtemp.dropna()\n",
    "\n",
    "    # Splitting data into x and y\n",
    "\n",
    "    for i in x.keys():\n",
    "        y[i] = pd.DataFrame(x[i].iloc[:, -pred_len])\n",
    "        x[i] = x[i].iloc[:, :-pred_len]\n",
    "\n",
    "    # Normalizing x and y values\n",
    "\n",
    "    for i in x.keys():\n",
    "        scalers[i + \"_x\"] = MinMaxScaler(feature_range=(0, 1))\n",
    "        x[i] = scalers[i + \"_x\"].fit_transform(x[i])\n",
    "        scalers[i + \"_y\"] = MinMaxScaler(feature_range=(0, 1))\n",
    "        y[i] = scalers[i + \"_y\"].fit_transform(y[i])\n",
    "\n",
    "    # setting train and test sizes\n",
    "\n",
    "    tr_len = int(split_ratio * y[\"India\"].shape[0])\n",
    "    t_len = y[\"India\"].shape[0] - tr_len\n",
    "\n",
    "    # creating training and testing data\n",
    "\n",
    "    for i in x.keys():\n",
    "        xtr[i] = x[i][:tr_len]\n",
    "        ytr[i] = y[i][:tr_len]\n",
    "        xt[i] = x[i][-t_len:]\n",
    "        yt[i] = y[i][-t_len:]\n",
    "\n",
    "    # returning pertinent data\n",
    "\n",
    "    return x, y, xtr, xt, ytr, yt, scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc04bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,xtr,xt,ytr,yt,scalers = Create_Data(data, model=\"RNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "333787a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_model(x,y, lookback = LOOK_BACK, Pred_size = PREDICT_DAY):\n",
    "    models = {}\n",
    "    for i in x.keys():\n",
    "        models[i] = Sequential()\n",
    "        models[i].add(LSTM(32,input_shape=(LOOK_BACK,1,),return_sequences=True,activation=\"relu\"))\n",
    "        models[i].add(LSTM(64,return_sequences=True,activation=\"relu\"))\n",
    "        models[i].add(Dropout(0.2))\n",
    "        models[i].add(LSTM(128,return_sequences=True,activation=\"relu\"))\n",
    "        models[i].add(Dropout(0.2))\n",
    "        models[i].add(LSTM(64,return_sequences=True,activation=\"relu\"))\n",
    "        models[i].add(Dropout(0.2))\n",
    "        models[i].add(LSTM(16,activation=\"relu\"))\n",
    "        models[i].add(Dense(Pred_size))\n",
    "        models[i].compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "        print(i)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2df7ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 11:36:22.175481: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 11:36:22.202078: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 11:36:22.202533: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 11:36:22.206451: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 11:36:22.206879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 11:36:22.207188: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 11:36:22.311118: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 11:36:22.311351: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 11:36:22.311534: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 11:36:22.311679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4807 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2023-10-12 11:36:22.312310: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Australia\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Europe\n",
      "WARNING:tensorflow:Layer lstm_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Brazil\n",
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Canada\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "China\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Denmark\n",
      "WARNING:tensorflow:Layer lstm_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Hong Kong\n",
      "WARNING:tensorflow:Layer lstm_35 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_37 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_38 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_39 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "India\n",
      "WARNING:tensorflow:Layer lstm_40 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_41 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_42 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_43 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_44 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Japan\n",
      "WARNING:tensorflow:Layer lstm_45 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_46 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_47 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Malaysia\n"
     ]
    }
   ],
   "source": [
    "m = Create_model(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "599dc5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Execute_model(model, xtr, ytr, xt, yt, scaler):\n",
    "    MAPE = {}\n",
    "    MAE = {}\n",
    "    MSE = {}\n",
    "    for i in model.keys():\n",
    "        print(i)\n",
    "        # Creating EarlyStopping and ReduceLROnPlateau callbacks\n",
    "        es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=1, patience=10)\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", factor=0.2, patience=5, min_lr=0.0001\n",
    "        )\n",
    "\n",
    "        # Training the model with EarlyStopping and ReduceLROnPlateau callbacks\n",
    "        model[i].fit(\n",
    "            xtr[i],\n",
    "            ytr[i],\n",
    "            epochs=100,\n",
    "            batch_size=64,\n",
    "            verbose=1,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[es, reduce_lr],\n",
    "        )\n",
    "\n",
    "        # collecting predicted and actual values\n",
    "        temp = model[i].predict(xt[i])\n",
    "        pred = scaler[i + \"_y\"].inverse_transform(temp)\n",
    "        act = scaler[i + \"_y\"].inverse_transform(yt[i])\n",
    "\n",
    "        # calculating Mean Square Error, Mean Absolute Error, and Mean Absolute Error\n",
    "        MSE[i] = mean_squared_error(act, pred)\n",
    "        MAE[i] = mean_absolute_error(act, pred)\n",
    "        MAPE[i] = mean_absolute_percentage_error(act, pred)\n",
    "\n",
    "    # Tabulating Data\n",
    "    results = pd.DataFrame([MSE, MAE, MAPE])\n",
    "    results[\"Metric\"] = [\"MSE\", \"MAE\", \"MAPE\"]\n",
    "    results.set_index(\"Metric\", inplace=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd35db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 11:36:29.967216: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555db64426e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-10-12 11:36:29.967252: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2023-10-12 11:36:29.973356: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-10-12 11:36:29.987123: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8902\n",
      "2023-10-12 11:36:30.089223: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 12s 147ms/step - loss: 0.1255 - val_loss: 0.0275 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.0105 - val_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0045 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 4s 120ms/step - loss: 0.0039 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0035 - val_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0035 - val_loss: 0.0010 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0034 - val_loss: 0.0025 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0032 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0028 - val_loss: 0.0032 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0029 - val_loss: 7.5226e-04 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0026 - val_loss: 6.7142e-04 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0026 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 4s 122ms/step - loss: 0.0024 - val_loss: 5.7249e-04 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.0024 - val_loss: 6.2151e-04 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0021 - val_loss: 7.1968e-04 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0021 - val_loss: 0.0010 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0019 - val_loss: 9.9605e-04 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 0.0021 - val_loss: 5.8505e-04 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0017 - val_loss: 6.6762e-04 - lr: 2.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0018 - val_loss: 6.7817e-04 - lr: 2.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 0.0017 - val_loss: 8.0693e-04 - lr: 2.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0017 - val_loss: 6.5159e-04 - lr: 2.0000e-04\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0016 - val_loss: 7.6524e-04 - lr: 2.0000e-04\n",
      "Epoch 23: early stopping\n",
      "23/23 [==============================] - 1s 17ms/step\n",
      "Europe\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 11s 146ms/step - loss: 0.0893 - val_loss: 0.0261 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0079 - val_loss: 0.0028 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 136ms/step - loss: 0.0042 - val_loss: 0.0030 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 133ms/step - loss: 0.0041 - val_loss: 0.0033 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 0.0042 - val_loss: 0.0028 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 130ms/step - loss: 0.0040 - val_loss: 0.0028 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 0.0039 - val_loss: 0.0025 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 0.0035 - val_loss: 0.0035 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0036 - val_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0033 - val_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 129ms/step - loss: 0.0031 - val_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 130ms/step - loss: 0.0031 - val_loss: 0.0017 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0032 - val_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 129ms/step - loss: 0.0029 - val_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 131ms/step - loss: 0.0027 - val_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 5s 131ms/step - loss: 0.0025 - val_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0023 - val_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0021 - val_loss: 0.0018 - lr: 2.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0021 - val_loss: 0.0013 - lr: 2.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0021 - val_loss: 0.0015 - lr: 2.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 5s 129ms/step - loss: 0.0020 - val_loss: 0.0015 - lr: 2.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0019 - val_loss: 0.0015 - lr: 2.0000e-04\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0020 - val_loss: 0.0014 - lr: 2.0000e-04\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0020 - val_loss: 0.0012 - lr: 2.0000e-04\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 0.0020 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0019 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0019 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.0019 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0019 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 4s 122ms/step - loss: 0.0019 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 0.0019 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0019 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0019 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0019 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.0019 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0019 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 5s 131ms/step - loss: 0.0019 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 5s 131ms/step - loss: 0.0019 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 5s 133ms/step - loss: 0.0019 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0018 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0018 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0018 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 5s 130ms/step - loss: 0.0018 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0018 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0018 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0018 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.0018 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0018 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0018 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0017 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 50: early stopping\n",
      "23/23 [==============================] - 1s 22ms/step\n",
      "Brazil\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 11s 141ms/step - loss: 0.0354 - val_loss: 0.0089 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 0.0018 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0013 - val_loss: 7.1600e-04 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0012 - val_loss: 6.6963e-04 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 0.0012 - val_loss: 6.9880e-04 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0012 - val_loss: 6.4565e-04 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0012 - val_loss: 7.3973e-04 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0011 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 9.1907e-04 - val_loss: 8.0698e-04 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 8.6708e-04 - val_loss: 0.0011 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 4s 117ms/step - loss: 8.9110e-04 - val_loss: 0.0013 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 9.1921e-04 - val_loss: 6.8463e-04 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 9.0002e-04 - val_loss: 8.8085e-04 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 8.8434e-04 - val_loss: 7.5711e-04 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 8.9686e-04 - val_loss: 7.9788e-04 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 9.2729e-04 - val_loss: 6.2662e-04 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 8.7135e-04 - val_loss: 8.5376e-04 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 8.5843e-04 - val_loss: 9.0225e-04 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 8.5054e-04 - val_loss: 9.1238e-04 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 8.6347e-04 - val_loss: 8.4470e-04 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 8.6035e-04 - val_loss: 7.6746e-04 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 8.6272e-04 - val_loss: 6.1294e-04 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 8.4408e-04 - val_loss: 8.3652e-04 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 8.9187e-04 - val_loss: 6.5795e-04 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 8.5971e-04 - val_loss: 6.2027e-04 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 8.3731e-04 - val_loss: 8.5947e-04 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 8.7748e-04 - val_loss: 6.0709e-04 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 4s 119ms/step - loss: 8.6436e-04 - val_loss: 6.1839e-04 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 4s 120ms/step - loss: 8.1788e-04 - val_loss: 8.5646e-04 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 8.8137e-04 - val_loss: 6.2959e-04 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 8.3661e-04 - val_loss: 6.5584e-04 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 4s 118ms/step - loss: 8.3466e-04 - val_loss: 6.0180e-04 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 8.1901e-04 - val_loss: 6.2656e-04 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 7.9606e-04 - val_loss: 6.2186e-04 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 8.0954e-04 - val_loss: 7.6167e-04 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 8.2186e-04 - val_loss: 7.6368e-04 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 8.0151e-04 - val_loss: 8.7951e-04 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 7.7067e-04 - val_loss: 6.5600e-04 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 8.5513e-04 - val_loss: 6.1229e-04 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 7.9286e-04 - val_loss: 6.4974e-04 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 7.8595e-04 - val_loss: 5.6107e-04 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 7.8637e-04 - val_loss: 6.7058e-04 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 7.8488e-04 - val_loss: 5.9426e-04 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 7.9035e-04 - val_loss: 7.1214e-04 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 7.8062e-04 - val_loss: 9.9677e-04 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 8.2667e-04 - val_loss: 8.5755e-04 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 8.1444e-04 - val_loss: 6.0275e-04 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 7.9457e-04 - val_loss: 5.6951e-04 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 7.7461e-04 - val_loss: 5.7353e-04 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 4s 120ms/step - loss: 7.9934e-04 - val_loss: 5.9936e-04 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 7.4951e-04 - val_loss: 6.2959e-04 - lr: 1.0000e-04\n",
      "Epoch 51: early stopping\n",
      "23/23 [==============================] - 1s 22ms/step\n",
      "Canada\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 11s 144ms/step - loss: 0.0404 - val_loss: 0.0039 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0029 - val_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0018 - val_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0015 - val_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0016 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0014 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0014 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0013 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0013 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0013 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0013 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0012 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0011 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 4s 120ms/step - loss: 0.0011 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0012 - val_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0010 - val_loss: 0.0019 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0010 - val_loss: 0.0022 - lr: 2.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 9.9031e-04 - val_loss: 0.0012 - lr: 2.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 9.7840e-04 - val_loss: 0.0011 - lr: 2.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 9.4896e-04 - val_loss: 0.0012 - lr: 2.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0010 - val_loss: 0.0011 - lr: 2.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 4s 120ms/step - loss: 0.0010 - val_loss: 0.0010 - lr: 2.0000e-04\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 9.7458e-04 - val_loss: 0.0018 - lr: 2.0000e-04\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 4s 122ms/step - loss: 9.3278e-04 - val_loss: 9.7938e-04 - lr: 2.0000e-04\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 9.0519e-04 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 8.9148e-04 - val_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 8.6872e-04 - val_loss: 0.0014 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 9.1099e-04 - val_loss: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 8.8841e-04 - val_loss: 9.3875e-04 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 8.8715e-04 - val_loss: 0.0017 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 5s 121ms/step - loss: 8.6550e-04 - val_loss: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 8.9579e-04 - val_loss: 0.0017 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 8.6558e-04 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 8.9385e-04 - val_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 8.7473e-04 - val_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 8.7173e-04 - val_loss: 0.0017 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 8.1959e-04 - val_loss: 0.0020 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 8.8062e-04 - val_loss: 0.0017 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 4s 118ms/step - loss: 8.7193e-04 - val_loss: 0.0023 - lr: 1.0000e-04\n",
      "Epoch 39: early stopping\n",
      "23/23 [==============================] - 1s 30ms/step\n",
      "China\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 10s 128ms/step - loss: 0.0263 - val_loss: 0.0100 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 4s 114ms/step - loss: 0.0018 - val_loss: 0.0033 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 4s 114ms/step - loss: 0.0010 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 4s 117ms/step - loss: 9.3683e-04 - val_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 9.2871e-04 - val_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 9.4209e-04 - val_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 9.5285e-04 - val_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 129ms/step - loss: 9.1864e-04 - val_loss: 0.0026 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 7.7777e-04 - val_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 121ms/step - loss: 8.2394e-04 - val_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 7.0509e-04 - val_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 7.8990e-04 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 6.6589e-04 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 6.7976e-04 - val_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 6.4006e-04 - val_loss: 0.0019 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 4s 119ms/step - loss: 6.4357e-04 - val_loss: 0.0023 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 5.9774e-04 - val_loss: 0.0019 - lr: 2.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 5.8408e-04 - val_loss: 0.0017 - lr: 2.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 5.9692e-04 - val_loss: 0.0017 - lr: 2.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 5.8624e-04 - val_loss: 0.0017 - lr: 2.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 5.7081e-04 - val_loss: 0.0016 - lr: 2.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 5.6175e-04 - val_loss: 0.0019 - lr: 2.0000e-04\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 5.7300e-04 - val_loss: 0.0018 - lr: 2.0000e-04\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 5.6577e-04 - val_loss: 0.0017 - lr: 2.0000e-04\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 5.6518e-04 - val_loss: 0.0015 - lr: 2.0000e-04\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 5s 121ms/step - loss: 5.8312e-04 - val_loss: 0.0015 - lr: 2.0000e-04\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 5.5350e-04 - val_loss: 0.0016 - lr: 2.0000e-04\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 5.3627e-04 - val_loss: 0.0014 - lr: 2.0000e-04\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 5.5433e-04 - val_loss: 0.0014 - lr: 2.0000e-04\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 5.2021e-04 - val_loss: 0.0018 - lr: 2.0000e-04\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 5.0329e-04 - val_loss: 0.0020 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 5.3213e-04 - val_loss: 0.0023 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 5.3733e-04 - val_loss: 0.0016 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 5.1079e-04 - val_loss: 0.0015 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 5.1293e-04 - val_loss: 0.0016 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 4.9961e-04 - val_loss: 0.0017 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 4.8702e-04 - val_loss: 0.0017 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 5.0410e-04 - val_loss: 0.0015 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 5.0447e-04 - val_loss: 0.0015 - lr: 1.0000e-04\n",
      "Epoch 39: early stopping\n",
      "23/23 [==============================] - 1s 20ms/step\n",
      "Denmark\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 11s 139ms/step - loss: 0.0417 - val_loss: 0.0054 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 4s 119ms/step - loss: 0.0054 - val_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0038 - val_loss: 0.0043 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0034 - val_loss: 0.0017 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 4s 120ms/step - loss: 0.0033 - val_loss: 0.0040 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0032 - val_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0028 - val_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 0.0027 - val_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0030 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0022 - val_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0022 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0019 - val_loss: 0.0031 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0021 - val_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0017 - val_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 0.0015 - val_loss: 0.0029 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0016 - val_loss: 0.0045 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 4s 120ms/step - loss: 0.0014 - val_loss: 0.0024 - lr: 2.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0014 - val_loss: 0.0020 - lr: 2.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.0014 - val_loss: 0.0019 - lr: 2.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0013 - val_loss: 0.0025 - lr: 2.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0013 - val_loss: 0.0025 - lr: 2.0000e-04\n",
      "Epoch 21: early stopping\n",
      "23/23 [==============================] - 1s 22ms/step\n",
      "Hong Kong\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 10s 136ms/step - loss: 0.0121 - val_loss: 0.0071 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0055 - val_loss: 0.0044 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 4s 120ms/step - loss: 0.0048 - val_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 121ms/step - loss: 0.0043 - val_loss: 0.0089 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0040 - val_loss: 0.0044 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0036 - val_loss: 0.0043 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0035 - val_loss: 0.0063 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0031 - val_loss: 0.0035 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 4s 120ms/step - loss: 0.0029 - val_loss: 0.0053 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0027 - val_loss: 0.0052 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0027 - val_loss: 0.0078 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 4s 116ms/step - loss: 0.0026 - val_loss: 0.0060 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 4s 114ms/step - loss: 0.0026 - val_loss: 0.0047 - lr: 2.0000e-04\n",
      "Epoch 13: early stopping\n",
      "23/23 [==============================] - 1s 25ms/step\n",
      "India\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 10s 128ms/step - loss: 0.0775 - val_loss: 0.0050 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 4s 120ms/step - loss: 0.0032 - val_loss: 9.9568e-04 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0024 - val_loss: 5.0693e-04 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0020 - val_loss: 6.5573e-04 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0018 - val_loss: 3.5203e-04 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 0.0019 - val_loss: 5.7971e-04 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0016 - val_loss: 3.7199e-04 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0014 - val_loss: 3.3886e-04 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0015 - val_loss: 0.0017 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0016 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0014 - val_loss: 3.1107e-04 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0012 - val_loss: 2.9052e-04 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 4s 120ms/step - loss: 0.0013 - val_loss: 5.5454e-04 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0012 - val_loss: 2.9369e-04 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0012 - val_loss: 4.5611e-04 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0012 - val_loss: 2.9828e-04 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0012 - val_loss: 2.9157e-04 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0012 - val_loss: 3.6228e-04 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0012 - val_loss: 3.1867e-04 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0011 - val_loss: 2.8115e-04 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0011 - val_loss: 2.9268e-04 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 5s 121ms/step - loss: 0.0012 - val_loss: 5.5902e-04 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0012 - val_loss: 3.3742e-04 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.0011 - val_loss: 2.9776e-04 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 0.0012 - val_loss: 3.7566e-04 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0010 - val_loss: 3.0462e-04 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 5s 129ms/step - loss: 0.0011 - val_loss: 3.3434e-04 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0011 - val_loss: 3.1102e-04 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 4s 116ms/step - loss: 0.0011 - val_loss: 2.9464e-04 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 5s 121ms/step - loss: 0.0012 - val_loss: 2.9817e-04 - lr: 1.0000e-04\n",
      "Epoch 30: early stopping\n",
      "23/23 [==============================] - 1s 29ms/step\n",
      "Japan\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 11s 139ms/step - loss: 0.0754 - val_loss: 0.0047 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 4s 118ms/step - loss: 0.0039 - val_loss: 0.0035 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 4s 120ms/step - loss: 0.0024 - val_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 4s 119ms/step - loss: 0.0021 - val_loss: 0.0034 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 4s 117ms/step - loss: 0.0020 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 121ms/step - loss: 0.0018 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0018 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 0.0017 - val_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 121ms/step - loss: 0.0016 - val_loss: 0.0031 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 4s 119ms/step - loss: 0.0016 - val_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 4s 122ms/step - loss: 0.0015 - val_loss: 0.0028 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0015 - val_loss: 0.0026 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.0014 - val_loss: 0.0017 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0014 - val_loss: 0.0017 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 0.0014 - val_loss: 0.0017 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 0.0013 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 5s 128ms/step - loss: 0.0012 - val_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0012 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 4s 119ms/step - loss: 0.0012 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 0.0013 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 0.0014 - val_loss: 0.0017 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 0.0013 - val_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0012 - val_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 5s 129ms/step - loss: 9.6999e-04 - val_loss: 0.0014 - lr: 2.0000e-04\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0011 - val_loss: 0.0011 - lr: 2.0000e-04\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 4s 120ms/step - loss: 0.0010 - val_loss: 0.0014 - lr: 2.0000e-04\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 4s 122ms/step - loss: 9.8781e-04 - val_loss: 0.0014 - lr: 2.0000e-04\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 9.6212e-04 - val_loss: 0.0014 - lr: 2.0000e-04\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 9.3914e-04 - val_loss: 0.0017 - lr: 2.0000e-04\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 9.4775e-04 - val_loss: 0.0013 - lr: 2.0000e-04\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 9.3486e-04 - val_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 4s 122ms/step - loss: 9.3816e-04 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 9.4376e-04 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 9.0474e-04 - val_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 9.0311e-04 - val_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 9.4520e-04 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 5s 129ms/step - loss: 8.9915e-04 - val_loss: 0.0015 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 5s 130ms/step - loss: 9.0360e-04 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 5s 132ms/step - loss: 8.8629e-04 - val_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 9.2828e-04 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 8.8816e-04 - val_loss: 0.0014 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 9.3828e-04 - val_loss: 0.0018 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 8.9920e-04 - val_loss: 0.0014 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 9.3224e-04 - val_loss: 0.0015 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 8.9250e-04 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 5s 122ms/step - loss: 8.7306e-04 - val_loss: 0.0016 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 4s 122ms/step - loss: 8.7929e-04 - val_loss: 0.0018 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 5s 125ms/step - loss: 8.9240e-04 - val_loss: 0.0013 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 9.3569e-04 - val_loss: 0.0015 - lr: 1.0000e-04\n",
      "Epoch 49: early stopping\n",
      "23/23 [==============================] - 1s 24ms/step\n",
      "Malaysia\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 11s 145ms/step - loss: 0.0299 - val_loss: 0.0068 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0030 - val_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 4s 119ms/step - loss: 0.0021 - val_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0019 - val_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0016 - val_loss: 0.0043 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 4s 120ms/step - loss: 0.0015 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 0.0016 - val_loss: 0.0017 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 126ms/step - loss: 0.0015 - val_loss: 0.0033 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0016 - val_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0015 - val_loss: 0.0017 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0014 - val_loss: 0.0051 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 4s 121ms/step - loss: 0.0013 - val_loss: 0.0027 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 123ms/step - loss: 0.0011 - val_loss: 0.0019 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 127ms/step - loss: 0.0012 - val_loss: 0.0026 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0011 - val_loss: 0.0020 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 5s 124ms/step - loss: 0.0011 - val_loss: 0.0023 - lr: 2.0000e-04\n",
      "Epoch 16: early stopping\n",
      "23/23 [==============================] - 1s 32ms/step\n"
     ]
    }
   ],
   "source": [
    "result = Execute_model(m,xtr,ytr,xt,yt,scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76c1e7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>0.023463</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.003628</td>\n",
       "      <td>0.003449</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>2.885308</td>\n",
       "      <td>2.024695</td>\n",
       "      <td>0.000751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.011314</td>\n",
       "      <td>0.009021</td>\n",
       "      <td>0.126029</td>\n",
       "      <td>0.022150</td>\n",
       "      <td>0.045143</td>\n",
       "      <td>0.048273</td>\n",
       "      <td>0.031039</td>\n",
       "      <td>1.418237</td>\n",
       "      <td>1.250376</td>\n",
       "      <td>0.021861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE</th>\n",
       "      <td>0.015950</td>\n",
       "      <td>0.007802</td>\n",
       "      <td>0.032131</td>\n",
       "      <td>0.016797</td>\n",
       "      <td>0.006645</td>\n",
       "      <td>0.007428</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>0.020039</td>\n",
       "      <td>0.011368</td>\n",
       "      <td>0.005354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Australia    Europe    Brazil    Canada     China   Denmark  \\\n",
       "Metric                                                                \n",
       "MSE      0.000195  0.000130  0.023463  0.000594  0.003628  0.003449   \n",
       "MAE      0.011314  0.009021  0.126029  0.022150  0.045143  0.048273   \n",
       "MAPE     0.015950  0.007802  0.032131  0.016797  0.006645  0.007428   \n",
       "\n",
       "        Hong Kong     India     Japan  Malaysia  \n",
       "Metric                                           \n",
       "MSE      0.001130  2.885308  2.024695  0.000751  \n",
       "MAE      0.031039  1.418237  1.250376  0.021861  \n",
       "MAPE     0.003958  0.020039  0.011368  0.005354  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d61367a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Define the filename\n",
    "filename = \"../results.json\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.isfile(filename):\n",
    "    # If the file exists, load the existing data\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "else:\n",
    "    # If the file doesn't exist, create an empty dictionary\n",
    "    data = {}\n",
    "\n",
    "# Add the result to the dictionary\n",
    "data[\"LSTM\"] = result.to_dict()\n",
    "\n",
    "# Write the dictionary to the file\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump(data, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
