{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e36a0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 11:31:45.476544: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/lib/python3.11/site-packages/h5py/__init__.py:36: UserWarning: h5py is running against HDF5 1.14.2 when it was built against 1.14.1, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, MultiHeadAttention, Flatten, Add\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a649d955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>0.9133</td>\n",
       "      <td>1.4419</td>\n",
       "      <td>1.7200</td>\n",
       "      <td>1.0377</td>\n",
       "      <td>6.8273</td>\n",
       "      <td>5.1597</td>\n",
       "      <td>7.7555</td>\n",
       "      <td>46.27</td>\n",
       "      <td>92.55</td>\n",
       "      <td>3.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>0.9143</td>\n",
       "      <td>1.4402</td>\n",
       "      <td>1.7296</td>\n",
       "      <td>1.0371</td>\n",
       "      <td>6.8258</td>\n",
       "      <td>5.1668</td>\n",
       "      <td>7.7564</td>\n",
       "      <td>46.13</td>\n",
       "      <td>91.48</td>\n",
       "      <td>3.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>0.9189</td>\n",
       "      <td>1.4404</td>\n",
       "      <td>1.7292</td>\n",
       "      <td>1.0333</td>\n",
       "      <td>6.8272</td>\n",
       "      <td>5.1638</td>\n",
       "      <td>7.7546</td>\n",
       "      <td>45.72</td>\n",
       "      <td>92.53</td>\n",
       "      <td>3.379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>0.9168</td>\n",
       "      <td>1.4314</td>\n",
       "      <td>1.7409</td>\n",
       "      <td>1.0351</td>\n",
       "      <td>6.8280</td>\n",
       "      <td>5.1981</td>\n",
       "      <td>7.7539</td>\n",
       "      <td>45.67</td>\n",
       "      <td>93.31</td>\n",
       "      <td>3.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>0.9218</td>\n",
       "      <td>1.4357</td>\n",
       "      <td>1.7342</td>\n",
       "      <td>1.0345</td>\n",
       "      <td>6.8274</td>\n",
       "      <td>5.1827</td>\n",
       "      <td>7.7553</td>\n",
       "      <td>45.50</td>\n",
       "      <td>92.70</td>\n",
       "      <td>3.375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  Australia  Europe  Brazil  Canada   China  \\\n",
       "0             0  2010-01-04     0.9133  1.4419  1.7200  1.0377  6.8273   \n",
       "1             1  2010-01-05     0.9143  1.4402  1.7296  1.0371  6.8258   \n",
       "2             2  2010-01-06     0.9189  1.4404  1.7292  1.0333  6.8272   \n",
       "3             3  2010-01-07     0.9168  1.4314  1.7409  1.0351  6.8280   \n",
       "4             4  2010-01-08     0.9218  1.4357  1.7342  1.0345  6.8274   \n",
       "\n",
       "   Denmark  Hong Kong  India  Japan  Malaysia  \n",
       "0   5.1597     7.7555  46.27  92.55     3.396  \n",
       "1   5.1668     7.7564  46.13  91.48     3.385  \n",
       "2   5.1638     7.7546  45.72  92.53     3.379  \n",
       "3   5.1981     7.7539  45.67  93.31     3.368  \n",
       "4   5.1827     7.7553  45.50  92.70     3.375  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c86e1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(\"Unnamed: 0.1\", axis=1, inplace=True)\n",
    "data.rename(columns={\"Unnamed: 0\": \"Date\"}, inplace=True)\n",
    "data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
    "data.set_index(\"Date\", inplace=True)\n",
    "data.replace(0, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "747e0996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>0.9133</td>\n",
       "      <td>1.4419</td>\n",
       "      <td>1.7200</td>\n",
       "      <td>1.0377</td>\n",
       "      <td>6.8273</td>\n",
       "      <td>5.1597</td>\n",
       "      <td>7.7555</td>\n",
       "      <td>46.27</td>\n",
       "      <td>92.55</td>\n",
       "      <td>3.3960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.9143</td>\n",
       "      <td>1.4402</td>\n",
       "      <td>1.7296</td>\n",
       "      <td>1.0371</td>\n",
       "      <td>6.8258</td>\n",
       "      <td>5.1668</td>\n",
       "      <td>7.7564</td>\n",
       "      <td>46.13</td>\n",
       "      <td>91.48</td>\n",
       "      <td>3.3850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>0.9189</td>\n",
       "      <td>1.4404</td>\n",
       "      <td>1.7292</td>\n",
       "      <td>1.0333</td>\n",
       "      <td>6.8272</td>\n",
       "      <td>5.1638</td>\n",
       "      <td>7.7546</td>\n",
       "      <td>45.72</td>\n",
       "      <td>92.53</td>\n",
       "      <td>3.3790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>0.9168</td>\n",
       "      <td>1.4314</td>\n",
       "      <td>1.7409</td>\n",
       "      <td>1.0351</td>\n",
       "      <td>6.8280</td>\n",
       "      <td>5.1981</td>\n",
       "      <td>7.7539</td>\n",
       "      <td>45.67</td>\n",
       "      <td>93.31</td>\n",
       "      <td>3.3680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>0.9218</td>\n",
       "      <td>1.4357</td>\n",
       "      <td>1.7342</td>\n",
       "      <td>1.0345</td>\n",
       "      <td>6.8274</td>\n",
       "      <td>5.1827</td>\n",
       "      <td>7.7553</td>\n",
       "      <td>45.50</td>\n",
       "      <td>92.70</td>\n",
       "      <td>3.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>0.6978</td>\n",
       "      <td>1.1174</td>\n",
       "      <td>4.0507</td>\n",
       "      <td>1.3073</td>\n",
       "      <td>6.9954</td>\n",
       "      <td>6.6829</td>\n",
       "      <td>7.7874</td>\n",
       "      <td>71.45</td>\n",
       "      <td>109.47</td>\n",
       "      <td>4.1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>0.7004</td>\n",
       "      <td>1.1217</td>\n",
       "      <td>4.0152</td>\n",
       "      <td>1.3058</td>\n",
       "      <td>6.9864</td>\n",
       "      <td>6.6589</td>\n",
       "      <td>7.7857</td>\n",
       "      <td>71.30</td>\n",
       "      <td>108.85</td>\n",
       "      <td>4.1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>0.7030</td>\n",
       "      <td>1.1227</td>\n",
       "      <td>4.0190</td>\n",
       "      <td>1.2962</td>\n",
       "      <td>6.9618</td>\n",
       "      <td>6.6554</td>\n",
       "      <td>7.7894</td>\n",
       "      <td>71.36</td>\n",
       "      <td>108.67</td>\n",
       "      <td>4.0918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3648 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Australia  Europe  Brazil  Canada   China  Denmark  Hong Kong  \\\n",
       "Date                                                                        \n",
       "2010-01-04     0.9133  1.4419  1.7200  1.0377  6.8273   5.1597     7.7555   \n",
       "2010-01-05     0.9143  1.4402  1.7296  1.0371  6.8258   5.1668     7.7564   \n",
       "2010-01-06     0.9189  1.4404  1.7292  1.0333  6.8272   5.1638     7.7546   \n",
       "2010-01-07     0.9168  1.4314  1.7409  1.0351  6.8280   5.1981     7.7539   \n",
       "2010-01-08     0.9218  1.4357  1.7342  1.0345  6.8274   5.1827     7.7553   \n",
       "...               ...     ...     ...     ...     ...      ...        ...   \n",
       "2019-12-27     0.6978  1.1174  4.0507  1.3073  6.9954   6.6829     7.7874   \n",
       "2019-12-28        NaN     NaN     NaN     NaN     NaN      NaN        NaN   \n",
       "2019-12-29        NaN     NaN     NaN     NaN     NaN      NaN        NaN   \n",
       "2019-12-30     0.7004  1.1217  4.0152  1.3058  6.9864   6.6589     7.7857   \n",
       "2019-12-31     0.7030  1.1227  4.0190  1.2962  6.9618   6.6554     7.7894   \n",
       "\n",
       "            India   Japan  Malaysia  \n",
       "Date                                 \n",
       "2010-01-04  46.27   92.55    3.3960  \n",
       "2010-01-05  46.13   91.48    3.3850  \n",
       "2010-01-06  45.72   92.53    3.3790  \n",
       "2010-01-07  45.67   93.31    3.3680  \n",
       "2010-01-08  45.50   92.70    3.3750  \n",
       "...           ...     ...       ...  \n",
       "2019-12-27  71.45  109.47    4.1260  \n",
       "2019-12-28    NaN     NaN       NaN  \n",
       "2019-12-29    NaN     NaN       NaN  \n",
       "2019-12-30  71.30  108.85    4.1053  \n",
       "2019-12-31  71.36  108.67    4.0918  \n",
       "\n",
       "[3648 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92fd39d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>0.913300</td>\n",
       "      <td>1.441900</td>\n",
       "      <td>1.720000</td>\n",
       "      <td>1.0377</td>\n",
       "      <td>6.8273</td>\n",
       "      <td>5.1597</td>\n",
       "      <td>7.755500</td>\n",
       "      <td>46.27</td>\n",
       "      <td>92.550000</td>\n",
       "      <td>3.3960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.914300</td>\n",
       "      <td>1.440200</td>\n",
       "      <td>1.729600</td>\n",
       "      <td>1.0371</td>\n",
       "      <td>6.8258</td>\n",
       "      <td>5.1668</td>\n",
       "      <td>7.756400</td>\n",
       "      <td>46.13</td>\n",
       "      <td>91.480000</td>\n",
       "      <td>3.3850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>0.918900</td>\n",
       "      <td>1.440400</td>\n",
       "      <td>1.729200</td>\n",
       "      <td>1.0333</td>\n",
       "      <td>6.8272</td>\n",
       "      <td>5.1638</td>\n",
       "      <td>7.754600</td>\n",
       "      <td>45.72</td>\n",
       "      <td>92.530000</td>\n",
       "      <td>3.3790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>0.916800</td>\n",
       "      <td>1.431400</td>\n",
       "      <td>1.740900</td>\n",
       "      <td>1.0351</td>\n",
       "      <td>6.8280</td>\n",
       "      <td>5.1981</td>\n",
       "      <td>7.753900</td>\n",
       "      <td>45.67</td>\n",
       "      <td>93.310000</td>\n",
       "      <td>3.3680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>0.921800</td>\n",
       "      <td>1.435700</td>\n",
       "      <td>1.734200</td>\n",
       "      <td>1.0345</td>\n",
       "      <td>6.8274</td>\n",
       "      <td>5.1827</td>\n",
       "      <td>7.755300</td>\n",
       "      <td>45.50</td>\n",
       "      <td>92.700000</td>\n",
       "      <td>3.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>0.697800</td>\n",
       "      <td>1.117400</td>\n",
       "      <td>4.050700</td>\n",
       "      <td>1.3073</td>\n",
       "      <td>6.9954</td>\n",
       "      <td>6.6829</td>\n",
       "      <td>7.787400</td>\n",
       "      <td>71.45</td>\n",
       "      <td>109.470000</td>\n",
       "      <td>4.1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-28</th>\n",
       "      <td>0.698667</td>\n",
       "      <td>1.118833</td>\n",
       "      <td>4.038867</td>\n",
       "      <td>1.3068</td>\n",
       "      <td>6.9924</td>\n",
       "      <td>6.6749</td>\n",
       "      <td>7.786833</td>\n",
       "      <td>71.40</td>\n",
       "      <td>109.263333</td>\n",
       "      <td>4.1191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-29</th>\n",
       "      <td>0.699533</td>\n",
       "      <td>1.120267</td>\n",
       "      <td>4.027033</td>\n",
       "      <td>1.3063</td>\n",
       "      <td>6.9894</td>\n",
       "      <td>6.6669</td>\n",
       "      <td>7.786267</td>\n",
       "      <td>71.35</td>\n",
       "      <td>109.056667</td>\n",
       "      <td>4.1122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>0.700400</td>\n",
       "      <td>1.121700</td>\n",
       "      <td>4.015200</td>\n",
       "      <td>1.3058</td>\n",
       "      <td>6.9864</td>\n",
       "      <td>6.6589</td>\n",
       "      <td>7.785700</td>\n",
       "      <td>71.30</td>\n",
       "      <td>108.850000</td>\n",
       "      <td>4.1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>0.703000</td>\n",
       "      <td>1.122700</td>\n",
       "      <td>4.019000</td>\n",
       "      <td>1.2962</td>\n",
       "      <td>6.9618</td>\n",
       "      <td>6.6554</td>\n",
       "      <td>7.789400</td>\n",
       "      <td>71.36</td>\n",
       "      <td>108.670000</td>\n",
       "      <td>4.0918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3648 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Australia    Europe    Brazil  Canada   China  Denmark  Hong Kong  \\\n",
       "Date                                                                            \n",
       "2010-01-04   0.913300  1.441900  1.720000  1.0377  6.8273   5.1597   7.755500   \n",
       "2010-01-05   0.914300  1.440200  1.729600  1.0371  6.8258   5.1668   7.756400   \n",
       "2010-01-06   0.918900  1.440400  1.729200  1.0333  6.8272   5.1638   7.754600   \n",
       "2010-01-07   0.916800  1.431400  1.740900  1.0351  6.8280   5.1981   7.753900   \n",
       "2010-01-08   0.921800  1.435700  1.734200  1.0345  6.8274   5.1827   7.755300   \n",
       "...               ...       ...       ...     ...     ...      ...        ...   \n",
       "2019-12-27   0.697800  1.117400  4.050700  1.3073  6.9954   6.6829   7.787400   \n",
       "2019-12-28   0.698667  1.118833  4.038867  1.3068  6.9924   6.6749   7.786833   \n",
       "2019-12-29   0.699533  1.120267  4.027033  1.3063  6.9894   6.6669   7.786267   \n",
       "2019-12-30   0.700400  1.121700  4.015200  1.3058  6.9864   6.6589   7.785700   \n",
       "2019-12-31   0.703000  1.122700  4.019000  1.2962  6.9618   6.6554   7.789400   \n",
       "\n",
       "            India       Japan  Malaysia  \n",
       "Date                                     \n",
       "2010-01-04  46.27   92.550000    3.3960  \n",
       "2010-01-05  46.13   91.480000    3.3850  \n",
       "2010-01-06  45.72   92.530000    3.3790  \n",
       "2010-01-07  45.67   93.310000    3.3680  \n",
       "2010-01-08  45.50   92.700000    3.3750  \n",
       "...           ...         ...       ...  \n",
       "2019-12-27  71.45  109.470000    4.1260  \n",
       "2019-12-28  71.40  109.263333    4.1191  \n",
       "2019-12-29  71.35  109.056667    4.1122  \n",
       "2019-12-30  71.30  108.850000    4.1053  \n",
       "2019-12-31  71.36  108.670000    4.0918  \n",
       "\n",
       "[3648 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.interpolate(method='linear', limit_direction='forward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d22daf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.interpolate(method='linear', limit_direction='forward', inplace=True)\n",
    "LOOK_BACK = 30\n",
    "PREDICT_DAY = 1\n",
    "SPLIT_RATIO = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d8191f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Data(data, lookback = LOOK_BACK, pred_len = PREDICT_DAY, split_ratio = SPLIT_RATIO):\n",
    "    if lookback<2:\n",
    "        print(\"ERROR: Lookback too small\")\n",
    "        return -1\n",
    "\n",
    "# declarations\n",
    "\n",
    "    x = {}\n",
    "    y = {}\n",
    "    xtr = {}\n",
    "    xt = {}\n",
    "    ytr = {}\n",
    "    yt = {}\n",
    "    scalers = {}\n",
    "\n",
    "# Creating stepped data\n",
    "\n",
    "    for i in data.columns:\n",
    "        xtemp = pd.DataFrame(data[i])\n",
    "        for j in range(1,lookback+1):\n",
    "            xtemp[i+str(j)] = data[i].shift(-1*j)\n",
    "        x[i] = xtemp.dropna()\n",
    "\n",
    "# Splitting data into x and y\n",
    "        \n",
    "    for i in x.keys():\n",
    "        y[i] = pd.DataFrame(x[i].iloc[:,-pred_len])\n",
    "        x[i] = x[i].iloc[:,:-pred_len]\n",
    "        \n",
    "# Normalizing x and y values\n",
    "        \n",
    "    for i in x.keys():\n",
    "        scalers[i+\"_x\"] = MinMaxScaler(feature_range=(0,1))\n",
    "        x[i] = scalers[i+\"_x\"].fit_transform(x[i])\n",
    "        scalers[i+\"_y\"] = MinMaxScaler(feature_range=(0,1))\n",
    "        y[i] = scalers[i+\"_y\"].fit_transform(y[i])\n",
    "        \n",
    "# setting train and test sizes\n",
    "        \n",
    "    tr_len = int(split_ratio*y[\"India\"].shape[0])\n",
    "    t_len = y[\"India\"].shape[0]-tr_len\n",
    "    \n",
    "# creating training and testing data\n",
    "    \n",
    "    for i in x.keys():\n",
    "        xtr[i] = x[i][:tr_len]\n",
    "        ytr[i] = y[i][:tr_len]\n",
    "        xt[i] = x[i][-t_len:]\n",
    "        yt[i] = y[i][-t_len:]\n",
    "        \n",
    "# returning pertinent data\n",
    "    \n",
    "    return x,y,xtr,xt,ytr,yt,scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4dff5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,xtr,xt,ytr,yt,scalers = Create_Data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b99ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(sequence_length, feature_dim, num_transformer_blocks=3):\n",
    "    inputs = Input(shape=(sequence_length, feature_dim))\n",
    "    x = inputs\n",
    "\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        # Transformer Block with multi-head attention\n",
    "        query = LayerNormalization(epsilon=1e-6)(x)\n",
    "        key = LayerNormalization(epsilon=1e-6)(x)\n",
    "        value = LayerNormalization(epsilon=1e-6)(x)\n",
    "        \n",
    "        attn_output = MultiHeadAttention(num_heads=4, key_dim=feature_dim, dropout=0.3, self_attention=True)(query, value, key=key)\n",
    "        x = Add()([x, attn_output])  # Residual connection\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "        # Feed Forward Network inside transformer\n",
    "        ffn_output = Dense(128, activation=\"relu\")(x)\n",
    "        ffn_output = Dropout(0.3)(ffn_output)\n",
    "        ffn_output = Dense(64, activation=\"relu\")(ffn_output)\n",
    "        ffn_output = Dense(feature_dim)(ffn_output)\n",
    "        x = Add()([x, ffn_output])  # Residual connection\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    \n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(64, activation=\"relu\")(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    outputs = Dense(PREDICT_DAY)(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def Create_model(x, y, lookback=LOOK_BACK, Pred_size=PREDICT_DAY):\n",
    "    models = {}\n",
    "    for i in x.keys():\n",
    "        reshaped_x = np.reshape(x[i], (x[i].shape[0], 1, x[i].shape[1]))\n",
    "        models[i] = transformer_encoder(1, reshaped_x.shape[2])\n",
    "        print(i)\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c91c0503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 11:31:47.278860: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 11:31:47.303544: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 11:31:47.304198: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 11:31:47.308134: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 11:31:47.308602: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 11:31:47.308910: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 11:31:47.408625: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 11:31:47.408982: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 11:31:47.409200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-12 11:31:47.409352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 131 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2023-10-12 11:31:47.409823: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2023-10-12 11:31:47.431969: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 131.88MiB (138280960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "('Keyword argument not understood:', 'self_attention')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ghoul/Documents/projects/dl/DL_Currency_Exchange_Rate/models/Transformer(self).ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ghoul/Documents/projects/dl/DL_Currency_Exchange_Rate/models/Transformer%28self%29.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m m \u001b[39m=\u001b[39m Create_model(x,y)\n",
      "\u001b[1;32m/home/ghoul/Documents/projects/dl/DL_Currency_Exchange_Rate/models/Transformer(self).ipynb Cell 10\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ghoul/Documents/projects/dl/DL_Currency_Exchange_Rate/models/Transformer%28self%29.ipynb#X12sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ghoul/Documents/projects/dl/DL_Currency_Exchange_Rate/models/Transformer%28self%29.ipynb#X12sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     reshaped_x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(x[i], (x[i]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m1\u001b[39m, x[i]\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ghoul/Documents/projects/dl/DL_Currency_Exchange_Rate/models/Transformer%28self%29.ipynb#X12sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     models[i] \u001b[39m=\u001b[39m transformer_encoder(\u001b[39m1\u001b[39;49m, reshaped_x\u001b[39m.\u001b[39;49mshape[\u001b[39m2\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ghoul/Documents/projects/dl/DL_Currency_Exchange_Rate/models/Transformer%28self%29.ipynb#X12sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39mprint\u001b[39m(i)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ghoul/Documents/projects/dl/DL_Currency_Exchange_Rate/models/Transformer%28self%29.ipynb#X12sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mreturn\u001b[39;00m models\n",
      "\u001b[1;32m/home/ghoul/Documents/projects/dl/DL_Currency_Exchange_Rate/models/Transformer(self).ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ghoul/Documents/projects/dl/DL_Currency_Exchange_Rate/models/Transformer%28self%29.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m key \u001b[39m=\u001b[39m LayerNormalization(epsilon\u001b[39m=\u001b[39m\u001b[39m1e-6\u001b[39m)(x)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ghoul/Documents/projects/dl/DL_Currency_Exchange_Rate/models/Transformer%28self%29.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m value \u001b[39m=\u001b[39m LayerNormalization(epsilon\u001b[39m=\u001b[39m\u001b[39m1e-6\u001b[39m)(x)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ghoul/Documents/projects/dl/DL_Currency_Exchange_Rate/models/Transformer%28self%29.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m attn_output \u001b[39m=\u001b[39m MultiHeadAttention(num_heads\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, key_dim\u001b[39m=\u001b[39;49mfeature_dim, dropout\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m, self_attention\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)(query, value, key\u001b[39m=\u001b[39mkey)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ghoul/Documents/projects/dl/DL_Currency_Exchange_Rate/models/Transformer%28self%29.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m x \u001b[39m=\u001b[39m Add()([x, attn_output])  \u001b[39m# Residual connection\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ghoul/Documents/projects/dl/DL_Currency_Exchange_Rate/models/Transformer%28self%29.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m x \u001b[39m=\u001b[39m LayerNormalization(epsilon\u001b[39m=\u001b[39m\u001b[39m1e-6\u001b[39m)(x)\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/keras/layers/attention/multi_head_attention.py:252\u001b[0m, in \u001b[0;36mMultiHeadAttention.__init__\u001b[0;34m(self, num_heads, key_dim, value_dim, dropout, use_bias, output_shape, attention_axes, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    235\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    236\u001b[0m     num_heads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    251\u001b[0m ):\n\u001b[0;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    253\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_masking \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_heads \u001b[39m=\u001b[39m num_heads\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/tensorflow/python/trackable/base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    205\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/keras/engine/base_layer.py:341\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m allowed_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    331\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput_dim\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    332\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimplementation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    339\u001b[0m }\n\u001b[1;32m    340\u001b[0m \u001b[39m# Validate optional keyword arguments.\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m generic_utils\u001b[39m.\u001b[39;49mvalidate_kwargs(kwargs, allowed_kwargs)\n\u001b[1;32m    343\u001b[0m \u001b[39m# Mutable properties\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[39m# Indicates whether the layer's weights are updated during training\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39m# and whether the layer's updates are run during training.\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\n\u001b[1;32m    347\u001b[0m     \u001b[39misinstance\u001b[39m(trainable, \u001b[39mbool\u001b[39m)\n\u001b[1;32m    348\u001b[0m     \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    351\u001b[0m     )\n\u001b[1;32m    352\u001b[0m ):\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/keras/utils/generic_utils.py:514\u001b[0m, in \u001b[0;36mvalidate_kwargs\u001b[0;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mfor\u001b[39;00m kwarg \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    513\u001b[0m     \u001b[39mif\u001b[39;00m kwarg \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allowed_kwargs:\n\u001b[0;32m--> 514\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(error_message, kwarg)\n",
      "\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'self_attention')"
     ]
    }
   ],
   "source": [
    "m = Create_model(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e10c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Execute_model(model, xtr, ytr, xt, yt, scaler):\n",
    "    MAPE = {}\n",
    "    MAE = {}\n",
    "    MSE = {}\n",
    "    for i in model.keys():\n",
    "        print(i)\n",
    "        reshaped_xtr = np.reshape(xtr[i], (xtr[i].shape[0], 1, xtr[i].shape[1]))\n",
    "        reshaped_xt = np.reshape(xt[i], (xt[i].shape[0], 1, xt[i].shape[1]))\n",
    "        \n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "        \n",
    "        model[i].fit(reshaped_xtr, ytr[i], epochs=100, batch_size=64, verbose=1, validation_split=0.2, callbacks=[es, reduce_lr])\n",
    "        \n",
    "        temp = model[i].predict(reshaped_xt)\n",
    "        pred = scaler[i+\"_y\"].inverse_transform(temp)\n",
    "        act = scaler[i+\"_y\"].inverse_transform(yt[i])\n",
    "        \n",
    "        MSE[i] = mean_squared_error(act, pred)\n",
    "        MAE[i] = mean_absolute_error(act, pred)\n",
    "        MAPE[i] = mean_absolute_percentage_error(act, pred)\n",
    "        \n",
    "    results = pd.DataFrame([MSE, MAE, MAPE])\n",
    "    results[\"Metric\"] = [\"MSE\", \"MAE\", \"MAPE\"]\n",
    "    results.set_index(\"Metric\", inplace=True)\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e0a822",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 11:19:24.211692: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5645c853f0c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-10-12 11:19:24.211753: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2023-10-12 11:19:24.223330: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-10-12 11:19:24.254323: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8902\n",
      "2023-10-12 11:19:24.368533: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 13s 38ms/step - loss: 0.1790 - val_loss: 0.0207 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0992 - val_loss: 0.0217 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0910 - val_loss: 0.0181 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0848 - val_loss: 0.0169 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0788 - val_loss: 0.0287 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0782 - val_loss: 0.0308 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0774 - val_loss: 0.0261 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0763 - val_loss: 0.0308 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0745 - val_loss: 0.0412 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0748 - val_loss: 0.0362 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0737 - val_loss: 0.0358 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0748 - val_loss: 0.0398 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0737 - val_loss: 0.0379 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0732 - val_loss: 0.0371 - lr: 2.0000e-04\n",
      "Epoch 14: early stopping\n",
      "23/23 [==============================] - 1s 6ms/step\n",
      "Europe\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 13s 37ms/step - loss: 0.1197 - val_loss: 0.0291 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0806 - val_loss: 0.0264 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0691 - val_loss: 0.0296 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 1s 19ms/step - loss: 0.0674 - val_loss: 0.0251 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0618 - val_loss: 0.0425 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0628 - val_loss: 0.0398 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0591 - val_loss: 0.0354 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0593 - val_loss: 0.0517 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0584 - val_loss: 0.0515 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0568 - val_loss: 0.0596 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0564 - val_loss: 0.0584 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0568 - val_loss: 0.0621 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0561 - val_loss: 0.0568 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0561 - val_loss: 0.0607 - lr: 2.0000e-04\n",
      "Epoch 14: early stopping\n",
      "23/23 [==============================] - 1s 6ms/step\n",
      "Brazil\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 13s 37ms/step - loss: 0.1261 - val_loss: 0.2184 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0762 - val_loss: 0.2176 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0740 - val_loss: 0.1839 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0704 - val_loss: 0.1871 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0687 - val_loss: 0.1784 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 0.0687 - val_loss: 0.1698 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0683 - val_loss: 0.1722 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0675 - val_loss: 0.1644 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0663 - val_loss: 0.1689 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0661 - val_loss: 0.1480 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0660 - val_loss: 0.1723 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 1s 19ms/step - loss: 0.0657 - val_loss: 0.1616 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0653 - val_loss: 0.1563 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 1s 20ms/step - loss: 0.0655 - val_loss: 0.1575 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0650 - val_loss: 0.1552 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0657 - val_loss: 0.1524 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0648 - val_loss: 0.1536 - lr: 2.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0653 - val_loss: 0.1560 - lr: 2.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0646 - val_loss: 0.1519 - lr: 2.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0649 - val_loss: 0.1566 - lr: 2.0000e-04\n",
      "Epoch 20: early stopping\n",
      "23/23 [==============================] - 1s 6ms/step\n",
      "Canada\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 13s 39ms/step - loss: 0.1538 - val_loss: 0.2979 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0724 - val_loss: 0.2807 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0640 - val_loss: 0.2983 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0636 - val_loss: 0.2841 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0594 - val_loss: 0.2778 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0591 - val_loss: 0.2633 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0579 - val_loss: 0.2729 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0576 - val_loss: 0.2582 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0567 - val_loss: 0.2690 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0566 - val_loss: 0.2524 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0562 - val_loss: 0.2540 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0547 - val_loss: 0.2450 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0546 - val_loss: 0.2436 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0556 - val_loss: 0.2301 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0544 - val_loss: 0.2483 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0551 - val_loss: 0.2357 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0541 - val_loss: 0.2291 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0539 - val_loss: 0.2153 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0532 - val_loss: 0.2212 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0530 - val_loss: 0.2100 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0527 - val_loss: 0.2240 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0529 - val_loss: 0.2281 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0538 - val_loss: 0.2079 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0529 - val_loss: 0.2094 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0533 - val_loss: 0.1966 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0531 - val_loss: 0.2223 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 0.0528 - val_loss: 0.2046 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0526 - val_loss: 0.2094 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0531 - val_loss: 0.2003 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0522 - val_loss: 0.2127 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0521 - val_loss: 0.2100 - lr: 2.0000e-04\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0518 - val_loss: 0.2075 - lr: 2.0000e-04\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0516 - val_loss: 0.2108 - lr: 2.0000e-04\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0512 - val_loss: 0.2094 - lr: 2.0000e-04\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0511 - val_loss: 0.2141 - lr: 2.0000e-04\n",
      "Epoch 35: early stopping\n",
      "23/23 [==============================] - 1s 7ms/step\n",
      "China\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 13s 40ms/step - loss: 0.0973 - val_loss: 0.1865 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0508 - val_loss: 0.2074 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0443 - val_loss: 0.2051 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0412 - val_loss: 0.1999 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 0.0407 - val_loss: 0.1942 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0387 - val_loss: 0.1820 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0372 - val_loss: 0.1569 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0376 - val_loss: 0.1519 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0370 - val_loss: 0.1622 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0365 - val_loss: 0.1533 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0367 - val_loss: 0.1469 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 1s 31ms/step - loss: 0.0364 - val_loss: 0.1394 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0363 - val_loss: 0.1428 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0363 - val_loss: 0.1470 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0362 - val_loss: 0.1462 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0358 - val_loss: 0.1335 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0361 - val_loss: 0.1339 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0361 - val_loss: 0.1413 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0362 - val_loss: 0.1432 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0357 - val_loss: 0.1264 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0360 - val_loss: 0.1317 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0358 - val_loss: 0.1345 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0357 - val_loss: 0.1236 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0360 - val_loss: 0.1311 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0360 - val_loss: 0.1277 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0357 - val_loss: 0.1342 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0359 - val_loss: 0.1189 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0357 - val_loss: 0.1395 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0360 - val_loss: 0.1201 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0357 - val_loss: 0.1418 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0360 - val_loss: 0.1292 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0358 - val_loss: 0.1281 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0357 - val_loss: 0.1276 - lr: 2.0000e-04\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0357 - val_loss: 0.1304 - lr: 2.0000e-04\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0357 - val_loss: 0.1312 - lr: 2.0000e-04\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0357 - val_loss: 0.1294 - lr: 2.0000e-04\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0357 - val_loss: 0.1283 - lr: 2.0000e-04\n",
      "Epoch 37: early stopping\n",
      "23/23 [==============================] - 1s 5ms/step\n",
      "Denmark\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 13s 42ms/step - loss: 0.1893 - val_loss: 0.3528 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0877 - val_loss: 0.3123 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0721 - val_loss: 0.2885 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0681 - val_loss: 0.2551 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0652 - val_loss: 0.2263 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0625 - val_loss: 0.2116 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0613 - val_loss: 0.2089 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0605 - val_loss: 0.2051 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 0.0597 - val_loss: 0.2039 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0595 - val_loss: 0.2170 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0580 - val_loss: 0.2012 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0584 - val_loss: 0.2111 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0580 - val_loss: 0.1935 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0583 - val_loss: 0.1796 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0572 - val_loss: 0.1766 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0571 - val_loss: 0.1882 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0566 - val_loss: 0.1999 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0571 - val_loss: 0.1755 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0567 - val_loss: 0.1879 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0563 - val_loss: 0.1634 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0570 - val_loss: 0.1738 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0569 - val_loss: 0.1806 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0569 - val_loss: 0.1715 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0568 - val_loss: 0.1682 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0563 - val_loss: 0.1644 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0563 - val_loss: 0.1669 - lr: 2.0000e-04\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0563 - val_loss: 0.1698 - lr: 2.0000e-04\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0563 - val_loss: 0.1645 - lr: 2.0000e-04\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0560 - val_loss: 0.1682 - lr: 2.0000e-04\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0562 - val_loss: 0.1716 - lr: 2.0000e-04\n",
      "Epoch 30: early stopping\n",
      "23/23 [==============================] - 1s 6ms/step\n",
      "Hong Kong\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 13s 38ms/step - loss: 0.0603 - val_loss: 0.1031 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0245 - val_loss: 0.1019 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0215 - val_loss: 0.0962 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0217 - val_loss: 0.0978 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0196 - val_loss: 0.1021 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0193 - val_loss: 0.1032 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0191 - val_loss: 0.0957 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0185 - val_loss: 0.0942 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0181 - val_loss: 0.0966 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0180 - val_loss: 0.0966 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0176 - val_loss: 0.0934 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0177 - val_loss: 0.0927 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0176 - val_loss: 0.0934 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 0.0175 - val_loss: 0.0917 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0177 - val_loss: 0.0913 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0173 - val_loss: 0.0885 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0171 - val_loss: 0.0851 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0170 - val_loss: 0.0876 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0162 - val_loss: 0.0956 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0163 - val_loss: 0.0900 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0162 - val_loss: 0.0835 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0158 - val_loss: 0.0889 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0161 - val_loss: 0.0773 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0163 - val_loss: 0.0937 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0153 - val_loss: 0.0799 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 0.0155 - val_loss: 0.0952 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0148 - val_loss: 0.0851 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0151 - val_loss: 0.0976 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0146 - val_loss: 0.0877 - lr: 2.0000e-04\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0144 - val_loss: 0.0856 - lr: 2.0000e-04\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0139 - val_loss: 0.0843 - lr: 2.0000e-04\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 0.0143 - val_loss: 0.0874 - lr: 2.0000e-04\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0135 - val_loss: 0.0858 - lr: 2.0000e-04\n",
      "Epoch 33: early stopping\n",
      "23/23 [==============================] - 1s 5ms/step\n",
      "India\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 13s 40ms/step - loss: 0.1593 - val_loss: 0.2418 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0867 - val_loss: 0.2654 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0748 - val_loss: 0.2672 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0729 - val_loss: 0.2510 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0706 - val_loss: 0.2534 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0693 - val_loss: 0.2356 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0691 - val_loss: 0.2209 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0677 - val_loss: 0.2136 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0673 - val_loss: 0.2205 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0684 - val_loss: 0.2198 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0670 - val_loss: 0.2039 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0666 - val_loss: 0.1951 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0651 - val_loss: 0.1800 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0651 - val_loss: 0.1710 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0656 - val_loss: 0.1585 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0649 - val_loss: 0.1650 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0643 - val_loss: 0.1704 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0649 - val_loss: 0.1633 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0653 - val_loss: 0.1670 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0645 - val_loss: 0.1614 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0640 - val_loss: 0.1617 - lr: 2.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0637 - val_loss: 0.1587 - lr: 2.0000e-04\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 0.0646 - val_loss: 0.1572 - lr: 2.0000e-04\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0645 - val_loss: 0.1561 - lr: 2.0000e-04\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0635 - val_loss: 0.1562 - lr: 2.0000e-04\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0640 - val_loss: 0.1561 - lr: 2.0000e-04\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0636 - val_loss: 0.1580 - lr: 2.0000e-04\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0638 - val_loss: 0.1556 - lr: 2.0000e-04\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0640 - val_loss: 0.1544 - lr: 2.0000e-04\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0631 - val_loss: 0.1573 - lr: 2.0000e-04\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0640 - val_loss: 0.1554 - lr: 2.0000e-04\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0632 - val_loss: 0.1552 - lr: 2.0000e-04\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0630 - val_loss: 0.1543 - lr: 2.0000e-04\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0632 - val_loss: 0.1520 - lr: 2.0000e-04\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0632 - val_loss: 0.1530 - lr: 2.0000e-04\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0629 - val_loss: 0.1527 - lr: 2.0000e-04\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0639 - val_loss: 0.1538 - lr: 2.0000e-04\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0635 - val_loss: 0.1557 - lr: 2.0000e-04\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0634 - val_loss: 0.1525 - lr: 2.0000e-04\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0632 - val_loss: 0.1534 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0635 - val_loss: 0.1545 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0627 - val_loss: 0.1521 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0631 - val_loss: 0.1540 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0639 - val_loss: 0.1517 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0630 - val_loss: 0.1516 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0626 - val_loss: 0.1518 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0633 - val_loss: 0.1509 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0634 - val_loss: 0.1505 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 1s 30ms/step - loss: 0.0627 - val_loss: 0.1488 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0632 - val_loss: 0.1494 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0631 - val_loss: 0.1488 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0635 - val_loss: 0.1487 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0632 - val_loss: 0.1496 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0634 - val_loss: 0.1475 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0627 - val_loss: 0.1501 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0635 - val_loss: 0.1506 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0624 - val_loss: 0.1480 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0631 - val_loss: 0.1484 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0630 - val_loss: 0.1490 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0627 - val_loss: 0.1492 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0638 - val_loss: 0.1490 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0632 - val_loss: 0.1478 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0632 - val_loss: 0.1481 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0632 - val_loss: 0.1481 - lr: 1.0000e-04\n",
      "Epoch 64: early stopping\n",
      "23/23 [==============================] - 1s 6ms/step\n",
      "Japan\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 13s 35ms/step - loss: 0.1811 - val_loss: 0.2103 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.1143 - val_loss: 0.1965 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.1132 - val_loss: 0.2181 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.1056 - val_loss: 0.2199 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.1031 - val_loss: 0.1770 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.1038 - val_loss: 0.1994 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.1018 - val_loss: 0.1826 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.1011 - val_loss: 0.1502 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.1002 - val_loss: 0.1594 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.1009 - val_loss: 0.1333 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.1003 - val_loss: 0.1414 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0998 - val_loss: 0.1458 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0977 - val_loss: 0.1371 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0993 - val_loss: 0.1321 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0985 - val_loss: 0.1264 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0995 - val_loss: 0.1427 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0985 - val_loss: 0.1384 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0984 - val_loss: 0.1300 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0977 - val_loss: 0.1285 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0975 - val_loss: 0.1343 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0971 - val_loss: 0.1261 - lr: 2.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0972 - val_loss: 0.1277 - lr: 2.0000e-04\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0965 - val_loss: 0.1282 - lr: 2.0000e-04\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0982 - val_loss: 0.1248 - lr: 2.0000e-04\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0970 - val_loss: 0.1258 - lr: 2.0000e-04\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0967 - val_loss: 0.1241 - lr: 2.0000e-04\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0968 - val_loss: 0.1233 - lr: 2.0000e-04\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0984 - val_loss: 0.1244 - lr: 2.0000e-04\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0977 - val_loss: 0.1194 - lr: 2.0000e-04\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0968 - val_loss: 0.1169 - lr: 2.0000e-04\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 1s 29ms/step - loss: 0.0970 - val_loss: 0.1213 - lr: 2.0000e-04\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0967 - val_loss: 0.1170 - lr: 2.0000e-04\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0971 - val_loss: 0.1192 - lr: 2.0000e-04\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0971 - val_loss: 0.1149 - lr: 2.0000e-04\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0974 - val_loss: 0.1166 - lr: 2.0000e-04\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0954 - val_loss: 0.1115 - lr: 2.0000e-04\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0964 - val_loss: 0.1197 - lr: 2.0000e-04\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0969 - val_loss: 0.1151 - lr: 2.0000e-04\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0964 - val_loss: 0.1144 - lr: 2.0000e-04\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0961 - val_loss: 0.1138 - lr: 2.0000e-04\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0971 - val_loss: 0.1158 - lr: 2.0000e-04\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0967 - val_loss: 0.1136 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0966 - val_loss: 0.1111 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0961 - val_loss: 0.1101 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0965 - val_loss: 0.1110 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0965 - val_loss: 0.1132 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0962 - val_loss: 0.1111 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0968 - val_loss: 0.1125 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0960 - val_loss: 0.1109 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0968 - val_loss: 0.1098 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0964 - val_loss: 0.1099 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0960 - val_loss: 0.1096 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0962 - val_loss: 0.1098 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0967 - val_loss: 0.1096 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0959 - val_loss: 0.1092 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0965 - val_loss: 0.1104 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0965 - val_loss: 0.1095 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0959 - val_loss: 0.1099 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0958 - val_loss: 0.1092 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 1s 19ms/step - loss: 0.0969 - val_loss: 0.1127 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0962 - val_loss: 0.1082 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0961 - val_loss: 0.1106 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0961 - val_loss: 0.1088 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0955 - val_loss: 0.1097 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0967 - val_loss: 0.1120 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0957 - val_loss: 0.1082 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0958 - val_loss: 0.1077 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0964 - val_loss: 0.1093 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0954 - val_loss: 0.1070 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0959 - val_loss: 0.1080 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0964 - val_loss: 0.1086 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0959 - val_loss: 0.1090 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0961 - val_loss: 0.1064 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0957 - val_loss: 0.1056 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0962 - val_loss: 0.1082 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0958 - val_loss: 0.1079 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0956 - val_loss: 0.1049 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0957 - val_loss: 0.1071 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0960 - val_loss: 0.1062 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0956 - val_loss: 0.1055 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 1s 22ms/step - loss: 0.0965 - val_loss: 0.1079 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0958 - val_loss: 0.1060 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0956 - val_loss: 0.1057 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0963 - val_loss: 0.1070 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0961 - val_loss: 0.1058 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0957 - val_loss: 0.1059 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0955 - val_loss: 0.1046 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0953 - val_loss: 0.1050 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0949 - val_loss: 0.1043 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 1s 21ms/step - loss: 0.0953 - val_loss: 0.1053 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0951 - val_loss: 0.1054 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0959 - val_loss: 0.1046 - lr: 1.0000e-04\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0957 - val_loss: 0.1052 - lr: 1.0000e-04\n",
      "Epoch 94/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0952 - val_loss: 0.1041 - lr: 1.0000e-04\n",
      "Epoch 95/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0954 - val_loss: 0.1026 - lr: 1.0000e-04\n",
      "Epoch 96/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0952 - val_loss: 0.1038 - lr: 1.0000e-04\n",
      "Epoch 97/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0953 - val_loss: 0.1006 - lr: 1.0000e-04\n",
      "Epoch 98/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0957 - val_loss: 0.1025 - lr: 1.0000e-04\n",
      "Epoch 99/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0948 - val_loss: 0.1021 - lr: 1.0000e-04\n",
      "Epoch 100/100\n",
      "37/37 [==============================] - 1s 28ms/step - loss: 0.0951 - val_loss: 0.1028 - lr: 1.0000e-04\n",
      "23/23 [==============================] - 1s 6ms/step\n",
      "Malaysia\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 13s 40ms/step - loss: 0.1442 - val_loss: 0.4807 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0718 - val_loss: 0.5391 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0658 - val_loss: 0.4648 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0635 - val_loss: 0.4644 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0624 - val_loss: 0.4425 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0610 - val_loss: 0.4511 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0602 - val_loss: 0.4572 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0600 - val_loss: 0.4230 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0586 - val_loss: 0.4069 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0587 - val_loss: 0.4467 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 1s 27ms/step - loss: 0.0574 - val_loss: 0.4455 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0578 - val_loss: 0.4443 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0574 - val_loss: 0.4255 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0570 - val_loss: 0.4147 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 1s 26ms/step - loss: 0.0561 - val_loss: 0.4226 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 1s 24ms/step - loss: 0.0567 - val_loss: 0.4145 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0569 - val_loss: 0.4179 - lr: 2.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 1s 25ms/step - loss: 0.0566 - val_loss: 0.4151 - lr: 2.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 1s 23ms/step - loss: 0.0569 - val_loss: 0.4216 - lr: 2.0000e-04\n",
      "Epoch 19: early stopping\n",
      "23/23 [==============================] - 1s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "result = Execute_model(m,xtr,ytr,xt,yt,scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256b816d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>0.015315</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>2.760220</td>\n",
       "      <td>0.057862</td>\n",
       "      <td>0.232093</td>\n",
       "      <td>0.522025</td>\n",
       "      <td>0.005544</td>\n",
       "      <td>226.040782</td>\n",
       "      <td>230.518630</td>\n",
       "      <td>0.679650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.118531</td>\n",
       "      <td>0.072268</td>\n",
       "      <td>1.639256</td>\n",
       "      <td>0.238259</td>\n",
       "      <td>0.419454</td>\n",
       "      <td>0.684943</td>\n",
       "      <td>0.072891</td>\n",
       "      <td>14.791360</td>\n",
       "      <td>14.950280</td>\n",
       "      <td>0.816415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE</th>\n",
       "      <td>0.166854</td>\n",
       "      <td>0.063945</td>\n",
       "      <td>0.427886</td>\n",
       "      <td>0.181362</td>\n",
       "      <td>0.060751</td>\n",
       "      <td>0.104270</td>\n",
       "      <td>0.009299</td>\n",
       "      <td>0.211885</td>\n",
       "      <td>0.136057</td>\n",
       "      <td>0.199320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Australia    Europe    Brazil    Canada     China   Denmark  \\\n",
       "Metric                                                                \n",
       "MSE      0.015315  0.006400  2.760220  0.057862  0.232093  0.522025   \n",
       "MAE      0.118531  0.072268  1.639256  0.238259  0.419454  0.684943   \n",
       "MAPE     0.166854  0.063945  0.427886  0.181362  0.060751  0.104270   \n",
       "\n",
       "        Hong Kong       India       Japan  Malaysia  \n",
       "Metric                                               \n",
       "MSE      0.005544  226.040782  230.518630  0.679650  \n",
       "MAE      0.072891   14.791360   14.950280  0.816415  \n",
       "MAPE     0.009299    0.211885    0.136057  0.199320  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97fc57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Define the filename\n",
    "filename = \"../results.json\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.isfile(filename):\n",
    "    # If the file exists, load the existing data\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "else:\n",
    "    # If the file doesn't exist, create an empty dictionary\n",
    "    data = {}\n",
    "\n",
    "# Add the result to the dictionary\n",
    "data[\"Transformer(Self Attention)\"] = result.to_dict()\n",
    "\n",
    "# Write the dictionary to the file\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump(data, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
