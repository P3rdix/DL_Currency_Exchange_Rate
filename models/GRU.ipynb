{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f01c67d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GRU\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bee2e983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-04</td>\n",
       "      <td>0.9133</td>\n",
       "      <td>1.4419</td>\n",
       "      <td>1.7200</td>\n",
       "      <td>1.0377</td>\n",
       "      <td>6.8273</td>\n",
       "      <td>5.1597</td>\n",
       "      <td>7.7555</td>\n",
       "      <td>46.27</td>\n",
       "      <td>92.55</td>\n",
       "      <td>3.396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2010-01-05</td>\n",
       "      <td>0.9143</td>\n",
       "      <td>1.4402</td>\n",
       "      <td>1.7296</td>\n",
       "      <td>1.0371</td>\n",
       "      <td>6.8258</td>\n",
       "      <td>5.1668</td>\n",
       "      <td>7.7564</td>\n",
       "      <td>46.13</td>\n",
       "      <td>91.48</td>\n",
       "      <td>3.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2010-01-06</td>\n",
       "      <td>0.9189</td>\n",
       "      <td>1.4404</td>\n",
       "      <td>1.7292</td>\n",
       "      <td>1.0333</td>\n",
       "      <td>6.8272</td>\n",
       "      <td>5.1638</td>\n",
       "      <td>7.7546</td>\n",
       "      <td>45.72</td>\n",
       "      <td>92.53</td>\n",
       "      <td>3.379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2010-01-07</td>\n",
       "      <td>0.9168</td>\n",
       "      <td>1.4314</td>\n",
       "      <td>1.7409</td>\n",
       "      <td>1.0351</td>\n",
       "      <td>6.8280</td>\n",
       "      <td>5.1981</td>\n",
       "      <td>7.7539</td>\n",
       "      <td>45.67</td>\n",
       "      <td>93.31</td>\n",
       "      <td>3.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2010-01-08</td>\n",
       "      <td>0.9218</td>\n",
       "      <td>1.4357</td>\n",
       "      <td>1.7342</td>\n",
       "      <td>1.0345</td>\n",
       "      <td>6.8274</td>\n",
       "      <td>5.1827</td>\n",
       "      <td>7.7553</td>\n",
       "      <td>45.50</td>\n",
       "      <td>92.70</td>\n",
       "      <td>3.375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0  Australia  ...  India  Japan  Malaysia\n",
       "0             0  2010-01-04     0.9133  ...  46.27  92.55     3.396\n",
       "1             1  2010-01-05     0.9143  ...  46.13  91.48     3.385\n",
       "2             2  2010-01-06     0.9189  ...  45.72  92.53     3.379\n",
       "3             3  2010-01-07     0.9168  ...  45.67  93.31     3.368\n",
       "4             4  2010-01-08     0.9218  ...  45.50  92.70     3.375\n",
       "\n",
       "[5 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5cd89d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data.drop(\"Unnamed: 0.1\", axis = 1, inplace = True)\n",
    "    data.rename(columns={\"Unnamed: 0\": \"Date\"}, inplace=True)\n",
    "    data[\"Date\"] = pd.to_datetime(data[\"Date\"])\n",
    "    data.set_index(\"Date\", inplace=True)\n",
    "    data.replace(0, np.nan, inplace=True)\n",
    "    display(data)\n",
    "    print(\"Filling missing Values: \")\n",
    "    display(data.interpolate(method='linear', limit_direction='forward'))\n",
    "    data.interpolate(method='linear', limit_direction='forward', inplace = True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f1ffad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>0.9133</td>\n",
       "      <td>1.4419</td>\n",
       "      <td>1.7200</td>\n",
       "      <td>1.0377</td>\n",
       "      <td>6.8273</td>\n",
       "      <td>5.1597</td>\n",
       "      <td>7.7555</td>\n",
       "      <td>46.27</td>\n",
       "      <td>92.55</td>\n",
       "      <td>3.3960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.9143</td>\n",
       "      <td>1.4402</td>\n",
       "      <td>1.7296</td>\n",
       "      <td>1.0371</td>\n",
       "      <td>6.8258</td>\n",
       "      <td>5.1668</td>\n",
       "      <td>7.7564</td>\n",
       "      <td>46.13</td>\n",
       "      <td>91.48</td>\n",
       "      <td>3.3850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>0.9189</td>\n",
       "      <td>1.4404</td>\n",
       "      <td>1.7292</td>\n",
       "      <td>1.0333</td>\n",
       "      <td>6.8272</td>\n",
       "      <td>5.1638</td>\n",
       "      <td>7.7546</td>\n",
       "      <td>45.72</td>\n",
       "      <td>92.53</td>\n",
       "      <td>3.3790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>0.9168</td>\n",
       "      <td>1.4314</td>\n",
       "      <td>1.7409</td>\n",
       "      <td>1.0351</td>\n",
       "      <td>6.8280</td>\n",
       "      <td>5.1981</td>\n",
       "      <td>7.7539</td>\n",
       "      <td>45.67</td>\n",
       "      <td>93.31</td>\n",
       "      <td>3.3680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>0.9218</td>\n",
       "      <td>1.4357</td>\n",
       "      <td>1.7342</td>\n",
       "      <td>1.0345</td>\n",
       "      <td>6.8274</td>\n",
       "      <td>5.1827</td>\n",
       "      <td>7.7553</td>\n",
       "      <td>45.50</td>\n",
       "      <td>92.70</td>\n",
       "      <td>3.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>0.6978</td>\n",
       "      <td>1.1174</td>\n",
       "      <td>4.0507</td>\n",
       "      <td>1.3073</td>\n",
       "      <td>6.9954</td>\n",
       "      <td>6.6829</td>\n",
       "      <td>7.7874</td>\n",
       "      <td>71.45</td>\n",
       "      <td>109.47</td>\n",
       "      <td>4.1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>0.7004</td>\n",
       "      <td>1.1217</td>\n",
       "      <td>4.0152</td>\n",
       "      <td>1.3058</td>\n",
       "      <td>6.9864</td>\n",
       "      <td>6.6589</td>\n",
       "      <td>7.7857</td>\n",
       "      <td>71.30</td>\n",
       "      <td>108.85</td>\n",
       "      <td>4.1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>0.7030</td>\n",
       "      <td>1.1227</td>\n",
       "      <td>4.0190</td>\n",
       "      <td>1.2962</td>\n",
       "      <td>6.9618</td>\n",
       "      <td>6.6554</td>\n",
       "      <td>7.7894</td>\n",
       "      <td>71.36</td>\n",
       "      <td>108.67</td>\n",
       "      <td>4.0918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3648 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Australia  Europe  Brazil  ...  India   Japan  Malaysia\n",
       "Date                                   ...                         \n",
       "2010-01-04     0.9133  1.4419  1.7200  ...  46.27   92.55    3.3960\n",
       "2010-01-05     0.9143  1.4402  1.7296  ...  46.13   91.48    3.3850\n",
       "2010-01-06     0.9189  1.4404  1.7292  ...  45.72   92.53    3.3790\n",
       "2010-01-07     0.9168  1.4314  1.7409  ...  45.67   93.31    3.3680\n",
       "2010-01-08     0.9218  1.4357  1.7342  ...  45.50   92.70    3.3750\n",
       "...               ...     ...     ...  ...    ...     ...       ...\n",
       "2019-12-27     0.6978  1.1174  4.0507  ...  71.45  109.47    4.1260\n",
       "2019-12-28        NaN     NaN     NaN  ...    NaN     NaN       NaN\n",
       "2019-12-29        NaN     NaN     NaN  ...    NaN     NaN       NaN\n",
       "2019-12-30     0.7004  1.1217  4.0152  ...  71.30  108.85    4.1053\n",
       "2019-12-31     0.7030  1.1227  4.0190  ...  71.36  108.67    4.0918\n",
       "\n",
       "[3648 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling missing Values: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>0.913300</td>\n",
       "      <td>1.441900</td>\n",
       "      <td>1.720000</td>\n",
       "      <td>1.0377</td>\n",
       "      <td>6.8273</td>\n",
       "      <td>5.1597</td>\n",
       "      <td>7.755500</td>\n",
       "      <td>46.27</td>\n",
       "      <td>92.550000</td>\n",
       "      <td>3.3960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.914300</td>\n",
       "      <td>1.440200</td>\n",
       "      <td>1.729600</td>\n",
       "      <td>1.0371</td>\n",
       "      <td>6.8258</td>\n",
       "      <td>5.1668</td>\n",
       "      <td>7.756400</td>\n",
       "      <td>46.13</td>\n",
       "      <td>91.480000</td>\n",
       "      <td>3.3850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-06</th>\n",
       "      <td>0.918900</td>\n",
       "      <td>1.440400</td>\n",
       "      <td>1.729200</td>\n",
       "      <td>1.0333</td>\n",
       "      <td>6.8272</td>\n",
       "      <td>5.1638</td>\n",
       "      <td>7.754600</td>\n",
       "      <td>45.72</td>\n",
       "      <td>92.530000</td>\n",
       "      <td>3.3790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-07</th>\n",
       "      <td>0.916800</td>\n",
       "      <td>1.431400</td>\n",
       "      <td>1.740900</td>\n",
       "      <td>1.0351</td>\n",
       "      <td>6.8280</td>\n",
       "      <td>5.1981</td>\n",
       "      <td>7.753900</td>\n",
       "      <td>45.67</td>\n",
       "      <td>93.310000</td>\n",
       "      <td>3.3680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-08</th>\n",
       "      <td>0.921800</td>\n",
       "      <td>1.435700</td>\n",
       "      <td>1.734200</td>\n",
       "      <td>1.0345</td>\n",
       "      <td>6.8274</td>\n",
       "      <td>5.1827</td>\n",
       "      <td>7.755300</td>\n",
       "      <td>45.50</td>\n",
       "      <td>92.700000</td>\n",
       "      <td>3.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>0.697800</td>\n",
       "      <td>1.117400</td>\n",
       "      <td>4.050700</td>\n",
       "      <td>1.3073</td>\n",
       "      <td>6.9954</td>\n",
       "      <td>6.6829</td>\n",
       "      <td>7.787400</td>\n",
       "      <td>71.45</td>\n",
       "      <td>109.470000</td>\n",
       "      <td>4.1260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-28</th>\n",
       "      <td>0.698667</td>\n",
       "      <td>1.118833</td>\n",
       "      <td>4.038867</td>\n",
       "      <td>1.3068</td>\n",
       "      <td>6.9924</td>\n",
       "      <td>6.6749</td>\n",
       "      <td>7.786833</td>\n",
       "      <td>71.40</td>\n",
       "      <td>109.263333</td>\n",
       "      <td>4.1191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-29</th>\n",
       "      <td>0.699533</td>\n",
       "      <td>1.120267</td>\n",
       "      <td>4.027033</td>\n",
       "      <td>1.3063</td>\n",
       "      <td>6.9894</td>\n",
       "      <td>6.6669</td>\n",
       "      <td>7.786267</td>\n",
       "      <td>71.35</td>\n",
       "      <td>109.056667</td>\n",
       "      <td>4.1122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-30</th>\n",
       "      <td>0.700400</td>\n",
       "      <td>1.121700</td>\n",
       "      <td>4.015200</td>\n",
       "      <td>1.3058</td>\n",
       "      <td>6.9864</td>\n",
       "      <td>6.6589</td>\n",
       "      <td>7.785700</td>\n",
       "      <td>71.30</td>\n",
       "      <td>108.850000</td>\n",
       "      <td>4.1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>0.703000</td>\n",
       "      <td>1.122700</td>\n",
       "      <td>4.019000</td>\n",
       "      <td>1.2962</td>\n",
       "      <td>6.9618</td>\n",
       "      <td>6.6554</td>\n",
       "      <td>7.789400</td>\n",
       "      <td>71.36</td>\n",
       "      <td>108.670000</td>\n",
       "      <td>4.0918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3648 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Australia    Europe    Brazil  ...  India       Japan  Malaysia\n",
       "Date                                       ...                             \n",
       "2010-01-04   0.913300  1.441900  1.720000  ...  46.27   92.550000    3.3960\n",
       "2010-01-05   0.914300  1.440200  1.729600  ...  46.13   91.480000    3.3850\n",
       "2010-01-06   0.918900  1.440400  1.729200  ...  45.72   92.530000    3.3790\n",
       "2010-01-07   0.916800  1.431400  1.740900  ...  45.67   93.310000    3.3680\n",
       "2010-01-08   0.921800  1.435700  1.734200  ...  45.50   92.700000    3.3750\n",
       "...               ...       ...       ...  ...    ...         ...       ...\n",
       "2019-12-27   0.697800  1.117400  4.050700  ...  71.45  109.470000    4.1260\n",
       "2019-12-28   0.698667  1.118833  4.038867  ...  71.40  109.263333    4.1191\n",
       "2019-12-29   0.699533  1.120267  4.027033  ...  71.35  109.056667    4.1122\n",
       "2019-12-30   0.700400  1.121700  4.015200  ...  71.30  108.850000    4.1053\n",
       "2019-12-31   0.703000  1.122700  4.019000  ...  71.36  108.670000    4.0918\n",
       "\n",
       "[3648 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab94630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOOK_BACK = 30\n",
    "PREDICT_DAY = 1\n",
    "SPLIT_RATIO = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "673eb188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Data(\n",
    "    data, lookback=LOOK_BACK, pred_len=PREDICT_DAY, split_ratio=SPLIT_RATIO, model=\"FNN\"\n",
    "):\n",
    "    if lookback < 2:\n",
    "        print(\"ERROR: Lookback too small\")\n",
    "        return -1\n",
    "\n",
    "    # declarations\n",
    "\n",
    "    x = {}\n",
    "    y = {}\n",
    "    xtr = {}\n",
    "    xt = {}\n",
    "    ytr = {}\n",
    "    yt = {}\n",
    "    scalers = {}\n",
    "\n",
    "    # Creating stepped data\n",
    "\n",
    "    for i in data.columns:\n",
    "        xtemp = pd.DataFrame(data[i])\n",
    "        for j in range(1, lookback + 1):\n",
    "            xtemp[i + str(j)] = data[i].shift(-1 * j)\n",
    "        x[i] = xtemp.dropna()\n",
    "\n",
    "    # Splitting data into x and y\n",
    "\n",
    "    for i in x.keys():\n",
    "        y[i] = pd.DataFrame(x[i].iloc[:, -pred_len])\n",
    "        x[i] = x[i].iloc[:, :-pred_len]\n",
    "\n",
    "    # Normalizing x and y values\n",
    "\n",
    "    for i in x.keys():\n",
    "        scalers[i + \"_x\"] = MinMaxScaler(feature_range=(0, 1))\n",
    "        x[i] = scalers[i + \"_x\"].fit_transform(x[i])\n",
    "        scalers[i + \"_y\"] = MinMaxScaler(feature_range=(0, 1))\n",
    "        y[i] = scalers[i + \"_y\"].fit_transform(y[i])\n",
    "\n",
    "    # setting train and test sizes\n",
    "\n",
    "    tr_len = int(split_ratio * y[\"India\"].shape[0])\n",
    "    t_len = y[\"India\"].shape[0] - tr_len\n",
    "\n",
    "    # creating training and testing data\n",
    "\n",
    "    for i in x.keys():\n",
    "        xtr[i] = x[i][:tr_len]\n",
    "        ytr[i] = y[i][:tr_len]\n",
    "        xt[i] = x[i][-t_len:]\n",
    "        yt[i] = y[i][-t_len:]\n",
    "\n",
    "    # returning pertinent data\n",
    "\n",
    "    return x, y, xtr, xt, ytr, yt, scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fc04bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,xtr,xt,ytr,yt,scalers = Create_Data(data, model=\"RNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "333787a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_model(x,y, lookback = LOOK_BACK, Pred_size = PREDICT_DAY):\n",
    "    models = {}\n",
    "    for i in x.keys():\n",
    "        models[i] = Sequential()\n",
    "        models[i].add(GRU(32,input_shape=(LOOK_BACK,1,),return_sequences=True,activation=\"relu\"))\n",
    "        models[i].add(GRU(64,return_sequences=True,activation=\"relu\"))\n",
    "        models[i].add(Dropout(0.2))\n",
    "        models[i].add(GRU(128,return_sequences=True,activation=\"relu\"))\n",
    "        models[i].add(Dropout(0.2))\n",
    "        models[i].add(GRU(64,return_sequences=True,activation=\"relu\"))\n",
    "        models[i].add(Dropout(0.2))\n",
    "        models[i].add(GRU(16,activation=\"relu\"))\n",
    "        models[i].add(Dense(Pred_size))\n",
    "        models[i].compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "        print(i)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2df7ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 11:55:56.555013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:55:56.578033: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:55:56.578749: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:55:56.582248: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:55:56.582684: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:55:56.582992: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:55:56.686949: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:55:56.687312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:55:56.687540: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-10-05 11:55:56.687726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 732 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2023-10-05 11:55:56.688339: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Australia\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru (GRU)                   (None, 30, 32)            3360      \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 30, 64)            18816     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_2 (GRU)                 (None, 30, 128)           74496     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_3 (GRU)                 (None, 30, 64)            37248     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_4 (GRU)                 (None, 16)                3936      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137,873\n",
      "Trainable params: 137,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_6 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Europe\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_5 (GRU)                 (None, 30, 32)            3360      \n",
      "                                                                 \n",
      " gru_6 (GRU)                 (None, 30, 64)            18816     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_7 (GRU)                 (None, 30, 128)           74496     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_8 (GRU)                 (None, 30, 64)            37248     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_9 (GRU)                 (None, 16)                3936      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137,873\n",
      "Trainable params: 137,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_12 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_13 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_14 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Brazil\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_10 (GRU)                (None, 30, 32)            3360      \n",
      "                                                                 \n",
      " gru_11 (GRU)                (None, 30, 64)            18816     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_12 (GRU)                (None, 30, 128)           74496     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_13 (GRU)                (None, 30, 64)            37248     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_14 (GRU)                (None, 16)                3936      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137,873\n",
      "Trainable params: 137,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_15 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_16 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_17 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_18 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_19 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Canada\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_15 (GRU)                (None, 30, 32)            3360      \n",
      "                                                                 \n",
      " gru_16 (GRU)                (None, 30, 64)            18816     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_17 (GRU)                (None, 30, 128)           74496     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_18 (GRU)                (None, 30, 64)            37248     \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_19 (GRU)                (None, 16)                3936      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137,873\n",
      "Trainable params: 137,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_20 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_24 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "China\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_20 (GRU)                (None, 30, 32)            3360      \n",
      "                                                                 \n",
      " gru_21 (GRU)                (None, 30, 64)            18816     \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_22 (GRU)                (None, 30, 128)           74496     \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_23 (GRU)                (None, 30, 64)            37248     \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_24 (GRU)                (None, 16)                3936      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137,873\n",
      "Trainable params: 137,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_27 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_28 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_29 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Denmark\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_25 (GRU)                (None, 30, 32)            3360      \n",
      "                                                                 \n",
      " gru_26 (GRU)                (None, 30, 64)            18816     \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_27 (GRU)                (None, 30, 128)           74496     \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_28 (GRU)                (None, 30, 64)            37248     \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_29 (GRU)                (None, 16)                3936      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137,873\n",
      "Trainable params: 137,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_30 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_31 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_32 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_33 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Hong Kong\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_30 (GRU)                (None, 30, 32)            3360      \n",
      "                                                                 \n",
      " gru_31 (GRU)                (None, 30, 64)            18816     \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_32 (GRU)                (None, 30, 128)           74496     \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_33 (GRU)                (None, 30, 64)            37248     \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_34 (GRU)                (None, 16)                3936      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137,873\n",
      "Trainable params: 137,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_35 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_37 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_38 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_39 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "India\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_35 (GRU)                (None, 30, 32)            3360      \n",
      "                                                                 \n",
      " gru_36 (GRU)                (None, 30, 64)            18816     \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_37 (GRU)                (None, 30, 128)           74496     \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_38 (GRU)                (None, 30, 64)            37248     \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_39 (GRU)                (None, 16)                3936      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137,873\n",
      "Trainable params: 137,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_40 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_41 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_42 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_43 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_44 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Japan\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_40 (GRU)                (None, 30, 32)            3360      \n",
      "                                                                 \n",
      " gru_41 (GRU)                (None, 30, 64)            18816     \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_42 (GRU)                (None, 30, 128)           74496     \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_43 (GRU)                (None, 30, 64)            37248     \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_44 (GRU)                (None, 16)                3936      \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137,873\n",
      "Trainable params: 137,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_45 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_46 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_47 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Malaysia\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " gru_45 (GRU)                (None, 30, 32)            3360      \n",
      "                                                                 \n",
      " gru_46 (GRU)                (None, 30, 64)            18816     \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_47 (GRU)                (None, 30, 128)           74496     \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 30, 128)           0         \n",
      "                                                                 \n",
      " gru_48 (GRU)                (None, 30, 64)            37248     \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 30, 64)            0         \n",
      "                                                                 \n",
      " gru_49 (GRU)                (None, 16)                3936      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137,873\n",
      "Trainable params: 137,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m = Create_model(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "599dc5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Execute_model(model, xtr, ytr, xt, yt, scaler):\n",
    "    MAPE = {}\n",
    "    MAE = {}\n",
    "    MSE = {}\n",
    "    for i in model.keys():\n",
    "        print(i)\n",
    "        # Creating EarlyStopping and ReduceLROnPlateau callbacks\n",
    "        es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", verbose=1, patience=10)\n",
    "        reduce_lr = ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\", factor=0.2, patience=5, min_lr=0.0001\n",
    "        )\n",
    "\n",
    "        # Training the model with EarlyStopping and ReduceLROnPlateau callbacks\n",
    "        model[i].fit(\n",
    "            xtr[i],\n",
    "            ytr[i],\n",
    "            epochs=100,\n",
    "            batch_size=64,\n",
    "            verbose=1,\n",
    "            validation_split=0.2,\n",
    "            callbacks=[es, reduce_lr],\n",
    "        )\n",
    "\n",
    "        # collecting predicted and actual values\n",
    "        temp = model[i].predict(xt[i])\n",
    "        pred = scaler[i + \"_y\"].inverse_transform(temp)\n",
    "        act = scaler[i + \"_y\"].inverse_transform(yt[i])\n",
    "\n",
    "        # calculating Mean Square Error, Mean Absolute Error, and Mean Absolute Error\n",
    "        MSE[i] = mean_squared_error(act, pred)\n",
    "        MAE[i] = mean_absolute_error(act, pred)\n",
    "        MAPE[i] = mean_absolute_percentage_error(act, pred)\n",
    "\n",
    "    # Tabulating Data\n",
    "    results = pd.DataFrame([MSE, MAE, MAPE])\n",
    "    results[\"Metric\"] = [\"MSE\", \"MAE\", \"MAPE\"]\n",
    "    results.set_index(\"Metric\", inplace=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddd35db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Australia\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-05 11:56:05.016208: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559e22844ec0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-10-05 11:56:05.016242: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2023-10-05 11:56:05.021902: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-10-05 11:56:05.035112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8902\n",
      "2023-10-05 11:56:05.142235: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 13s 161ms/step - loss: 0.1147 - val_loss: 0.0596 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0096 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 139ms/step - loss: 0.0032 - val_loss: 6.3418e-04 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0027 - val_loss: 4.6577e-04 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 137ms/step - loss: 0.0025 - val_loss: 4.3089e-04 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 0.0023 - val_loss: 3.5592e-04 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 138ms/step - loss: 0.0022 - val_loss: 4.3839e-04 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0019 - val_loss: 4.6310e-04 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0019 - val_loss: 6.7252e-04 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 139ms/step - loss: 0.0017 - val_loss: 3.7204e-04 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 137ms/step - loss: 0.0017 - val_loss: 3.1924e-04 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0016 - val_loss: 4.8842e-04 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0015 - val_loss: 4.7735e-04 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 139ms/step - loss: 0.0015 - val_loss: 4.2304e-04 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 148ms/step - loss: 0.0016 - val_loss: 3.7037e-04 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0015 - val_loss: 4.3057e-04 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 6s 149ms/step - loss: 0.0015 - val_loss: 4.5477e-04 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 5s 140ms/step - loss: 0.0015 - val_loss: 4.9099e-04 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0015 - val_loss: 3.9568e-04 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 5s 148ms/step - loss: 0.0015 - val_loss: 3.9870e-04 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0014 - val_loss: 4.3442e-04 - lr: 1.0000e-04\n",
      "Epoch 21: early stopping\n",
      "23/23 [==============================] - 1s 22ms/step\n",
      "Europe\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 12s 156ms/step - loss: 0.0730 - val_loss: 0.0269 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0067 - val_loss: 0.0023 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 135ms/step - loss: 0.0028 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 136ms/step - loss: 0.0024 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0022 - val_loss: 9.3673e-04 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 138ms/step - loss: 0.0020 - val_loss: 9.5531e-04 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 138ms/step - loss: 0.0020 - val_loss: 9.5152e-04 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 0.0017 - val_loss: 9.1119e-04 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 0.0018 - val_loss: 8.2570e-04 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0016 - val_loss: 8.5114e-04 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 0.0016 - val_loss: 6.6347e-04 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0014 - val_loss: 8.3986e-04 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 140ms/step - loss: 0.0016 - val_loss: 9.4114e-04 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0014 - val_loss: 7.0059e-04 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0014 - val_loss: 7.5751e-04 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 5s 138ms/step - loss: 0.0014 - val_loss: 5.8964e-04 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 5s 137ms/step - loss: 0.0013 - val_loss: 6.3568e-04 - lr: 2.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0012 - val_loss: 6.3348e-04 - lr: 2.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 5s 148ms/step - loss: 0.0012 - val_loss: 6.1207e-04 - lr: 2.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0012 - val_loss: 6.0751e-04 - lr: 2.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0013 - val_loss: 7.3232e-04 - lr: 2.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0012 - val_loss: 6.1532e-04 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 6s 149ms/step - loss: 0.0012 - val_loss: 6.0725e-04 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0012 - val_loss: 6.3511e-04 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0012 - val_loss: 5.9994e-04 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 5s 135ms/step - loss: 0.0012 - val_loss: 6.1259e-04 - lr: 1.0000e-04\n",
      "Epoch 26: early stopping\n",
      "23/23 [==============================] - 1s 23ms/step\n",
      "Brazil\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 12s 161ms/step - loss: 0.0416 - val_loss: 0.0044 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 139ms/step - loss: 0.0019 - val_loss: 7.6410e-04 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0011 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 9.8714e-04 - val_loss: 5.2351e-04 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 139ms/step - loss: 8.1065e-04 - val_loss: 0.0020 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 7.6789e-04 - val_loss: 4.2922e-04 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 8.0341e-04 - val_loss: 4.8167e-04 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 139ms/step - loss: 6.6213e-04 - val_loss: 0.0017 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 135ms/step - loss: 7.7803e-04 - val_loss: 0.0017 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 6.2431e-04 - val_loss: 5.1091e-04 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 148ms/step - loss: 6.2422e-04 - val_loss: 3.2270e-04 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 6.1456e-04 - val_loss: 3.7684e-04 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 5.8118e-04 - val_loss: 3.8773e-04 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 136ms/step - loss: 5.7652e-04 - val_loss: 5.1620e-04 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 6.0224e-04 - val_loss: 4.5316e-04 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 5.5630e-04 - val_loss: 5.0257e-04 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 5.6311e-04 - val_loss: 3.4115e-04 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 5.8578e-04 - val_loss: 5.2650e-04 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 5.4403e-04 - val_loss: 3.4107e-04 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 5s 138ms/step - loss: 5.7388e-04 - val_loss: 4.2714e-04 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 5.1267e-04 - val_loss: 3.9891e-04 - lr: 1.0000e-04\n",
      "Epoch 21: early stopping\n",
      "23/23 [==============================] - 1s 20ms/step\n",
      "Canada\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 12s 165ms/step - loss: 0.0313 - val_loss: 0.0047 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0018 - val_loss: 6.9239e-04 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0013 - val_loss: 0.0026 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 148ms/step - loss: 0.0013 - val_loss: 6.9348e-04 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 6s 150ms/step - loss: 9.5347e-04 - val_loss: 0.0014 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 6s 149ms/step - loss: 9.7934e-04 - val_loss: 8.2681e-04 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 8.4720e-04 - val_loss: 0.0010 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 7.8074e-04 - val_loss: 8.3449e-04 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 8.0511e-04 - val_loss: 7.6831e-04 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 7.5873e-04 - val_loss: 0.0013 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 7.8651e-04 - val_loss: 0.0010 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 7.4667e-04 - val_loss: 6.4948e-04 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 139ms/step - loss: 7.7731e-04 - val_loss: 0.0016 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 7.6000e-04 - val_loss: 7.2592e-04 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 7.3380e-04 - val_loss: 0.0012 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 7.6428e-04 - val_loss: 7.4500e-04 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 6s 153ms/step - loss: 7.6730e-04 - val_loss: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 7.5176e-04 - val_loss: 0.0014 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 6.8791e-04 - val_loss: 0.0010 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 6.9064e-04 - val_loss: 8.0403e-04 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 7.5146e-04 - val_loss: 0.0011 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 7.1047e-04 - val_loss: 7.4197e-04 - lr: 1.0000e-04\n",
      "Epoch 22: early stopping\n",
      "23/23 [==============================] - 1s 20ms/step\n",
      "China\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 12s 159ms/step - loss: 0.0500 - val_loss: 0.0159 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 0.0034 - val_loss: 8.3848e-04 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 148ms/step - loss: 9.4607e-04 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 8.2441e-04 - val_loss: 9.2352e-04 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 7.8345e-04 - val_loss: 0.0021 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 6.2912e-04 - val_loss: 0.0017 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 5.7588e-04 - val_loss: 0.0022 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 148ms/step - loss: 5.7503e-04 - val_loss: 0.0014 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 5.2477e-04 - val_loss: 0.0014 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 4.8740e-04 - val_loss: 0.0014 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 149ms/step - loss: 4.9641e-04 - val_loss: 0.0013 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 6s 150ms/step - loss: 5.0113e-04 - val_loss: 0.0011 - lr: 2.0000e-04\n",
      "Epoch 12: early stopping\n",
      "23/23 [==============================] - 1s 32ms/step\n",
      "Denmark\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 12s 164ms/step - loss: 0.0611 - val_loss: 0.0230 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0032 - val_loss: 0.0013 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 0.0021 - val_loss: 9.2052e-04 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 6s 152ms/step - loss: 0.0019 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0018 - val_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 148ms/step - loss: 0.0017 - val_loss: 0.0016 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 6s 151ms/step - loss: 0.0016 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0015 - val_loss: 0.0018 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 0.0014 - val_loss: 0.0012 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0014 - val_loss: 0.0013 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 6s 151ms/step - loss: 0.0013 - val_loss: 0.0014 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0014 - val_loss: 0.0015 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 147ms/step - loss: 0.0013 - val_loss: 0.0016 - lr: 2.0000e-04\n",
      "Epoch 13: early stopping\n",
      "23/23 [==============================] - 1s 27ms/step\n",
      "Hong Kong\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 12s 164ms/step - loss: 0.0140 - val_loss: 0.0024 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 138ms/step - loss: 0.0029 - val_loss: 0.0048 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0024 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0022 - val_loss: 0.0034 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 145ms/step - loss: 0.0017 - val_loss: 0.0055 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 6s 149ms/step - loss: 0.0017 - val_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0014 - val_loss: 0.0025 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0013 - val_loss: 0.0028 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 135ms/step - loss: 0.0011 - val_loss: 0.0012 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0011 - val_loss: 0.0019 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 0.0011 - val_loss: 0.0012 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0011 - val_loss: 0.0010 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 137ms/step - loss: 0.0010 - val_loss: 0.0013 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 0.0011 - val_loss: 0.0013 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 137ms/step - loss: 0.0011 - val_loss: 0.0011 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 5s 148ms/step - loss: 0.0011 - val_loss: 0.0010 - lr: 2.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 5s 138ms/step - loss: 0.0010 - val_loss: 0.0016 - lr: 2.0000e-04\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 5s 136ms/step - loss: 0.0011 - val_loss: 0.0019 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 5s 140ms/step - loss: 0.0010 - val_loss: 0.0015 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 5s 138ms/step - loss: 0.0010 - val_loss: 0.0016 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 5s 137ms/step - loss: 9.6827e-04 - val_loss: 0.0016 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 5s 140ms/step - loss: 9.5198e-04 - val_loss: 0.0015 - lr: 1.0000e-04\n",
      "Epoch 22: early stopping\n",
      "23/23 [==============================] - 1s 26ms/step\n",
      "India\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 12s 155ms/step - loss: 0.0595 - val_loss: 0.0073 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 138ms/step - loss: 0.0030 - val_loss: 2.2343e-04 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0017 - val_loss: 5.6756e-04 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 0.0012 - val_loss: 2.0576e-04 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 137ms/step - loss: 0.0013 - val_loss: 0.0019 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 136ms/step - loss: 0.0012 - val_loss: 6.1054e-04 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 9.8921e-04 - val_loss: 1.7888e-04 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 9.0135e-04 - val_loss: 2.7576e-04 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 139ms/step - loss: 8.4826e-04 - val_loss: 3.7323e-04 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 135ms/step - loss: 8.2154e-04 - val_loss: 2.6358e-04 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 139ms/step - loss: 8.1959e-04 - val_loss: 4.3426e-04 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 138ms/step - loss: 8.0764e-04 - val_loss: 2.9792e-04 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 136ms/step - loss: 7.9615e-04 - val_loss: 5.4715e-04 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 7.8783e-04 - val_loss: 5.4801e-04 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 138ms/step - loss: 7.9965e-04 - val_loss: 4.4015e-04 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 5s 144ms/step - loss: 7.7750e-04 - val_loss: 5.8218e-04 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 5s 136ms/step - loss: 7.6907e-04 - val_loss: 5.4094e-04 - lr: 1.0000e-04\n",
      "Epoch 17: early stopping\n",
      "23/23 [==============================] - 1s 31ms/step\n",
      "Japan\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 11s 155ms/step - loss: 0.1101 - val_loss: 0.0058 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 137ms/step - loss: 0.0051 - val_loss: 9.4943e-04 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 140ms/step - loss: 0.0024 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 6s 149ms/step - loss: 0.0019 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0015 - val_loss: 0.0011 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 139ms/step - loss: 0.0014 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 140ms/step - loss: 0.0011 - val_loss: 0.0012 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 142ms/step - loss: 0.0011 - val_loss: 0.0012 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 135ms/step - loss: 0.0011 - val_loss: 0.0014 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 136ms/step - loss: 0.0011 - val_loss: 0.0013 - lr: 2.0000e-04\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 139ms/step - loss: 0.0011 - val_loss: 0.0015 - lr: 2.0000e-04\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 146ms/step - loss: 0.0011 - val_loss: 0.0013 - lr: 2.0000e-04\n",
      "Epoch 12: early stopping\n",
      "23/23 [==============================] - 1s 27ms/step\n",
      "Malaysia\n",
      "Epoch 1/100\n",
      "37/37 [==============================] - 11s 161ms/step - loss: 0.0423 - val_loss: 0.0038 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 5s 139ms/step - loss: 0.0020 - val_loss: 0.0030 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 5s 137ms/step - loss: 0.0014 - val_loss: 6.3426e-04 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 5s 143ms/step - loss: 0.0012 - val_loss: 5.4813e-04 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 5s 139ms/step - loss: 0.0011 - val_loss: 0.0017 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 5s 141ms/step - loss: 0.0010 - val_loss: 4.5799e-04 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 9.6123e-04 - val_loss: 4.8585e-04 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 5s 136ms/step - loss: 0.0011 - val_loss: 0.0017 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 5s 131ms/step - loss: 9.5427e-04 - val_loss: 6.2790e-04 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 5s 136ms/step - loss: 0.0011 - val_loss: 0.0015 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 8.0810e-04 - val_loss: 6.7513e-04 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 7.8680e-04 - val_loss: 7.0499e-04 - lr: 2.0000e-04\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 5s 138ms/step - loss: 7.2563e-04 - val_loss: 5.3337e-04 - lr: 2.0000e-04\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 5s 133ms/step - loss: 7.7751e-04 - val_loss: 9.2972e-04 - lr: 2.0000e-04\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 5s 136ms/step - loss: 7.3957e-04 - val_loss: 7.2276e-04 - lr: 2.0000e-04\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 5s 134ms/step - loss: 7.4126e-04 - val_loss: 5.0275e-04 - lr: 2.0000e-04\n",
      "Epoch 16: early stopping\n",
      "23/23 [==============================] - 1s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "result = Execute_model(m,xtr,ytr,xt,yt,scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76c1e7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Australia</th>\n",
       "      <th>Europe</th>\n",
       "      <th>Brazil</th>\n",
       "      <th>Canada</th>\n",
       "      <th>China</th>\n",
       "      <th>Denmark</th>\n",
       "      <th>Hong Kong</th>\n",
       "      <th>India</th>\n",
       "      <th>Japan</th>\n",
       "      <th>Malaysia</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MSE</th>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.013526</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.003703</td>\n",
       "      <td>0.004048</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>2.155339</td>\n",
       "      <td>1.168184</td>\n",
       "      <td>0.000315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAE</th>\n",
       "      <td>0.010306</td>\n",
       "      <td>0.006356</td>\n",
       "      <td>0.097011</td>\n",
       "      <td>0.010111</td>\n",
       "      <td>0.046240</td>\n",
       "      <td>0.053737</td>\n",
       "      <td>0.012867</td>\n",
       "      <td>1.317829</td>\n",
       "      <td>0.920567</td>\n",
       "      <td>0.013895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MAPE</th>\n",
       "      <td>0.014614</td>\n",
       "      <td>0.005464</td>\n",
       "      <td>0.024983</td>\n",
       "      <td>0.007681</td>\n",
       "      <td>0.006755</td>\n",
       "      <td>0.008230</td>\n",
       "      <td>0.001641</td>\n",
       "      <td>0.018773</td>\n",
       "      <td>0.008379</td>\n",
       "      <td>0.003409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Australia    Europe    Brazil  ...     India     Japan  Malaysia\n",
       "Metric                                 ...                              \n",
       "MSE      0.000180  0.000070  0.013526  ...  2.155339  1.168184  0.000315\n",
       "MAE      0.010306  0.006356  0.097011  ...  1.317829  0.920567  0.013895\n",
       "MAPE     0.014614  0.005464  0.024983  ...  0.018773  0.008379  0.003409\n",
       "\n",
       "[3 rows x 10 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
